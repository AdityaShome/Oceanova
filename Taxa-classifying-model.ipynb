{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34ccf425-e268-45a6-8918-3e10a50f4f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy: 1.26.4\n",
      "TensorFlow: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, tensorflow as tf\n",
    "print(\"NumPy:\", np.__version__)\n",
    "print(\"TensorFlow:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96269d53-1f60-4d9a-b69c-b162854e24e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: biopython in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (1.85)\n",
      "Requirement already satisfied: gensim in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: hdbscan in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (0.8.40)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (3.10.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: torch in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (6.30.1)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from gensim) (7.3.1)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from hdbscan) (1.5.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from matplotlib) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from matplotlib) (3.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorflow) (6.32.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorflow) (78.1.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorflow) (1.17.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorflow) (1.74.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorflow) (3.11.3)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.9)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from ipykernel) (0.2.1)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from ipykernel) (1.8.16)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from ipykernel) (9.1.0)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from ipykernel) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from ipykernel) (5.8.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from ipykernel) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from ipykernel) (1.6.0)\n",
      "Requirement already satisfied: psutil>=5.7 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from ipykernel) (7.0.0)\n",
      "Requirement already satisfied: pyzmq>=25 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from ipykernel) (26.2.0)\n",
      "Requirement already satisfied: tornado>=6.2 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from ipykernel) (6.5.1)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from ipykernel) (5.14.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.19.2)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (2.19.1)\n",
      "Requirement already satisfied: stack_data in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.4)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (4.3.7)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (311)\n",
      "Requirement already satisfied: rich in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from keras>=3.10.0->tensorflow) (14.1.0)\n",
      "Requirement already satisfied: namex in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in c:\\users\\srijit\\anaconda3\\envs\\sih\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install all detected packages (CPU/default wheels) from PyPI\n",
    "%pip install biopython gensim hdbscan matplotlib numpy pandas scikit-learn tensorflow torch ipykernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d2e28b1-c114-43ec-94a3-0dc7be835805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working dir: C:\\Users\\Srijit\\sih\\ncbi_blast_db\n",
      "DB files to fetch: {'ssu': 'SSU_eukaryote_rRNA.tar.gz', 'lsu': 'LSU_eukaryote_rRNA.tar.gz', 'its': 'ITS_eukaryote_sequences.tar.gz', 'ssu_meta': 'SSU_eukaryote_rRNA-nucl-metadata.json', 'lsu_meta': 'LSU_eukaryote_rRNA-nucl-metadata.json', 'its_meta': 'ITS_eukaryote_sequences-nucl-metadata.json'}\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: imports, constants, reproducibility\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import tarfile\n",
    "import hashlib\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Basic ML / bio libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PyTorch + sklearn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# NLP / k-mer embeddings\n",
    "import gensim\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Paths and chosen DB files (from NCBI ftp listing)\n",
    "DOWNLOAD_DIR = Path(\"./ncbi_blast_db\")\n",
    "DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Filenames observed on the BLAST DB FTP listing (we will download these exact files).\n",
    "DB_FILES = {\n",
    "    \"ssu\": \"SSU_eukaryote_rRNA.tar.gz\",\n",
    "    \"lsu\": \"LSU_eukaryote_rRNA.tar.gz\",\n",
    "    \"its\": \"ITS_eukaryote_sequences.tar.gz\",\n",
    "    \"ssu_meta\": \"SSU_eukaryote_rRNA-nucl-metadata.json\",\n",
    "    \"lsu_meta\": \"LSU_eukaryote_rRNA-nucl-metadata.json\",\n",
    "    \"its_meta\": \"ITS_eukaryote_sequences-nucl-metadata.json\",\n",
    "}\n",
    "\n",
    "print(\"Working dir:\", DOWNLOAD_DIR.resolve())\n",
    "print(\"DB files to fetch:\", DB_FILES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0f2da3e-e412-441d-b652-49535c81617c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SKIP] Already present: SSU_eukaryote_rRNA.tar.gz  (57.01 MB)\n",
      "[SKIP] Already present: SSU_eukaryote_rRNA-nucl-metadata.json  (0.00 MB)\n",
      "[SKIP] Already present: LSU_eukaryote_rRNA.tar.gz  (56.64 MB)\n",
      "[SKIP] Already present: LSU_eukaryote_rRNA-nucl-metadata.json  (0.00 MB)\n",
      "[SKIP] Already present: ITS_eukaryote_sequences.tar.gz  (71.03 MB)\n",
      "[SKIP] Already present: ITS_eukaryote_sequences-nucl-metadata.json  (0.00 MB)\n",
      "\n",
      "Files in download directory:\n",
      " - ITS_eukaryote_sequences-nucl-metadata.json  0.00 MB\n",
      " - ITS_eukaryote_sequences.tar.gz  71.03 MB\n",
      " - kmer_w2v_k6.model  4.90 MB\n",
      " - LSU_eukaryote_rRNA-nucl-metadata.json  0.00 MB\n",
      " - LSU_eukaryote_rRNA.tar.gz  56.64 MB\n",
      " - SSU_eukaryote_rRNA-nucl-metadata.json  0.00 MB\n",
      " - SSU_eukaryote_rRNA.tar.gz  57.01 MB\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 (fixed): Download the selected tarballs and metadata JSONs using Python (works in a Python kernel)\n",
    "import urllib.request\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Use variables from Cell 1: DOWNLOAD_DIR (Path) and DB_FILES (dict)\n",
    "# If you re-ran the kernel and haven't executed Cell 1, re-declare DOWNLOAD_DIR and DB_FILES accordingly.\n",
    "try:\n",
    "    DOWNLOAD_DIR\n",
    "    DB_FILES\n",
    "except NameError:\n",
    "    DOWNLOAD_DIR = Path(\"./ncbi_blast_db\")\n",
    "    DB_FILES = {\n",
    "        \"ssu\": \"SSU_eukaryote_rRNA.tar.gz\",\n",
    "        \"lsu\": \"LSU_eukaryote_rRNA.tar.gz\",\n",
    "        \"its\": \"ITS_eukaryote_sequences.tar.gz\",\n",
    "        \"ssu_meta\": \"SSU_eukaryote_rRNA-nucl-metadata.json\",\n",
    "        \"lsu_meta\": \"LSU_eukaryote_rRNA-nucl-metadata.json\",\n",
    "        \"its_meta\": \"ITS_eukaryote_sequences-nucl-metadata.json\",\n",
    "    }\n",
    "DOWNLOAD_DIR = Path(DOWNLOAD_DIR)\n",
    "DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BASE_URL = \"https://ftp.ncbi.nlm.nih.gov/blast/db\"\n",
    "files_to_fetch = [\n",
    "    DB_FILES[\"ssu\"],\n",
    "    DB_FILES[\"ssu_meta\"],\n",
    "    DB_FILES[\"lsu\"],\n",
    "    DB_FILES[\"lsu_meta\"],\n",
    "    DB_FILES[\"its\"],\n",
    "    DB_FILES[\"its_meta\"],\n",
    "]\n",
    "\n",
    "def download_url_to_path(url: str, out_path: Path, chunk_size=1024*1024):\n",
    "    \"\"\"Download URL to out_path with chunking and basic retries.\"\"\"\n",
    "    out_path_tmp = out_path.with_suffix(out_path.suffix + \".part\")\n",
    "    headers = {\"User-Agent\": \"python-urllib/3 - ncbi-download-script\"}\n",
    "    req = urllib.request.Request(url, headers=headers)\n",
    "    attempts = 3\n",
    "    for attempt in range(1, attempts+1):\n",
    "        try:\n",
    "            with urllib.request.urlopen(req, timeout=60) as resp, open(out_path_tmp, \"wb\") as outfh:\n",
    "                shutil.copyfileobj(resp, outfh)\n",
    "            out_path_tmp.replace(out_path)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt} failed for {url}: {e}\")\n",
    "            time.sleep(2 * attempt)\n",
    "    return False\n",
    "\n",
    "for fname in files_to_fetch:\n",
    "    url = f\"{BASE_URL}/{fname}\"\n",
    "    out_path = DOWNLOAD_DIR / fname\n",
    "    if out_path.exists():\n",
    "        print(f\"[SKIP] Already present: {fname}  ({out_path.stat().st_size / (1024**2):.2f} MB)\")\n",
    "        continue\n",
    "    print(f\"[DOWNLOAD] {fname} from {url}\")\n",
    "    ok = download_url_to_path(url, out_path)\n",
    "    if not ok:\n",
    "        print(f\"[ERROR] Failed to download {fname}. Check network or try again manually.\")\n",
    "    else:\n",
    "        size_mb = out_path.stat().st_size / (1024**2)\n",
    "        print(f\"[OK] Saved {fname} — {size_mb:.2f} MB\")\n",
    "\n",
    "# Final listing\n",
    "print(\"\\nFiles in download directory:\")\n",
    "for p in sorted(DOWNLOAD_DIR.iterdir()):\n",
    "    if p.is_file():\n",
    "        print(f\" - {p.name}  {p.stat().st_size / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd8c91f4-964d-46a4-8e88-6c4ebe952116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EXTRACT] ssu from ncbi_blast_db\\SSU_eukaryote_rRNA.tar.gz -> ncbi_blast_db\\extracted\\ssu_combined.fasta\n",
      "[DEBUG] skipped member taxdb.bti due to 'charmap' codec can't encode character '\\ufffd' in position 4: character maps to <undefined>\n",
      "[DEBUG] skipped member taxonomy4blast.sqlite3 due to 'charmap' codec can't encode character '\\ufffd' in position 3: character maps to <undefined>\n",
      "[DEBUG] skipped member SSU_eukaryote_rRNA.nin due to 'charmap' codec can't encode character '\\ufffd' in position 1: character maps to <undefined>\n",
      "[DEBUG] skipped member SSU_eukaryote_rRNA.nsq due to 'charmap' codec can't encode characters in position 5-9: character maps to <undefined>\n",
      "  -> wrote approx 2 sequences to ncbi_blast_db\\extracted\\ssu_combined.fasta\n",
      "[EXTRACT] lsu from ncbi_blast_db\\LSU_eukaryote_rRNA.tar.gz -> ncbi_blast_db\\extracted\\lsu_combined.fasta\n",
      "[DEBUG] skipped member taxdb.bti due to 'charmap' codec can't encode character '\\ufffd' in position 4: character maps to <undefined>\n",
      "[DEBUG] skipped member taxonomy4blast.sqlite3 due to 'charmap' codec can't encode character '\\ufffd' in position 3: character maps to <undefined>\n",
      "[DEBUG] skipped member LSU_eukaryote_rRNA.nin due to 'charmap' codec can't encode character '\\ufffd' in position 1: character maps to <undefined>\n",
      "[DEBUG] skipped member LSU_eukaryote_rRNA.nsq due to 'charmap' codec can't encode characters in position 1-2: character maps to <undefined>\n",
      "  -> wrote approx 2 sequences to ncbi_blast_db\\extracted\\lsu_combined.fasta\n",
      "[EXTRACT] its from ncbi_blast_db\\ITS_eukaryote_sequences.tar.gz -> ncbi_blast_db\\extracted\\its_combined.fasta\n",
      "[DEBUG] skipped member ITS_eukaryote_sequences.ndb due to 'charmap' codec can't encode character '\\ufffd' in position 11: character maps to <undefined>\n",
      "[DEBUG] skipped member ITS_eukaryote_sequences.nhr due to 'charmap' codec can't encode character '\\ufffd' in position 8: character maps to <undefined>\n",
      "[DEBUG] skipped member ITS_eukaryote_sequences.nin due to 'charmap' codec can't encode character '\\ufffd' in position 1: character maps to <undefined>\n",
      "[DEBUG] skipped member ITS_eukaryote_sequences.nos due to 'charmap' codec can't encode character '\\ufffd' in position 87: character maps to <undefined>\n",
      "[DEBUG] skipped member ITS_eukaryote_sequences.not due to 'charmap' codec can't encode character '\\ufffd' in position 943: character maps to <undefined>\n",
      "[DEBUG] skipped member ITS_eukaryote_sequences.nsq due to 'charmap' codec can't encode characters in position 5-9: character maps to <undefined>\n",
      "[DEBUG] skipped member ITS_eukaryote_sequences.ntf due to 'charmap' codec can't encode characters in position 3-4: character maps to <undefined>\n",
      "[DEBUG] skipped member ITS_eukaryote_sequences.nto due to 'charmap' codec can't encode character '\\ufffd' in position 15: character maps to <undefined>\n",
      "[DEBUG] skipped member taxdb.bti due to 'charmap' codec can't encode character '\\ufffd' in position 4: character maps to <undefined>\n",
      "[DEBUG] skipped member taxonomy4blast.sqlite3 due to 'charmap' codec can't encode character '\\ufffd' in position 2: character maps to <undefined>\n",
      "  -> wrote approx 5 sequences to ncbi_blast_db\\extracted\\its_combined.fasta\n",
      "ssu combined FASTA: ncbi_blast_db\\extracted\\ssu_combined.fasta (sequences: 2)\n",
      "lsu combined FASTA: ncbi_blast_db\\extracted\\lsu_combined.fasta (sequences: 2)\n",
      "its combined FASTA: ncbi_blast_db\\extracted\\its_combined.fasta (sequences: 5)\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Extract FASTA-like content from downloaded tarballs into an 'extracted' directory.\n",
    "# This cell is defensive: it re-creates DOWNLOAD_DIR if missing and checks tarball presence.\n",
    "import io\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Re-define or sanity-check DOWNLOAD_DIR and DB_FILES if they are missing\n",
    "try:\n",
    "    DOWNLOAD_DIR\n",
    "except NameError:\n",
    "    DOWNLOAD_DIR = Path(\"./ncbi_blast_db\")\n",
    "DOWNLOAD_DIR = Path(DOWNLOAD_DIR)\n",
    "EXTRACT_DIR = DOWNLOAD_DIR / \"extracted\"\n",
    "EXTRACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Tarball names from earlier (safe fallback)\n",
    "try:\n",
    "    DB_FILES\n",
    "except NameError:\n",
    "    DB_FILES = {\n",
    "        \"ssu\": \"SSU_eukaryote_rRNA.tar.gz\",\n",
    "        \"lsu\": \"LSU_eukaryote_rRNA.tar.gz\",\n",
    "        \"its\": \"ITS_eukaryote_sequences.tar.gz\",\n",
    "        \"ssu_meta\": \"SSU_eukaryote_rRNA-nucl-metadata.json\",\n",
    "        \"lsu_meta\": \"LSU_eukaryote_rRNA-nucl-metadata.json\",\n",
    "        \"its_meta\": \"ITS_eukaryote_sequences-nucl-metadata.json\",\n",
    "    }\n",
    "\n",
    "# Limits (safe default)\n",
    "MAX_RECORDS_PER_MARKER = 5000  # change to None to extract everything\n",
    "\n",
    "def extract_and_concatenate_fastas(tarball_path: Path, out_fasta: Path, max_records=None):\n",
    "    \"\"\"\n",
    "    Extract any member file in tarball that appears to contain FASTA (detect '>' lines).\n",
    "    Concatenate only sequence entries found into out_fasta.\n",
    "    \"\"\"\n",
    "    count_sequences = 0\n",
    "    if not tarball_path.exists():\n",
    "        print(f\"[WARN] Tarball not found: {tarball_path}\")\n",
    "        return False, 0\n",
    "    with tarfile.open(tarball_path, \"r:gz\") as tar, open(out_fasta, \"w\") as outfh:\n",
    "        for member in tar:\n",
    "            if not member.isfile():\n",
    "                continue\n",
    "            # Read member in streaming mode\n",
    "            try:\n",
    "                f = tar.extractfile(member)\n",
    "                if f is None:\n",
    "                    continue\n",
    "                # We'll scan for lines starting with '>' and write contiguous blocks\n",
    "                writing = False\n",
    "                seq_lines_written = 0\n",
    "                for raw in f:\n",
    "                    try:\n",
    "                        line = raw.decode('utf-8', errors='replace')\n",
    "                    except Exception:\n",
    "                        line = raw.decode('latin-1', errors='replace')\n",
    "                    if not line:\n",
    "                        continue\n",
    "                    if line.startswith(\">\"):\n",
    "                        # New sequence header\n",
    "                        if max_records and count_sequences >= max_records:\n",
    "                            f.close()\n",
    "                            print(f\"[INFO] Reached max_records ({max_records}) for {tarball_path.name}\")\n",
    "                            return True, count_sequences\n",
    "                        outfh.write(line)\n",
    "                        writing = True\n",
    "                        count_sequences += 1\n",
    "                        seq_lines_written = 0\n",
    "                    else:\n",
    "                        if writing:\n",
    "                            outfh.write(line)\n",
    "                            seq_lines_written += 1\n",
    "                f.close()\n",
    "            except Exception as e:\n",
    "                # skip unreadable members\n",
    "                print(f\"[DEBUG] skipped member {member.name} due to {e}\")\n",
    "                continue\n",
    "    return True, count_sequences\n",
    "\n",
    "# Run extraction for each marker\n",
    "combined_fastas = {}\n",
    "for marker in (\"ssu\", \"lsu\", \"its\"):\n",
    "    tar_path = DOWNLOAD_DIR / DB_FILES[marker]\n",
    "    out_fasta = EXTRACT_DIR / f\"{marker}_combined.fasta\"\n",
    "    print(f\"[EXTRACT] {marker} from {tar_path} -> {out_fasta}\")\n",
    "    ok, nseq = extract_and_concatenate_fastas(tar_path, out_fasta, max_records=MAX_RECORDS_PER_MARKER)\n",
    "    if ok:\n",
    "        print(f\"  -> wrote approx {nseq} sequences to {out_fasta}\")\n",
    "        combined_fastas[marker] = out_fasta\n",
    "    else:\n",
    "        print(f\"  -> extraction failed for {marker}\")\n",
    "        combined_fastas[marker] = None\n",
    "\n",
    "# Quick summary: counts by marker\n",
    "for k, p in combined_fastas.items():\n",
    "    if p and p.exists():\n",
    "        with open(p) as fh:\n",
    "            seq_count = sum(1 for line in fh if line.startswith(\">\"))\n",
    "        print(f\"{k} combined FASTA: {p} (sequences: {seq_count})\")\n",
    "    else:\n",
    "        print(f\"{k} combined FASTA: not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64d7598d-c09a-4aa3-9427-86a81bd5d5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[META] ssu: parsed 12 entries -> DataFrame columns: ['accession', 'organism', 'taxid', 'lineage', 'raw']\n",
      "[META] lsu: parsed 12 entries -> DataFrame columns: ['accession', 'organism', 'taxid', 'lineage', 'raw']\n",
      "[META] its: parsed 12 entries -> DataFrame columns: ['accession', 'organism', 'taxid', 'lineage', 'raw']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accession</th>\n",
       "      <th>organism</th>\n",
       "      <th>taxid</th>\n",
       "      <th>lineage</th>\n",
       "      <th>raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dbname</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>SSU_eukaryote_rRNA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>version</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  accession organism taxid lineage                 raw\n",
       "0    dbname     None  None    None  SSU_eukaryote_rRNA\n",
       "1   version     None  None    None                 1.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 4: Load NCBI-provided metadata JSONs and construct a flexible DataFrame mapping accession -> taxonomy info\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "META_DIR = DOWNLOAD_DIR\n",
    "meta_paths = {\n",
    "    \"ssu\": META_DIR / DB_FILES[\"ssu_meta\"],\n",
    "    \"lsu\": META_DIR / DB_FILES[\"lsu_meta\"],\n",
    "    \"its\": META_DIR / DB_FILES[\"its_meta\"]\n",
    "}\n",
    "\n",
    "def load_json_if_exists(p: Path):\n",
    "    if p.exists():\n",
    "        try:\n",
    "            with open(p, \"r\") as fh:\n",
    "                return json.load(fh)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] failed to read {p}: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"[WARN] metadata file not found: {p}\")\n",
    "        return None\n",
    "\n",
    "# Load\n",
    "metadata = {k: load_json_if_exists(path) for k, path in meta_paths.items()}\n",
    "\n",
    "# The metadata format varies; we'll attempt to extract accession and any taxonomic lineage fields robustly.\n",
    "def parse_metadata_list(meta_list):\n",
    "    \"\"\"Return a list of dicts with keys: accession, organism, tax_lineage (list/str), taxid, extra.\"\"\"\n",
    "    out = []\n",
    "    if not meta_list:\n",
    "        return out\n",
    "    # meta_list could be dict or list depending on file - handle both\n",
    "    if isinstance(meta_list, dict):\n",
    "        # sometimes these JSONs are dict-of-accession->metadata\n",
    "        iterable = meta_list.items()\n",
    "    else:\n",
    "        iterable = enumerate(meta_list)\n",
    "    for key, entry in iterable:\n",
    "        e = {}\n",
    "        # entry may be dict with various structures\n",
    "        if isinstance(entry, dict):\n",
    "            # common fields (try many)\n",
    "            e['accession'] = entry.get('accession_version') or entry.get('accession') or entry.get('seqid') or entry.get('id') or key\n",
    "            e['organism'] = entry.get('organism') or entry.get('scientific_name') or entry.get('species') or entry.get('organism_name')\n",
    "            e['taxid'] = entry.get('taxid') or entry.get('tax_id') or entry.get('taxonomic_id')\n",
    "            # lineage may be a string or list\n",
    "            lineage = entry.get('lineage') or entry.get('taxonomy') or entry.get('taxonomic_lineage') or entry.get('taxonomic_lineage_names')\n",
    "            e['lineage_raw'] = lineage\n",
    "            # if lineage is a string, attempt split by ';' or '|'\n",
    "            if isinstance(lineage, str):\n",
    "                if ';' in lineage:\n",
    "                    e['lineage'] = [x.strip() for x in lineage.split(';') if x.strip()]\n",
    "                elif '|' in lineage:\n",
    "                    e['lineage'] = [x.strip() for x in lineage.split('|') if x.strip()]\n",
    "                else:\n",
    "                    e['lineage'] = [lineage.strip()]\n",
    "            elif isinstance(lineage, list):\n",
    "                e['lineage'] = lineage\n",
    "            else:\n",
    "                e['lineage'] = None\n",
    "            # store raw entry for provenance\n",
    "            e['raw'] = entry\n",
    "        else:\n",
    "            e['accession'] = key\n",
    "            e['organism'] = None\n",
    "            e['taxid'] = None\n",
    "            e['lineage'] = None\n",
    "            e['raw'] = entry\n",
    "        out.append(e)\n",
    "    return out\n",
    "\n",
    "# Parse each metadata\n",
    "parsed_meta = {}\n",
    "for k, meta in metadata.items():\n",
    "    parsed = parse_metadata_list(meta)\n",
    "    parsed_meta[k] = pd.DataFrame(parsed)\n",
    "    print(f\"[META] {k}: parsed {len(parsed)} entries -> DataFrame columns: {parsed_meta[k].columns.tolist()}\")\n",
    "\n",
    "# Example preview for one (if available)\n",
    "for k in parsed_meta:\n",
    "    if len(parsed_meta[k])>0:\n",
    "        display(parsed_meta[k].head(2))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb8b23f1-c1ae-4c54-838b-da9359085927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOWNLOAD_DIR found from notebook variable.\n",
      "\n",
      "DOWNLOAD_DIR: ncbi_blast_db\n",
      "Exists: True\n",
      "\n",
      "'extracted' folder: ncbi_blast_db\\extracted\n",
      "exists: True\n",
      "\n",
      "Files inside extracted/:\n",
      "  best_shared_heads.pt (size: 1014743 bytes)\n",
      "  best_shared_heads_defensive.pt (size: 452813 bytes)\n",
      "  best_shared_heads_defensive_state_dict.pt (size: 452693 bytes)\n",
      "  best_shared_heads_labeled.pt (size: 1212295 bytes)\n",
      "  best_shared_heads_pseudo_tensordataset_fix.pt (size: 1557749 bytes)\n",
      "  best_shared_heads_resumed.pt (size: 1359303 bytes)\n",
      "  best_shared_heads_resumed_state_dict.pt (size: 452645 bytes)\n",
      "  best_shared_heads_retrain.pt (size: 1015687 bytes)\n",
      "  calibration_bins_class.csv (size: 667 bytes)\n",
      "  calibration_bins_family.csv (size: 667 bytes)\n",
      "  calibration_bins_genus.csv (size: 668 bytes)\n",
      "  calibration_bins_kingdom.csv (size: 665 bytes)\n",
      "  calibration_bins_order.csv (size: 667 bytes)\n",
      "  calibration_bins_phylum.csv (size: 667 bytes)\n",
      "  calibration_bins_species.csv (size: 651 bytes)\n",
      "  calibration_metrics_by_rank.csv (size: 814 bytes)\n",
      "  cluster_summary.csv (size: 2764 bytes)\n",
      "  cluster_summary.json (size: 1130 bytes)\n",
      "  confusion_matrix_class.csv (size: 456 bytes)\n",
      "  confusion_matrix_family.csv (size: 1289 bytes)\n",
      "  confusion_matrix_genus.csv (size: 2242 bytes)\n",
      "  confusion_matrix_kingdom.csv (size: 62 bytes)\n",
      "  confusion_matrix_order.csv (size: 674 bytes)\n",
      "  confusion_matrix_phylum.csv (size: 156 bytes)\n",
      "  confusion_matrix_species.csv (size: 73726 bytes)\n",
      "  embeddings.npy (size: 1308288 bytes)\n",
      "  embeddings_meta.csv (size: 117236 bytes)\n",
      "  embeddings_meta_clustered.csv (size: 2093866 bytes)\n",
      "  embeddings_meta_clustered.json (size: 3563329 bytes)\n",
      "  embeddings_meta_pca.csv (size: 1953351 bytes)\n",
      "  embeddings_pca.npy (size: 654208 bytes)\n",
      "  evaluation_summary_by_rank.csv (size: 515 bytes)\n",
      "  fetched_summary.json (size: 2 bytes)\n",
      "  its_combined.fasta (size: 515 bytes)\n",
      "  its_fetched.fasta (size: 612802 bytes)\n",
      "  its_fetched_metadata.json (size: 440295 bytes)\n",
      "  label_assignment_debug.csv (size: 269385 bytes)\n",
      "  label_assignment_debug_final.csv (size: 2374549 bytes)\n",
      "  label_encoders_final.pkl (size: 61723 bytes)\n",
      "  label_encoders_rebuilt.pkl (size: 31766 bytes)\n",
      "  label_encoders_rebuilt_v2.pkl (size: 31782 bytes)\n",
      "  label_encoders_used.pkl (size: 31766 bytes)\n",
      "  lsu_combined.fasta (size: 23 bytes)\n",
      "  lsu_fetched.fasta (size: 67876422 bytes)\n",
      "  lsu_fetched_metadata.json (size: 262571 bytes)\n",
      "  metrics_defensive.json (size: 768 bytes)\n",
      "  metrics_labeled.json (size: 1164 bytes)\n",
      "  metrics_resumed.json (size: 767 bytes)\n",
      "  novel_candidates.csv (size: 8756 bytes)\n",
      "  novel_candidates_priority.csv (size: 8105 bytes)\n",
      "  per_class_metrics_class.csv (size: 393 bytes)\n",
      "  per_class_metrics_family.csv (size: 692 bytes)\n",
      "  per_class_metrics_genus.csv (size: 939 bytes)\n",
      "  per_class_metrics_kingdom.csv (size: 144 bytes)\n",
      "  per_class_metrics_order.csv (size: 470 bytes)\n",
      "  per_class_metrics_phylum.csv (size: 312 bytes)\n",
      "  per_class_metrics_species.csv (size: 7410 bytes)\n",
      "  predictions.jsonl (size: 6798859 bytes)\n",
      "  predictions.jsonl.bak (size: 6847594 bytes)\n",
      "  predictions_calibrated.csv (size: 728625 bytes)\n",
      "  predictions_calibrated.jsonl (size: 5385598 bytes)\n",
      "  predictions_raw.csv (size: 670026 bytes)\n",
      "  predictions_raw.jsonl (size: 5290019 bytes)\n",
      "  predictions_summary.csv (size: 706038 bytes)\n",
      "  predictions_summary.csv.bak (size: 746180 bytes)\n",
      "  predictions_summary_calibrated.csv (size: 328629 bytes)\n",
      "  predictions_with_mc_uncertainty.csv (size: 2901863 bytes)\n",
      "  predictions_with_mc_uncertainty.jsonl (size: 3491060 bytes)\n",
      "  predictions_with_uncertainty.csv (size: 325705 bytes)\n",
      "  reliability_class.png (size: 51649 bytes)\n",
      "  reliability_family.png (size: 51008 bytes)\n",
      "  reliability_genus.png (size: 50689 bytes)\n",
      "  reliability_kingdom.png (size: 56196 bytes)\n",
      "  reliability_order.png (size: 51886 bytes)\n",
      "  reliability_phylum.png (size: 55414 bytes)\n",
      "  reliability_species.png (size: 50506 bytes)\n",
      "  shared_heads_initial.pt (size: 337765 bytes)\n",
      "  species_topk_accuracy.csv (size: 37 bytes)\n",
      "  ssu_combined.fasta (size: 23 bytes)\n",
      "  ssu_fetched.fasta (size: 86980019 bytes)\n",
      "  ssu_fetched_metadata.json (size: 283597 bytes)\n",
      "  temp_scaling_by_rank.json (size: 217 bytes)\n",
      "  train_idx_by_acc.npy (size: 17584 bytes)\n",
      "  train_idx_final.npy (size: 8828 bytes)\n",
      "  train_val_split_by_accession.json (size: 22765 bytes)\n",
      "  training_history.csv (size: 2676 bytes)\n",
      "  training_history.json (size: 7673 bytes)\n",
      "  training_history_defensive.csv (size: 448 bytes)\n",
      "  training_history_labeled.csv (size: 18317 bytes)\n",
      "  training_history_pseudo_tensordataset_fix.csv (size: 2702 bytes)\n",
      "  training_history_resumed.csv (size: 2166 bytes)\n",
      "  training_history_retrain.csv (size: 1116 bytes)\n",
      "  val_idx_by_acc.npy (size: 3112 bytes)\n",
      "  val_idx_final.npy (size: 1648 bytes)\n",
      "  val_predictions_calibrated.csv (size: 160688 bytes)\n",
      "  y_encoded_final_class.npy (size: 10348 bytes)\n",
      "  y_encoded_final_family.npy (size: 10348 bytes)\n",
      "  y_encoded_final_genus.npy (size: 10348 bytes)\n",
      "  y_encoded_final_kingdom.npy (size: 10348 bytes)\n",
      "  y_encoded_final_order.npy (size: 10348 bytes)\n",
      "  y_encoded_final_phylum.npy (size: 10348 bytes)\n",
      "  y_encoded_final_species.npy (size: 10348 bytes)\n",
      "  y_encoded_rebuilt_class.npy (size: 10348 bytes)\n",
      "  y_encoded_rebuilt_family.npy (size: 10348 bytes)\n",
      "  y_encoded_rebuilt_genus.npy (size: 10348 bytes)\n",
      "  y_encoded_rebuilt_kingdom.npy (size: 10348 bytes)\n",
      "  y_encoded_rebuilt_order.npy (size: 10348 bytes)\n",
      "  y_encoded_rebuilt_phylum.npy (size: 10348 bytes)\n",
      "  y_encoded_rebuilt_species.npy (size: 10348 bytes)\n",
      "  ssu_combined.fasta        exists: True  size: 23\n",
      "  lsu_combined.fasta        exists: True  size: 23\n",
      "  its_combined.fasta        exists: True  size: 515\n"
     ]
    }
   ],
   "source": [
    "# Cell A — Diagnostic\n",
    "from pathlib import Path\n",
    "import os, sys, textwrap\n",
    "\n",
    "# 1) Check whether DOWNLOAD_DIR exists in the current notebook state\n",
    "try:\n",
    "    download_dir = DOWNLOAD_DIR  # use your notebook's DOWNLOAD_DIR variable\n",
    "    print(\"DOWNLOAD_DIR found from notebook variable.\")\n",
    "except NameError:\n",
    "    download_dir = None\n",
    "    print(\"DOWNLOAD_DIR is NOT defined in the current notebook namespace.\")\n",
    "\n",
    "# If not found, try common fallback locations (only for diagnostic; won't overwrite)\n",
    "if download_dir is None:\n",
    "    print(\"\\nAttempting to auto-detect possible download directories (diagnostic only):\")\n",
    "    candidates = []\n",
    "    cwd = Path.cwd()\n",
    "    candidates.extend([cwd, Path(\"/mnt/data\")])\n",
    "    for c in candidates:\n",
    "        if c.exists():\n",
    "            print(\"  candidate:\", c)\n",
    "    # do not set DOWNLOAD_DIR automatically; just show candidates\n",
    "else:\n",
    "    download_dir = Path(download_dir)\n",
    "    print(\"\\nDOWNLOAD_DIR:\", download_dir)\n",
    "    print(\"Exists:\", download_dir.exists())\n",
    "    # show 'extracted' subdir\n",
    "    extracted = download_dir / \"extracted\"\n",
    "    print(\"\\n'extracted' folder:\", extracted)\n",
    "    print(\"exists:\", extracted.exists())\n",
    "    if extracted.exists():\n",
    "        print(\"\\nFiles inside extracted/:\")\n",
    "        for p in sorted(extracted.iterdir()):\n",
    "            print(\" \", p.name, \"(size:\", p.stat().st_size, \"bytes)\")\n",
    "    else:\n",
    "        print(\"\\nNo extracted/ folder found at that path.\")\n",
    "\n",
    "    # show whether the combined fasta files exist\n",
    "    for m in (\"ssu\",\"lsu\",\"its\"):\n",
    "        p = extracted / f\"{m}_combined.fasta\"\n",
    "        print(f\"  {p.name:25} exists: {p.exists()}  size:\", p.stat().st_size if p.exists() else \"N/A\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79b93854-be9b-4e7f-8bd9-057ac6e04b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 candidate FASTA files under ncbi_blast_db:\n",
      "  extracted\\its_fetched.fasta\n",
      "  extracted\\lsu_fetched.fasta\n",
      "  extracted\\ssu_fetched.fasta\n",
      "\n",
      "Creating combined file for marker 'ssu': ncbi_blast_db\\extracted\\ssu_combined.fasta\n",
      "  adding extracted\\ssu_fetched.fasta\n",
      "  written 468 sequence headers to ncbi_blast_db\\extracted\\ssu_combined.fasta (approx).\n",
      "\n",
      "Creating combined file for marker 'lsu': ncbi_blast_db\\extracted\\lsu_combined.fasta\n",
      "  adding extracted\\lsu_fetched.fasta\n",
      "  written 406 sequence headers to ncbi_blast_db\\extracted\\lsu_combined.fasta (approx).\n",
      "\n",
      "Creating combined file for marker 'its': ncbi_blast_db\\extracted\\its_combined.fasta\n",
      "  adding extracted\\its_fetched.fasta\n",
      "  written 699 sequence headers to ncbi_blast_db\\extracted\\its_combined.fasta (approx).\n",
      "\n",
      "Done. Re-run your Word2Vec training cell now (the cell that previously raised 'Empty k-mer corpus').\n"
     ]
    }
   ],
   "source": [
    "# Cell B — Auto-combine FASTA sources into the expected combined files\n",
    "from pathlib import Path\n",
    "import gzip, shutil, sys, os\n",
    "\n",
    "# Use the notebook's DOWNLOAD_DIR — abort if missing to avoid creating paths unexpectedly\n",
    "try:\n",
    "    download_dir = Path(DOWNLOAD_DIR)\n",
    "except NameError:\n",
    "    raise RuntimeError(\"DOWNLOAD_DIR is not defined in this notebook. Run the earlier cells that set DOWNLOAD_DIR, then re-run this cell.\")\n",
    "\n",
    "if not download_dir.exists():\n",
    "    raise RuntimeError(f\"DOWNLOAD_DIR path does not exist: {download_dir}\")\n",
    "\n",
    "extracted_dir = download_dir / \"extracted\"\n",
    "extracted_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# search for FASTA-like files recursively under DOWNLOAD_DIR\n",
    "fasta_patterns = (\"*.fasta\",\"*.fa\",\"*.fna\",\"*.fasta.gz\",\"*.fa.gz\",\"*.fna.gz\")\n",
    "all_fasta_files = []\n",
    "for pat in fasta_patterns:\n",
    "    all_fasta_files.extend([p for p in download_dir.rglob(pat) if p.is_file()])\n",
    "\n",
    "# remove any combined files that already exist in extracted (we will consider them as sources if needed)\n",
    "# but prefer source files outside the combined names\n",
    "all_fasta_files = [p for p in sorted(set(all_fasta_files)) if not p.name.endswith((\"_combined.fasta\",\"_combined.fa\",\"_combined.fna\"))]\n",
    "\n",
    "print(f\"Found {len(all_fasta_files)} candidate FASTA files under {download_dir}:\")\n",
    "for p in all_fasta_files:\n",
    "    print(\" \", p.relative_to(download_dir))\n",
    "\n",
    "if len(all_fasta_files) == 0:\n",
    "    print(\"\\nNo FASTA files found under DOWNLOAD_DIR. Nothing to combine. You must run the earlier extraction/download step or place FASTA files under DOWNLOAD_DIR.\")\n",
    "else:\n",
    "    # group files by marker name if their filename contains the marker, else list them as 'unassigned'\n",
    "    markers = (\"ssu\",\"lsu\",\"its\")\n",
    "    grouped = {m: [] for m in markers}\n",
    "    unassigned = []\n",
    "    for p in all_fasta_files:\n",
    "        name = p.name.lower()\n",
    "        found = False\n",
    "        for m in markers:\n",
    "            if m in name:\n",
    "                grouped[m].append(p)\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            unassigned.append(p)\n",
    "\n",
    "    # If some markers received no files but there are unassigned files, we will NOT forcefully assign them.\n",
    "    # Instead, present the situation and combine only where we have explicit matches.\n",
    "    for m in markers:\n",
    "        files_for_m = grouped[m]\n",
    "        if not files_for_m:\n",
    "            print(f\"\\nNo source FASTA files detected for marker '{m}'. Will NOT create {m}_combined.fasta.\")\n",
    "            continue\n",
    "\n",
    "        dest = extracted_dir / f\"{m}_combined.fasta\"\n",
    "        print(f\"\\nCreating combined file for marker '{m}': {dest}\")\n",
    "        with open(dest, \"wb\") as outfh:\n",
    "            total_seqs = 0\n",
    "            for src in files_for_m:\n",
    "                print(\"  adding\", src.relative_to(download_dir))\n",
    "                # open gzipped or plain\n",
    "                if src.suffix == \".gz\" or src.name.endswith(\".fasta.gz\") or src.name.endswith(\".fa.gz\") or src.name.endswith(\".fna.gz\"):\n",
    "                    opener = gzip.open\n",
    "                    mode = \"rt\"\n",
    "                else:\n",
    "                    opener = open\n",
    "                    mode = \"r\"\n",
    "                with opener(src, mode) as inf:\n",
    "                    # write text as bytes\n",
    "                    for line in inf:\n",
    "                        # count headers as sequences (approx)\n",
    "                        if line.startswith(\">\"):\n",
    "                            total_seqs += 1\n",
    "                        outfh.write(line.encode() if isinstance(line, str) else line)\n",
    "            print(f\"  written {total_seqs} sequence headers to {dest} (approx).\")\n",
    "\n",
    "    if unassigned:\n",
    "        print(\"\\nThe following FASTA files did NOT match any marker name (ssu/lsu/its).\")\n",
    "        for p in unassigned:\n",
    "            print(\"  \", p.relative_to(download_dir))\n",
    "        print(\"If those should be part of a marker, rename them to include 'ssu','lsu' or 'its' in the filename, or move them into a folder whose name contains the marker.\")\n",
    "\n",
    "print(\"\\nDone. Re-run your Word2Vec training cell now (the cell that previously raised 'Empty k-mer corpus').\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37c805dd-faa0-48f3-b3ef-d60576569c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CORPUS] fasta paths used: [WindowsPath('ncbi_blast_db/extracted/ssu_combined.fasta'), WindowsPath('ncbi_blast_db/extracted/lsu_combined.fasta'), WindowsPath('ncbi_blast_db/extracted/its_combined.fasta')]\n",
      "[CORPUS] total sequences (kmers) discovered (preview cap 10000): 1573\n",
      "Example k-mers from first sequence: ['TTATAC', 'TATACC', 'ATACCG', 'TACCGT', 'ACCGTG', 'CCGTGA', 'CGTGAA', 'GTGAAA', 'TGAAAC', 'GAAACT']\n"
     ]
    }
   ],
   "source": [
    "# Cell C — Sanity check: list fasta paths and count sequences found by the fasta_kmer_generator\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# reuse the exact same variables and paths as your original cell\n",
    "download_dir = Path(DOWNLOAD_DIR)\n",
    "extracted_dir = download_dir / \"extracted\"\n",
    "\n",
    "fasta_paths = []\n",
    "for m in (\"ssu\", \"lsu\", \"its\"):\n",
    "    p = extracted_dir / f\"{m}_combined.fasta\"\n",
    "    if p.exists():\n",
    "        fasta_paths.append(p)\n",
    "print(\"[CORPUS] fasta paths used:\", fasta_paths)\n",
    "\n",
    "# quick k-mer generator (same as your code)\n",
    "K = 6\n",
    "def seq_to_kmers(seq, k=K):\n",
    "    s = seq.strip().upper()\n",
    "    return [s[i:i+k] for i in range(len(s)-k+1) if 'N' not in s[i:i+k]]\n",
    "\n",
    "def fasta_kmer_generator(fasta_paths, k=K):\n",
    "    for p in fasta_paths:\n",
    "        if not p or not Path(p).exists():\n",
    "            continue\n",
    "        with open(p) as fh:\n",
    "            seq = \"\"\n",
    "            for line in fh:\n",
    "                if line.startswith(\">\"):\n",
    "                    if seq:\n",
    "                        yield seq_to_kmers(seq, k=k)\n",
    "                    seq = \"\"\n",
    "                else:\n",
    "                    seq += line.strip()\n",
    "            if seq:\n",
    "                yield seq_to_kmers(seq, k=k)\n",
    "\n",
    "# count sequences found (cap to show first 5)\n",
    "corpus_preview = []\n",
    "for i, kmers in enumerate(fasta_kmer_generator(fasta_paths, k=K)):\n",
    "    if kmers:\n",
    "        corpus_preview.append(kmers)\n",
    "    if i >= 9999:\n",
    "        break\n",
    "print(\"[CORPUS] total sequences (kmers) discovered (preview cap 10000):\", len(corpus_preview))\n",
    "if len(corpus_preview):\n",
    "    print(\"Example k-mers from first sequence:\", corpus_preview[0][:10])\n",
    "else:\n",
    "    print(\"No sequences found. If this prints, the combined FASTA files are still missing or empty.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8fc6811-22f5-4d3f-ba16-37c57445f375",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CORPUS] fasta paths used: [WindowsPath('ncbi_blast_db/extracted/ssu_combined.fasta'), WindowsPath('ncbi_blast_db/extracted/lsu_combined.fasta'), WindowsPath('ncbi_blast_db/extracted/its_combined.fasta')]\n",
      "[CORPUS] total sequences (kmers) in corpus (scanned): 1573\n",
      "[W2V] loading existing model: ncbi_blast_db\\kmer_w2v_k6.model\n",
      "[W2V] example k-mer: TTATAC vector_len: 128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 5 (robust streaming, avoids OOM): Build k-mer corpus from combined FASTAs and train Word2Vec embeddings (k=6 default).\n",
    "# Installs gensim automatically if missing.\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "from collections import deque\n",
    "\n",
    "# Ensure gensim present\n",
    "try:\n",
    "    import gensim\n",
    "    from gensim.models import Word2Vec\n",
    "except Exception:\n",
    "    print(\"[INSTALL] gensim not found; installing gensim...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"gensim==4.3.1\"])\n",
    "    import gensim\n",
    "    from gensim.models import Word2Vec\n",
    "\n",
    "# Parameters (kept identical to your original names)\n",
    "K = 6\n",
    "W2V_VECTOR_SIZE = 128\n",
    "W2V_WINDOW = 4\n",
    "W2V_MIN_COUNT = 2\n",
    "W2V_EPOCHS = 8\n",
    "\n",
    "# New small constant to limit per-yield memory (buffer size of kmers yielded at once).\n",
    "# Lower this to reduce peak memory further (500 is very conservative); keep at 1000 by default.\n",
    "CHUNK_SIZE = 1000\n",
    "\n",
    "def fasta_kmer_generator(fasta_paths, k=K, chunk_size=CHUNK_SIZE):\n",
    "    \"\"\"\n",
    "    Stream k-mers from fasta files without building entire sequences.\n",
    "    Yields small lists (length <= chunk_size) of k-mers.\n",
    "    \"\"\"\n",
    "    for p in fasta_paths:\n",
    "        if not p or not p.exists():\n",
    "            continue\n",
    "        # open and stream file; maintain a sliding window per sequence\n",
    "        with open(p, 'r', errors='replace') as fh:\n",
    "            window = deque()  # holds up to k characters\n",
    "            buffer = []       # holds up to chunk_size k-mers to yield\n",
    "            for raw in fh:\n",
    "                if raw.startswith(\">\"):\n",
    "                    # new header -> flush buffer and reset window\n",
    "                    if buffer:\n",
    "                        yield buffer\n",
    "                        buffer = []\n",
    "                    window.clear()\n",
    "                    continue\n",
    "                line = raw.strip().upper()\n",
    "                if not line:\n",
    "                    continue\n",
    "                for ch in line:\n",
    "                    if ch == 'N':\n",
    "                        # ambiguous base -> reset window (k-mers cannot cross this)\n",
    "                        window.clear()\n",
    "                        continue\n",
    "                    window.append(ch)\n",
    "                    if len(window) == k:\n",
    "                        # form kmer\n",
    "                        kmer = ''.join(window)\n",
    "                        buffer.append(kmer)\n",
    "                        # slide window by 1\n",
    "                        window.popleft()\n",
    "                        # flush buffer if it reached chunk_size\n",
    "                        if len(buffer) >= chunk_size:\n",
    "                            yield buffer\n",
    "                            buffer = []\n",
    "            # End of file: flush any remaining buffer\n",
    "            if buffer:\n",
    "                yield buffer\n",
    "            # ensure window cleared at sequence boundary (header logic above handled)\n",
    "            # proceed to next file\n",
    "\n",
    "class CorpusIterable:\n",
    "    \"\"\"Re-iterable, low-memory corpus wrapper that yields small k-mer chunks per iteration.\n",
    "       __len__ scans files incrementally and counts sequences that would produce at least one valid k-mer.\n",
    "    \"\"\"\n",
    "    def __init__(self, fasta_paths, k=K, chunk_size=CHUNK_SIZE):\n",
    "        self.fasta_paths = list(fasta_paths)\n",
    "        self.k = k\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        return fasta_kmer_generator(self.fasta_paths, k=self.k, chunk_size=self.chunk_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Count sequences that yield at least one valid k-mer by scanning files without accumulating sequences\n",
    "        cnt = 0\n",
    "        for p in self.fasta_paths:\n",
    "            if not p or not p.exists():\n",
    "                continue\n",
    "            with open(p, 'r', errors='replace') as fh:\n",
    "                window = deque()\n",
    "                seen_for_seq = False\n",
    "                for raw in fh:\n",
    "                    if raw.startswith(\">\"):\n",
    "                        if seen_for_seq:\n",
    "                            cnt += 1\n",
    "                        seen_for_seq = False\n",
    "                        window.clear()\n",
    "                        continue\n",
    "                    line = raw.strip().upper()\n",
    "                    if not line:\n",
    "                        continue\n",
    "                    for ch in line:\n",
    "                        if ch == 'N':\n",
    "                            window.clear()\n",
    "                            continue\n",
    "                        window.append(ch)\n",
    "                        if len(window) == self.k:\n",
    "                            # we found at least one valid k-mer in this sequence\n",
    "                            seen_for_seq = True\n",
    "                            # slide to continue scanning without building strings\n",
    "                            window.popleft()\n",
    "                # EOF for this file: count last sequence if we saw any valid k-mer\n",
    "                if seen_for_seq:\n",
    "                    cnt += 1\n",
    "        return cnt\n",
    "\n",
    "# Collect list of combined FASTA paths from previous cells (kept identical)\n",
    "fasta_paths = []\n",
    "for m in (\"ssu\", \"lsu\", \"its\"):\n",
    "    p = DOWNLOAD_DIR / \"extracted\" / f\"{m}_combined.fasta\"\n",
    "    if p.exists():\n",
    "        fasta_paths.append(p)\n",
    "print(\"[CORPUS] fasta paths used:\", fasta_paths)\n",
    "\n",
    "# streaming corpus object (no materialized list)\n",
    "corpus = CorpusIterable(fasta_paths, k=K, chunk_size=CHUNK_SIZE)\n",
    "\n",
    "# keep max_seqs_to_collect for compatibility (not used to avoid materialization)\n",
    "max_seqs_to_collect = 10000\n",
    "\n",
    "print(\"[CORPUS] total sequences (kmers) in corpus (scanned):\", len(corpus))\n",
    "\n",
    "# Train or load Word2Vec model\n",
    "w2v_model_path = DOWNLOAD_DIR / f\"kmer_w2v_k{K}.model\"\n",
    "if w2v_model_path.exists():\n",
    "    print(\"[W2V] loading existing model:\", w2v_model_path)\n",
    "    w2v = Word2Vec.load(str(w2v_model_path))\n",
    "else:\n",
    "    if len(corpus) == 0:\n",
    "        raise RuntimeError(\"Empty k-mer corpus. Ensure combined FASTAs were extracted and not empty.\")\n",
    "    print(\"[W2V] training Word2Vec (streaming, memory-safe)...\")\n",
    "    # Use the streaming corpus directly (no materialized list)\n",
    "    # Note: Word2Vec will iterate corpus multiple times internally; our corpus is re-iterable.\n",
    "    w2v = Word2Vec(sentences=corpus, vector_size=W2V_VECTOR_SIZE, window=W2V_WINDOW,\n",
    "                   min_count=W2V_MIN_COUNT, workers=max(1, (os.cpu_count() or 1)-1),\n",
    "                   seed=42, epochs=W2V_EPOCHS)\n",
    "    w2v.save(str(w2v_model_path))\n",
    "    print(\"[W2V] saved model to\", w2v_model_path)\n",
    "\n",
    "# Quick sanity check: vector for a sample kmer\n",
    "sample_kmer = None\n",
    "for seq_kmers in corpus:\n",
    "    if len(seq_kmers) > 0:\n",
    "        sample_kmer = seq_kmers[0]\n",
    "        break\n",
    "\n",
    "if sample_kmer and sample_kmer in w2v.wv:\n",
    "    print(\"[W2V] example k-mer:\", sample_kmer, \"vector_len:\", w2v.vector_size)\n",
    "else:\n",
    "    print(\"[W2V] example k-mer not in vocabulary (common if min_count> observed).\")\n",
    "\n",
    "# final cleanup\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cdab9c6-bcff-48a2-a576-d57209344002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STATUS] existing extracted fasta counts: {'ssu': 468, 'lsu': 406, 'its': 699}\n",
      "[SKIP FETCH] ssu has 468 records (>= 200)\n",
      "[SKIP FETCH] lsu has 406 records (>= 200)\n",
      "[SKIP FETCH] its has 699 records (>= 200)\n",
      "[CELL6 COMPLETE] fetched counts: {}\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Fetch representative sequences and taxonomy from NCBI Entrez if extracted FASTAs are small.\n",
    "# Defensive: auto-install Biopython if missing, robust retries, saves outputs to DOWNLOAD_DIR/extracted/\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import json\n",
    "import math\n",
    "\n",
    "# Ensure Biopython installed\n",
    "import subprocess, sys\n",
    "try:\n",
    "    from Bio import Entrez, SeqIO\n",
    "except Exception:\n",
    "    print(\"[INSTALL] Installing biopython...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"biopython\"])\n",
    "    from Bio import Entrez, SeqIO\n",
    "\n",
    "# Config - change ENTREZ_EMAIL to a real email if you plan to fetch many records.\n",
    "ENTREZ_EMAIL = \"demoservice654@gmail.com\"   # <-- replace with your email if you like\n",
    "ENTREZ_API_KEY =\"b39eb52d13753525a46134bc1ed3ac2fa708\"  # set to a string if you have one; keeps requests higher-rate\n",
    "\n",
    "Entrez.email = ENTREZ_EMAIL\n",
    "if ENTREZ_API_KEY:\n",
    "    Entrez.api_key = ENTREZ_API_KEY\n",
    "\n",
    "EXTRACT_DIR = DOWNLOAD_DIR / \"extracted\"\n",
    "EXTRACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Read the combined FASTA counts from previous run\n",
    "def count_fasta_records(p: Path):\n",
    "    if not p.exists():\n",
    "        return 0\n",
    "    with open(p) as fh:\n",
    "        return sum(1 for line in fh if line.startswith(\">\"))\n",
    "\n",
    "combined_paths = {m: EXTRACT_DIR / f\"{m}_combined.fasta\" for m in (\"ssu\", \"lsu\", \"its\")}\n",
    "existing_counts = {m: count_fasta_records(p) for m, p in combined_paths.items()}\n",
    "print(\"[STATUS] existing extracted fasta counts:\", existing_counts)\n",
    "\n",
    "# If counts are small, we'll fetch sequences from NCBI (safe defaults)\n",
    "FETCH_IF_LESS_THAN = 200  # if existing sequences < this, fetch from Entrez\n",
    "FETCH_PER_MARKER = 700    # target number to fetch per marker (you can reduce to 200-500 if rate-limited)\n",
    "FETCH_BATCH = 200         # fetch in batches to avoid overloading responses\n",
    "\n",
    "# Search queries for markers\n",
    "search_queries = {\n",
    "    \"ssu\": '18S[All Fields] AND \"rRNA\"[All Fields] AND Eukaryota[Organism]',\n",
    "    \"lsu\": '28S[All Fields] AND \"rRNA\"[All Fields] AND Eukaryota[Organism]',\n",
    "    \"its\": 'internal transcribed spacer[All Fields] AND Eukaryota[Organism]'\n",
    "}\n",
    "\n",
    "fetched_metadata = {}\n",
    "for marker in (\"ssu\", \"lsu\", \"its\"):\n",
    "    if existing_counts.get(marker, 0) >= FETCH_IF_LESS_THAN:\n",
    "        print(f\"[SKIP FETCH] {marker} has {existing_counts[marker]} records (>= {FETCH_IF_LESS_THAN})\")\n",
    "        continue\n",
    "\n",
    "    query = search_queries[marker]\n",
    "    print(f\"[ENTREZ SEARCH] marker={marker} query={query} (fetch up to {FETCH_PER_MARKER})\")\n",
    "    # esearch\n",
    "    try:\n",
    "        handle = Entrez.esearch(db=\"nuccore\", term=query, retmax=FETCH_PER_MARKER)\n",
    "        result = Entrez.read(handle)\n",
    "        handle.close()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Entrez esearch failed for {marker}: {e}\")\n",
    "        result = {\"IdList\": []}\n",
    "\n",
    "    id_list = result.get(\"IdList\", [])\n",
    "    print(f\"  -> found {len(id_list)} ids (limiting to {FETCH_PER_MARKER})\")\n",
    "\n",
    "    if not id_list:\n",
    "        fetched_metadata[marker] = []\n",
    "        continue\n",
    "\n",
    "    # Fetch GenBank records in batches and save FASTA + structured metadata\n",
    "    out_fasta = EXTRACT_DIR / f\"{marker}_fetched.fasta\"\n",
    "    out_meta = EXTRACT_DIR / f\"{marker}_fetched_metadata.json\"\n",
    "    saved = 0\n",
    "    meta_list = []\n",
    "\n",
    "    with open(out_fasta, \"w\") as fasta_fh:\n",
    "        for start in range(0, len(id_list), FETCH_BATCH):\n",
    "            batch_ids = id_list[start:start+FETCH_BATCH]\n",
    "            ids_str = \",\".join(batch_ids)\n",
    "            print(f\"  [EFETCH] fetching {len(batch_ids)} ids (start={start})\")\n",
    "            try:\n",
    "                fh = Entrez.efetch(db=\"nuccore\", id=ids_str, rettype=\"gb\", retmode=\"text\")\n",
    "                records = SeqIO.parse(fh, \"gb\")\n",
    "                for rec in records:\n",
    "                    # write fasta\n",
    "                    seq_str = str(rec.seq)\n",
    "                    if len(seq_str) < 100:  # skip tiny sequences\n",
    "                        continue\n",
    "                    header = f\">{rec.id} {rec.annotations.get('organism','')}\\n\"\n",
    "                    fasta_fh.write(header)\n",
    "                    # wrap sequence lines 80 chars\n",
    "                    for i in range(0, len(seq_str), 80):\n",
    "                        fasta_fh.write(seq_str[i:i+80] + \"\\n\")\n",
    "                    saved += 1\n",
    "                    # extract taxonomy info from annotations\n",
    "                    tax = rec.annotations.get(\"taxonomy\", [])\n",
    "                    organism = rec.annotations.get(\"organism\", None)\n",
    "                    meta = {\n",
    "                        \"id\": rec.id,\n",
    "                        \"accession\": getattr(rec, \"name\", None) or rec.id,\n",
    "                        \"organism\": organism,\n",
    "                        \"taxonomy\": tax,\n",
    "                        \"description\": rec.description,\n",
    "                        \"seq_len\": len(seq_str)\n",
    "                    }\n",
    "                    meta_list.append(meta)\n",
    "                fh.close()\n",
    "            except Exception as e:\n",
    "                print(f\"    [WARN] efetch batch failed: {e}. Sleeping and retrying once.\")\n",
    "                time.sleep(3)\n",
    "                try:\n",
    "                    fh = Entrez.efetch(db=\"nuccore\", id=ids_str, rettype=\"gb\", retmode=\"text\")\n",
    "                    records = SeqIO.parse(fh, \"gb\")\n",
    "                    for rec in records:\n",
    "                        seq_str = str(rec.seq)\n",
    "                        if len(seq_str) < 100:\n",
    "                            continue\n",
    "                        header = f\">{rec.id} {rec.annotations.get('organism','')}\\n\"\n",
    "                        fasta_fh.write(header)\n",
    "                        for i in range(0, len(seq_str), 80):\n",
    "                            fasta_fh.write(seq_str[i:i+80] + \"\\n\")\n",
    "                        saved += 1\n",
    "                        tax = rec.annotations.get(\"taxonomy\", [])\n",
    "                        organism = rec.annotations.get(\"organism\", None)\n",
    "                        meta = {\n",
    "                            \"id\": rec.id,\n",
    "                            \"accession\": getattr(rec, \"name\", None) or rec.id,\n",
    "                            \"organism\": organism,\n",
    "                            \"taxonomy\": tax,\n",
    "                            \"description\": rec.description,\n",
    "                            \"seq_len\": len(seq_str)\n",
    "                        }\n",
    "                        meta_list.append(meta)\n",
    "                    fh.close()\n",
    "                except Exception as e2:\n",
    "                    print(f\"    [ERROR] efetch retry failed: {e2}. Skipping batch.\")\n",
    "                    continue\n",
    "            # be polite to NCBI\n",
    "            time.sleep(0.34)  # keep requests moderate (or longer if no API key)\n",
    "    print(f\"[SAVED] {saved} sequences written to {out_fasta}\")\n",
    "    # save metadata\n",
    "    with open(out_meta, \"w\") as mh:\n",
    "        json.dump(meta_list, mh, indent=2)\n",
    "    fetched_metadata[marker] = meta_list\n",
    "\n",
    "# Save a summary file\n",
    "with open(EXTRACT_DIR / \"fetched_summary.json\", \"w\") as fh:\n",
    "    json.dump({k: len(v) for k, v in fetched_metadata.items()}, fh, indent=2)\n",
    "\n",
    "print(\"[CELL6 COMPLETE] fetched counts:\", {k: len(v) for k, v in fetched_metadata.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e01bb64b-c725-4141-8213-8fb69ec68884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FASTA PATHS FOR CORPUS] [WindowsPath('ncbi_blast_db/extracted/ssu_combined.fasta'), WindowsPath('ncbi_blast_db/extracted/ssu_fetched.fasta'), WindowsPath('ncbi_blast_db/extracted/lsu_combined.fasta'), WindowsPath('ncbi_blast_db/extracted/lsu_fetched.fasta'), WindowsPath('ncbi_blast_db/extracted/its_combined.fasta'), WindowsPath('ncbi_blast_db/extracted/its_fetched.fasta')]\n",
      "  collected 1000 sequences for corpus\n",
      "  collected 2000 sequences for corpus\n",
      "  collected 3000 sequences for corpus\n",
      "[CORPUS] total sequences collected for training: 3146\n",
      "[W2V] loading existing model for update: ncbi_blast_db\\kmer_w2v_k6.model\n",
      "[W2V] building vocab (update=True) with new corpus\n",
      "[W2V] training updated model...\n",
      "[W2V] updated model saved: ncbi_blast_db\\kmer_w2v_k6.model\n",
      "[W2V] vocab size: 4874; vector_size: 128; total_examples used: 3146\n"
     ]
    }
   ],
   "source": [
    "# Fixed Cell 7: Build a re-iterable k-mer corpus (list, with cap) and update / train Word2Vec safely.\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure gensim installed and import\n",
    "try:\n",
    "    import gensim\n",
    "    from gensim.models import Word2Vec\n",
    "except Exception:\n",
    "    print(\"[INSTALL] gensim not found; installing gensim...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"gensim==4.3.1\"])\n",
    "    import gensim\n",
    "    from gensim.models import Word2Vec\n",
    "\n",
    "# Ensure required variables exist (fall back to defaults if not)\n",
    "try:\n",
    "    DOWNLOAD_DIR\n",
    "except NameError:\n",
    "    DOWNLOAD_DIR = Path(\"./ncbi_blast_db\")\n",
    "DOWNLOAD_DIR = Path(DOWNLOAD_DIR)\n",
    "EXTRACT_DIR = DOWNLOAD_DIR / \"extracted\"\n",
    "\n",
    "# k-mer / w2v params (safe defaults)\n",
    "K = globals().get(\"K\", 6)\n",
    "W2V_VECTOR_SIZE = globals().get(\"W2V_VECTOR_SIZE\", 128)\n",
    "W2V_WINDOW = globals().get(\"W2V_WINDOW\", 4)\n",
    "W2V_MIN_COUNT = globals().get(\"W2V_MIN_COUNT\", 2)\n",
    "W2V_EPOCHS = globals().get(\"W2V_EPOCHS\", 10)\n",
    "MAX_CORPUS_SEQS = 20000  # safety cap for in-memory corpus\n",
    "\n",
    "# Helper functions\n",
    "def seq_to_kmers(seq, k=K):\n",
    "    s = seq.strip().upper()\n",
    "    return [s[i:i+k] for i in range(len(s)-k+1) if 'N' not in s[i:i+k]]\n",
    "\n",
    "def fasta_kmer_gen(paths, k=K):\n",
    "    \"\"\"Generator: yields list of kmers for each sequence in given fasta paths\"\"\"\n",
    "    for p in paths:\n",
    "        if not p.exists():\n",
    "            continue\n",
    "        with open(p) as fh:\n",
    "            seq = \"\"\n",
    "            for line in fh:\n",
    "                if line.startswith(\">\"):\n",
    "                    if seq:\n",
    "                        yield seq_to_kmers(seq, k=k)\n",
    "                    seq = \"\"\n",
    "                else:\n",
    "                    seq += line.strip()\n",
    "            if seq:\n",
    "                yield seq_to_kmers(seq, k=k)\n",
    "\n",
    "# Collect fasta paths (include combined + fetched if present)\n",
    "fasta_paths = []\n",
    "for m in (\"ssu\",\"lsu\",\"its\"):\n",
    "    p_combined = EXTRACT_DIR / f\"{m}_combined.fasta\"\n",
    "    p_fetched = EXTRACT_DIR / f\"{m}_fetched.fasta\"\n",
    "    if p_combined.exists() and p_combined.stat().st_size > 0:\n",
    "        fasta_paths.append(p_combined)\n",
    "    if p_fetched.exists() and p_fetched.stat().st_size > 0:\n",
    "        fasta_paths.append(p_fetched)\n",
    "if not fasta_paths:\n",
    "    raise RuntimeError(\"No FASTA files found for corpus. Ensure previous extraction/fetch cells ran correctly.\")\n",
    "print(\"[FASTA PATHS FOR CORPUS]\", fasta_paths)\n",
    "\n",
    "# Materialize corpus as a list up to MAX_CORPUS_SEQS\n",
    "corpus_list = []\n",
    "for i, kmers in enumerate(fasta_kmer_gen(fasta_paths, k=K)):\n",
    "    if kmers:\n",
    "        corpus_list.append(kmers)\n",
    "    if (i+1) % 1000 == 0:\n",
    "        print(f\"  collected {i+1} sequences for corpus\")\n",
    "    if len(corpus_list) >= MAX_CORPUS_SEQS:\n",
    "        print(f\"[CAP] reached MAX_CORPUS_SEQS={MAX_CORPUS_SEQS}; stopping corpus collection\")\n",
    "        break\n",
    "\n",
    "print(f\"[CORPUS] total sequences collected for training: {len(corpus_list)}\")\n",
    "if len(corpus_list) == 0:\n",
    "    raise RuntimeError(\"Empty k-mer corpus after scanning FASTA files. Cannot train Word2Vec.\")\n",
    "\n",
    "# Train or update Word2Vec using the materialized corpus_list (re-iterable)\n",
    "w2v_model_path = DOWNLOAD_DIR / f\"kmer_w2v_k{K}.model\"\n",
    "workers = max(1, (os.cpu_count() or 1) - 1)\n",
    "\n",
    "if w2v_model_path.exists():\n",
    "    print(\"[W2V] loading existing model for update:\", w2v_model_path)\n",
    "    w2v = Word2Vec.load(str(w2v_model_path))\n",
    "    print(\"[W2V] building vocab (update=True) with new corpus\")\n",
    "    w2v.build_vocab(corpus_list, update=True)  # corpus_list is re-iterable (list)\n",
    "    print(\"[W2V] training updated model...\")\n",
    "    w2v.train(corpus_list, total_examples=len(corpus_list), epochs=W2V_EPOCHS)\n",
    "    w2v.save(str(w2v_model_path))\n",
    "    print(\"[W2V] updated model saved:\", w2v_model_path)\n",
    "else:\n",
    "    print(\"[W2V] training new Word2Vec model from scratch...\")\n",
    "    w2v = Word2Vec(\n",
    "        sentences=corpus_list,\n",
    "        vector_size=W2V_VECTOR_SIZE,\n",
    "        window=W2V_WINDOW,\n",
    "        min_count=W2V_MIN_COUNT,\n",
    "        workers=workers,\n",
    "        seed=42,\n",
    "        epochs=W2V_EPOCHS\n",
    "    )\n",
    "    w2v.save(str(w2v_model_path))\n",
    "    print(\"[W2V] new model saved:\", w2v_model_path)\n",
    "\n",
    "# Sanity & summary\n",
    "vocab_size = len(w2v.wv.index_to_key)\n",
    "print(f\"[W2V] vocab size: {vocab_size}; vector_size: {w2v.vector_size}; total_examples used: {len(corpus_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbfe14a0-f93b-4b6f-8996-4839a0329dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Loaded embeddings.npy: samples=2555, vec_size=128\n",
      "[INFO] fasta paths to scan for headers: [WindowsPath('ncbi_blast_db/extracted/ssu_combined.fasta'), WindowsPath('ncbi_blast_db/extracted/ssu_fetched.fasta'), WindowsPath('ncbi_blast_db/extracted/lsu_combined.fasta'), WindowsPath('ncbi_blast_db/extracted/lsu_fetched.fasta'), WindowsPath('ncbi_blast_db/extracted/its_combined.fasta'), WindowsPath('ncbi_blast_db/extracted/its_fetched.fasta')]\n",
      "[INFO] using existing metadata CSV.\n",
      "[PCA] computing PCA with n_components=64 ...\n",
      "[PCA] saved embeddings_pca.npy shape: (2555, 64)\n",
      "[SAVE] saved meta+PCA to ncbi_blast_db\\extracted\\embeddings_meta_pca.csv rows=2555 cols=70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srijit\\anaconda3\\envs\\sih\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Srijit\\anaconda3\\envs\\sih\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HDBSCAN] found clusters: 99, noise: 531\n",
      "[DONE] saved clustered metadata CSV: ncbi_blast_db\\extracted\\embeddings_meta_clustered.csv and JSON: ncbi_blast_db\\extracted\\embeddings_meta_clustered.json\n",
      "  clusters example labels (unique, truncated): [-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "  novelty min/max/mean: 0.0000/1.0000/0.2123\n"
     ]
    }
   ],
   "source": [
    "# Fixed Cell 9 (retry): Robust metadata rebuild (no fh.tell), PCA, clustering, novelty scoring, safe save\n",
    "import os, sys, csv, subprocess\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "DOWNLOAD_DIR = Path(globals().get(\"DOWNLOAD_DIR\", \"./ncbi_blast_db\"))\n",
    "EXTRACT_DIR = DOWNLOAD_DIR / \"extracted\"\n",
    "emb_npy = EXTRACT_DIR / \"embeddings.npy\"\n",
    "meta_csv = EXTRACT_DIR / \"embeddings_meta.csv\"\n",
    "pca_npy = EXTRACT_DIR / \"embeddings_pca.npy\"\n",
    "meta_pca_csv = EXTRACT_DIR / \"embeddings_meta_pca.csv\"\n",
    "clustered_csv = EXTRACT_DIR / \"embeddings_meta_clustered.csv\"\n",
    "clustered_json = EXTRACT_DIR / \"embeddings_meta_clustered.json\"\n",
    "\n",
    "# Ensure embeddings exist\n",
    "if not emb_npy.exists():\n",
    "    raise RuntimeError(f\"embeddings.npy not found at {emb_npy}. Run the embedding cell first.\")\n",
    "\n",
    "# Load embeddings\n",
    "X_full = np.load(emb_npy)\n",
    "n_samples, vec_size = X_full.shape\n",
    "print(f\"[START] Loaded embeddings.npy: samples={n_samples}, vec_size={vec_size}\")\n",
    "\n",
    "# Determine FASTA paths to read headers from\n",
    "fasta_paths = []\n",
    "for m in (\"ssu\", \"lsu\", \"its\"):\n",
    "    p_combined = EXTRACT_DIR / f\"{m}_combined.fasta\"\n",
    "    p_fetched  = EXTRACT_DIR / f\"{m}_fetched.fasta\"\n",
    "    if p_combined.exists() and p_combined.stat().st_size > 0:\n",
    "        fasta_paths.append(p_combined)\n",
    "    if p_fetched.exists() and p_fetched.stat().st_size > 0:\n",
    "        fasta_paths.append(p_fetched)\n",
    "print(\"[INFO] fasta paths to scan for headers:\", fasta_paths)\n",
    "\n",
    "# Robust header reader without using fh.tell()\n",
    "def robust_fasta_headers(paths):\n",
    "    seq_counter = 0\n",
    "    for p in paths:\n",
    "        try:\n",
    "            with open(p, \"r\", encoding=\"utf-8\", errors=\"replace\") as fh:\n",
    "                for line in fh:\n",
    "                    line = line.rstrip(\"\\n\\r\")\n",
    "                    if not line:\n",
    "                        continue\n",
    "                    if line.startswith(\">\"):\n",
    "                        seq_counter += 1\n",
    "                        header_text = line[1:].strip()\n",
    "                        if header_text == \"\":\n",
    "                            header = f\"{p.stem}generated{seq_counter}\"\n",
    "                        else:\n",
    "                            # take first token if exists\n",
    "                            parts = header_text.split()\n",
    "                            header = parts[0] if len(parts) > 0 and parts[0] != \"\" else f\"{p.stem}generated{seq_counter}\"\n",
    "                        yield header\n",
    "        except Exception as e:\n",
    "            # Skip unreadable files but warn\n",
    "            print(f\"[WARN] failed to read {p}: {e}\")\n",
    "            continue\n",
    "\n",
    "# Decide whether to rebuild metadata\n",
    "rebuild_meta = False\n",
    "if not meta_csv.exists():\n",
    "    print(\"[INFO] metadata CSV not found; will rebuild from FASTA headers.\")\n",
    "    rebuild_meta = True\n",
    "else:\n",
    "    try:\n",
    "        df_meta = pd.read_csv(meta_csv, dtype=str, keep_default_na=False, na_filter=False)\n",
    "        if len(df_meta) != n_samples:\n",
    "            print(f\"[INFO] metadata rows ({len(df_meta)}) != embeddings ({n_samples}) -> will rebuild meta.\")\n",
    "            rebuild_meta = True\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] failed to read existing metadata CSV:\", e)\n",
    "        rebuild_meta = True\n",
    "\n",
    "# Rebuild meta if needed\n",
    "if rebuild_meta:\n",
    "    headers = list(robust_fasta_headers(fasta_paths))\n",
    "    print(f\"[INFO] headers found from FASTA files: {len(headers)}\")\n",
    "    # Pad headers if fewer than embeddings\n",
    "    if len(headers) < n_samples:\n",
    "        print(f\"[WARN] found {len(headers)} headers but have {n_samples} embeddings. Will pad generated ids.\")\n",
    "    rows = []\n",
    "    for i in range(n_samples):\n",
    "        hdr = headers[i] if i < len(headers) else f\"generated_seq_{i+1}\"\n",
    "        rows.append({\"id\": hdr, \"seq_len\": \"\", \"no_kmer\": \"\"})\n",
    "    df_meta = pd.DataFrame(rows)\n",
    "    # sanitize strings\n",
    "    def sanitize_str(s):\n",
    "        try:\n",
    "            s2 = str(s)\n",
    "        except Exception:\n",
    "            s2 = \"\"\n",
    "        s2 = s2.replace(\"\\r\", \" \").replace(\"\\n\", \" \").replace(\"\\t\", \" \").replace('\"', \"''\").strip()\n",
    "        return s2\n",
    "    for col in df_meta.columns:\n",
    "        if df_meta[col].dtype == object:\n",
    "            df_meta[col] = df_meta[col].apply(sanitize_str)\n",
    "    # Save sanitized meta CSV\n",
    "    df_meta.to_csv(meta_csv, index=False, encoding='utf-8', escapechar='\\\\', quoting=csv.QUOTE_MINIMAL)\n",
    "    print(f\"[REBUILT] saved metadata CSV to {meta_csv} with {len(df_meta)} rows.\")\n",
    "else:\n",
    "    print(\"[INFO] using existing metadata CSV.\")\n",
    "    df_meta = pd.read_csv(meta_csv, dtype=str, keep_default_na=False, na_filter=False)\n",
    "\n",
    "# Compute PCA\n",
    "from sklearn.decomposition import PCA\n",
    "n_components = min(64, vec_size, max(1, n_samples - 1))\n",
    "print(f\"[PCA] computing PCA with n_components={n_components} ...\")\n",
    "pca = PCA(n_components=n_components, random_state=42)\n",
    "X_pca = pca.fit_transform(X_full)\n",
    "np.save(pca_npy, X_pca)\n",
    "print(f\"[PCA] saved embeddings_pca.npy shape: {X_pca.shape}\")\n",
    "\n",
    "# Build df_meta + PCA columns, align lengths\n",
    "pc_cols = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n",
    "df_pcas = pd.DataFrame(X_pca, columns=pc_cols)\n",
    "min_n = min(len(df_meta), df_pcas.shape[0])\n",
    "if len(df_meta) != df_pcas.shape[0]:\n",
    "    print(f\"[ALIGN] aligning meta ({len(df_meta)}) and PCA ({df_pcas.shape[0]}) to min_n={min_n}\")\n",
    "df_meta_full = pd.concat([df_meta.iloc[:min_n].reset_index(drop=True), df_pcas.iloc[:min_n].reset_index(drop=True)], axis=1)\n",
    "\n",
    "# sanitize df_meta_full for safe CSV\n",
    "def sanitize_df_for_csv(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == object or df[col].dtype.name == 'string':\n",
    "            df[col] = df[col].astype(str).apply(lambda s: s.replace(\"\\r\",\" \").replace(\"\\n\",\" \").replace(\"\\t\",\" \").replace('\"', \"''\").strip())\n",
    "    return df\n",
    "\n",
    "df_meta_full = sanitize_df_for_csv(df_meta_full)\n",
    "df_meta_full.to_csv(meta_pca_csv, index=False, encoding='utf-8', escapechar='\\\\', quoting=csv.QUOTE_MINIMAL)\n",
    "print(f\"[SAVE] saved meta+PCA to {meta_pca_csv} rows={len(df_meta_full)} cols={len(df_meta_full.columns)}\")\n",
    "\n",
    "# Standardize PCA features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "Xs = StandardScaler().fit_transform(X_pca[:min_n])\n",
    "\n",
    "# Clustering: prefer HDBSCAN; fallback to DBSCAN\n",
    "labels = None\n",
    "try:\n",
    "    import hdbscan\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=3, metric='euclidean')\n",
    "    labels = clusterer.fit_predict(Xs)\n",
    "    print(f\"[HDBSCAN] found clusters: {len(set(labels)) - (1 if -1 in labels else 0)}, noise: {(labels==-1).sum()}\")\n",
    "except Exception as e:\n",
    "    print(f\"[HDBSCAN] not available or failed ({e}). Falling back to DBSCAN.\")\n",
    "    from sklearn.cluster import DBSCAN\n",
    "    db = DBSCAN(eps=0.8, min_samples=3, metric='euclidean', n_jobs=-1)\n",
    "    labels = db.fit_predict(Xs)\n",
    "    print(f\"[DBSCAN] found clusters: {len(set(labels)) - (1 if -1 in labels else 0)}, noise: {(labels==-1).sum()}\")\n",
    "\n",
    "# Novelty scoring using IsolationForest (or fallback centroid distance if small N)\n",
    "from sklearn.ensemble import IsolationForest\n",
    "if Xs.shape[0] < 10:\n",
    "    centroid = Xs.mean(axis=0, keepdims=True)\n",
    "    dists = np.linalg.norm(Xs - centroid, axis=1)\n",
    "    novelty = (dists - dists.min()) / (dists.max() - dists.min() + 1e-12)\n",
    "else:\n",
    "    iso = IsolationForest(n_estimators=200, contamination=0.05, random_state=42)\n",
    "    iso.fit(Xs)\n",
    "    scores = iso.decision_function(Xs)  # higher -> more normal\n",
    "    smin, smax = float(scores.min()), float(scores.max())\n",
    "    if smax - smin == 0:\n",
    "        novelty = np.zeros_like(scores)\n",
    "    else:\n",
    "        novelty = 1.0 - ((scores - smin) / (smax - smin))\n",
    "\n",
    "# Attach and save clustered results (sanitize)\n",
    "df_meta_full = df_meta_full.iloc[:len(labels)].reset_index(drop=True)\n",
    "df_meta_full[\"cluster_label\"] = labels.astype(int).astype(str)\n",
    "df_meta_full[\"novelty_score\"] = novelty.astype(float)\n",
    "df_meta_full = sanitize_df_for_csv(df_meta_full)\n",
    "\n",
    "df_meta_full.to_csv(clustered_csv, index=False, encoding='utf-8', escapechar='\\\\', quoting=csv.QUOTE_MINIMAL)\n",
    "df_meta_full.to_json(clustered_json, orient='records', lines=False)\n",
    "print(f\"[DONE] saved clustered metadata CSV: {clustered_csv} and JSON: {clustered_json}\")\n",
    "print(f\"  clusters example labels (unique, truncated): {sorted(set(labels))[:20]}\")\n",
    "print(f\"  novelty min/max/mean: {float(np.min(novelty)):.4f}/{float(np.max(novelty)):.4f}/{float(np.mean(novelty)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90123eeb-be6f-485a-ba7b-862cc3d44486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD] X_pca shape: (2555, 64), meta rows: 2555\n",
      "[LOAD] label_encoders loaded from: label_encoders_rebuilt.pkl\n",
      "[DATA] train=2171, val=384, batch_size=64\n",
      "[MODEL PARTS] shared on cpu; heads: [('kingdom', 2), ('phylum', 5), ('class', 10), ('order', 13), ('family', 19), ('genus', 27), ('species', 182)]\n",
      "[READY] shared + heads built. Use train_one_epoch_shared(shared, heads, train_loader, optimizer, criterions) to train and evaluate_shared(shared, heads, val_loader) to evaluate.\n",
      "[SAVE] saved shared_heads_initial.pt and label_encoders_used.pkl\n"
     ]
    }
   ],
   "source": [
    "# Replacement Cell 10 fix (robust): Build shared feature extractor + ModuleDict heads (no custom subclass)\n",
    "# and provide training/evaluation functions that use them directly (avoids subclass/instantiation issues).\n",
    "\n",
    "import os, pickle, json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Paths\n",
    "DOWNLOAD_DIR = Path(globals().get(\"DOWNLOAD_DIR\", \"./ncbi_blast_db\"))\n",
    "EXTRACT_DIR = DOWNLOAD_DIR / \"extracted\"\n",
    "EMB_PCA_NPY = EXTRACT_DIR / \"embeddings_pca.npy\"\n",
    "META_CLUSTERED_CSV = EXTRACT_DIR / \"embeddings_meta_clustered.csv\"\n",
    "LABEL_ENCODERS_CANDIDATES = [\n",
    "    EXTRACT_DIR / \"label_encoders_safe.pkl\",\n",
    "    EXTRACT_DIR / \"label_encoders_v2.pkl\",\n",
    "    EXTRACT_DIR / \"label_encoders_rebuilt.pkl\",\n",
    "    EXTRACT_DIR / \"label_encoders_final.pkl\",\n",
    "    EXTRACT_DIR / \"label_encoders.pkl\"\n",
    "]\n",
    "\n",
    "RANKS = [\"kingdom\",\"phylum\",\"class\",\"order\",\"family\",\"genus\",\"species\"]\n",
    "BATCH_SIZE = 64\n",
    "SEED = 42\n",
    "\n",
    "# sanity\n",
    "if not Path(EMB_PCA_NPY).exists():\n",
    "    raise RuntimeError(f\"Missing {EMB_PCA_NPY}. Run embedding PCA cell first.\")\n",
    "if not Path(META_CLUSTERED_CSV).exists():\n",
    "    raise RuntimeError(f\"Missing {META_CLUSTERED_CSV}. Run previous clustering cell first.\")\n",
    "\n",
    "# load features and meta\n",
    "X_pca = np.load(EMB_PCA_NPY)\n",
    "df_meta = pd.read_csv(META_CLUSTERED_CSV, dtype=str, keep_default_na=False, na_filter=False)\n",
    "n = X_pca.shape[0]\n",
    "if len(df_meta) != n:\n",
    "    mn = min(len(df_meta), n)\n",
    "    print(f\"[ALIGN] trimming to {mn}\")\n",
    "    df_meta = df_meta.iloc[:mn].reset_index(drop=True)\n",
    "    X_pca = X_pca[:mn]\n",
    "    n = mn\n",
    "\n",
    "print(f\"[LOAD] X_pca shape: {X_pca.shape}, meta rows: {len(df_meta)}\")\n",
    "\n",
    "# load or rebuild label_encoders\n",
    "label_encoders = None\n",
    "for p in LABEL_ENCODERS_CANDIDATES:\n",
    "    if p.exists():\n",
    "        try:\n",
    "            with open(p, \"rb\") as fh:\n",
    "                label_encoders = pickle.load(fh)\n",
    "            print(f\"[LOAD] label_encoders loaded from: {p.name}\")\n",
    "            break\n",
    "        except Exception:\n",
    "            label_encoders = None\n",
    "\n",
    "# fallback: use in-memory label_encoders if present\n",
    "if label_encoders is None and \"label_encoders\" in globals():\n",
    "    label_encoders = globals()[\"label_encoders\"]\n",
    "    print(\"[LOAD] label_encoders loaded from globals\")\n",
    "\n",
    "# final fallback: reconstruct light-weight encoders (best-effort)\n",
    "if label_encoders is None:\n",
    "    print(\"[INFO] Reconstructing simple label_encoders from fetched metadata (best-effort).\")\n",
    "    id_to_tax = {}\n",
    "    for marker in (\"ssu\",\"lsu\",\"its\"):\n",
    "        meta_json = EXTRACT_DIR / f\"{marker}_fetched_metadata.json\"\n",
    "        if not meta_json.exists():\n",
    "            continue\n",
    "        try:\n",
    "            recs = json.load(open(meta_json))\n",
    "        except Exception:\n",
    "            continue\n",
    "        for rec in recs:\n",
    "            rid = str(rec.get(\"id\") or rec.get(\"accession\") or rec.get(\"accession_version\") or \"\")\n",
    "            if not rid:\n",
    "                continue\n",
    "            taxonomy = rec.get(\"taxonomy\") or []\n",
    "            tax_map = {}\n",
    "            for i, rank in enumerate([\"kingdom\",\"phylum\",\"class\",\"order\",\"family\",\"genus\"]):\n",
    "                if i < len(taxonomy) and taxonomy[i]:\n",
    "                    tax_map[rank] = taxonomy[i]\n",
    "            organism = rec.get(\"organism\") or rec.get(\"description\") or \"\"\n",
    "            parts = organism.split()\n",
    "            if len(parts) >= 2:\n",
    "                tax_map[\"genus\"] = tax_map.get(\"genus\") or parts[0]\n",
    "                tax_map[\"species\"] = \" \".join(parts[:2])\n",
    "            id_to_tax[rid] = tax_map\n",
    "    # map df_meta ids\n",
    "    labels_by_rank = {r: [] for r in RANKS}\n",
    "    for rid in df_meta[\"id\"].astype(str).tolist():\n",
    "        t = id_to_tax.get(rid) or id_to_tax.get(rid.split(\".\")[0]) or {}\n",
    "        for r in RANKS:\n",
    "            labels_by_rank[r].append(t.get(r) if t.get(r) is not None else \"UNASSIGNED\")\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    label_encoders = {}\n",
    "    for r in RANKS:\n",
    "        le = LabelEncoder()\n",
    "        le.fit(labels_by_rank[r])\n",
    "        label_encoders[r] = le\n",
    "    # save\n",
    "    with open(EXTRACT_DIR / \"label_encoders_rebuilt.pkl\", \"wb\") as fh:\n",
    "        pickle.dump(label_encoders, fh)\n",
    "    print(\"[SAVE] saved label_encoders_rebuilt.pkl\")\n",
    "\n",
    "# Prepare y_encoded arrays aligned with df_meta\n",
    "y_encoded = {}\n",
    "for r in RANKS:\n",
    "    le = label_encoders[r]\n",
    "    # Attempt mapping from df_meta columns if present (e.g., a column with taxon names) — otherwise fallback to UNASSIGNED\n",
    "    # We don't assume any taxon column exists; use best-effort mapping via fetched metadata if available\n",
    "    mapped_list = []\n",
    "    # Try to read column named r in df_meta (if earlier pipeline placed taxon names there)\n",
    "    if r in df_meta.columns:\n",
    "        for val in df_meta[r].astype(str).tolist():\n",
    "            mapped_list.append(val if val != \"\" else \"UNASSIGNED\")\n",
    "    else:\n",
    "        # fallback: create UNASSIGNED for all; label_encoder will still have classes (e.g., UNASSIGNED)\n",
    "        mapped_list = [\"UNASSIGNED\"] * len(df_meta)\n",
    "    # now encode, mapping unseen labels to UNASSIGNED or index 0\n",
    "    encoded = []\n",
    "    for lab in mapped_list:\n",
    "        if lab in le.classes_:\n",
    "            encoded.append(int(np.where(le.classes_ == lab)[0][0]))\n",
    "        else:\n",
    "            if \"UNASSIGNED\" in le.classes_:\n",
    "                encoded.append(int(np.where(le.classes_ == \"UNASSIGNED\")[0][0]))\n",
    "            else:\n",
    "                encoded.append(0)\n",
    "    y_encoded[r] = np.array(encoded, dtype=int)\n",
    "\n",
    "# Build TensorDatasets (assuming train_idx/val_idx were created earlier)\n",
    "if \"train_idx\" in globals() and \"val_idx\" in globals():\n",
    "    train_idx = globals()[\"train_idx\"]\n",
    "    val_idx = globals()[\"val_idx\"]\n",
    "else:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    idx = np.arange(n)\n",
    "    train_idx, val_idx = train_test_split(idx, test_size=0.15, random_state=SEED, shuffle=True)\n",
    "    globals()[\"train_idx\"], globals()[\"val_idx\"] = train_idx, val_idx\n",
    "\n",
    "X_tensor = torch.tensor(X_pca, dtype=torch.float32)\n",
    "y_tensors = [torch.tensor(y_encoded[r], dtype=torch.long) for r in RANKS]\n",
    "\n",
    "X_train = X_tensor[train_idx]\n",
    "X_val   = X_tensor[val_idx]\n",
    "y_train_list = [yt[train_idx] for yt in y_tensors]\n",
    "y_val_list   = [yt[val_idx] for yt in y_tensors]\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "train_ds = TensorDataset(X_train, *y_train_list)\n",
    "val_ds   = TensorDataset(X_val,   *y_val_list)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "print(f\"[DATA] train={len(train_ds)}, val={len(val_ds)}, batch_size={BATCH_SIZE}\")\n",
    "\n",
    "# Build shared extractor + ModuleDict heads (no subclass)\n",
    "input_dim = X_pca.shape[1]\n",
    "hidden_dim = 256\n",
    "shared = nn.Sequential(\n",
    "    nn.Linear(input_dim, hidden_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(hidden_dim, hidden_dim//2),\n",
    "    nn.ReLU()\n",
    ")\n",
    "heads = nn.ModuleDict()\n",
    "for r in RANKS:\n",
    "    ncls = len(label_encoders[r].classes_)\n",
    "    heads[r] = nn.Linear(hidden_dim//2, ncls)\n",
    "\n",
    "# Move modules to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "shared.to(device)\n",
    "heads.to(device)\n",
    "print(f\"[MODEL PARTS] shared on {device}; heads: {[ (r, heads[r].out_features) for r in RANKS ]}\")\n",
    "\n",
    "# Collect parameters and optimizer\n",
    "params = list(shared.parameters()) + list(heads.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=1e-3)\n",
    "\n",
    "# Build criterions: try to use any class_weights in globals, otherwise uniform\n",
    "criterions = {}\n",
    "for r in RANKS:\n",
    "    if \"class_weights\" in globals() and isinstance(globals()[\"class_weights\"], dict) and r in globals()[\"class_weights\"]:\n",
    "        w = globals()[\"class_weights\"][r]\n",
    "        if isinstance(w, (list, tuple, np.ndarray)):\n",
    "            w = torch.tensor(w, dtype=torch.float32)\n",
    "        if isinstance(w, torch.Tensor):\n",
    "            w = w.to(device)\n",
    "        else:\n",
    "            w = torch.tensor(np.asarray(w), dtype=torch.float32).to(device)\n",
    "    else:\n",
    "        w = torch.ones(len(label_encoders[r].classes_), dtype=torch.float32).to(device)\n",
    "    criterions[r] = nn.CrossEntropyLoss(weight=w)\n",
    "\n",
    "# Training & evaluation functions that use shared + heads directly\n",
    "def train_one_epoch_shared(shared, heads, loader, optimizer, criterions, device=device):\n",
    "    shared.train()\n",
    "    heads.train()\n",
    "    total_loss = 0.0\n",
    "    n_batches = 0\n",
    "    for batch in loader:\n",
    "        x = batch[0].to(device)\n",
    "        h = shared(x)\n",
    "        outputs = {r: heads[r](h) for r in RANKS}\n",
    "        loss = 0.0\n",
    "        for i, r in enumerate(RANKS):\n",
    "            target = batch[1 + i].to(device)\n",
    "            loss = loss + criterions[r](outputs[r], target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    return total_loss / max(1, n_batches)\n",
    "\n",
    "def evaluate_shared(shared, heads, loader, device=device):\n",
    "    shared.eval()\n",
    "    heads.eval()\n",
    "    from sklearn.metrics import accuracy_score, f1_score\n",
    "    preds = {r: [] for r in RANKS}\n",
    "    trues = {r: [] for r in RANKS}\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x = batch[0].to(device)\n",
    "            h = shared(x)\n",
    "            outputs = {r: heads[r](h) for r in RANKS}\n",
    "            for i, r in enumerate(RANKS):\n",
    "                logits = outputs[r]\n",
    "                pred = torch.argmax(torch.softmax(logits, dim=1), dim=1).cpu().numpy()\n",
    "                true = batch[1 + i].cpu().numpy()\n",
    "                preds[r].extend(pred.tolist())\n",
    "                trues[r].extend(true.tolist())\n",
    "    metrics = {}\n",
    "    for r in RANKS:\n",
    "        try:\n",
    "            acc = accuracy_score(trues[r], preds[r])\n",
    "            f1m = f1_score(trues[r], preds[r], average='macro', zero_division=0)\n",
    "        except Exception:\n",
    "            acc, f1m = None, None\n",
    "        metrics[r] = {\"accuracy\": acc, \"f1_macro\": f1m, \"n_classes\": len(label_encoders[r].classes_)}\n",
    "    return metrics\n",
    "\n",
    "print(\"[READY] shared + heads built. Use train_one_epoch_shared(shared, heads, train_loader, optimizer, criterions) to train and evaluate_shared(shared, heads, val_loader) to evaluate.\")\n",
    "\n",
    "# Save the shared+heads state dicts and label encoders for later inference\n",
    "torch.save({\"shared_state\": shared.state_dict(), \"heads_state\": {r: heads[r].state_dict() for r in RANKS}}, EXTRACT_DIR / \"shared_heads_initial.pt\")\n",
    "with open(EXTRACT_DIR / \"label_encoders_used.pkl\", \"wb\") as fh:\n",
    "    pickle.dump(label_encoders, fh)\n",
    "print(\"[SAVE] saved shared_heads_initial.pt and label_encoders_used.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42b4d5a1-6bdf-4b8e-a62a-6e4757497de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCHED WARN] Could not create ReduceLROnPlateau scheduler due to: AttributeError(\"type object 'ReduceLROnPlateau' has no attribute '_init_'\")\n",
      "Epoch 001 | train_loss: 11.7235 | val_loss: 1.4548 | val_agg_f1: 1.0000 | time: 1.1s\n",
      "  [CHECKPOINT] saved new best model at epoch 1, val_agg_f1=1.0000\n",
      "Epoch 002 | train_loss: 0.1883 | val_loss: 0.0025 | val_agg_f1: 1.0000 | time: 0.7s\n",
      "Epoch 003 | train_loss: 0.0021 | val_loss: 0.0013 | val_agg_f1: 1.0000 | time: 0.8s\n",
      "Epoch 004 | train_loss: 0.0014 | val_loss: 0.0009 | val_agg_f1: 1.0000 | time: 1.0s\n",
      "Epoch 005 | train_loss: 0.0010 | val_loss: 0.0006 | val_agg_f1: 1.0000 | time: 1.6s\n",
      "Epoch 006 | train_loss: 0.0006 | val_loss: 0.0004 | val_agg_f1: 1.0000 | time: 1.1s\n",
      "Epoch 007 | train_loss: 0.0004 | val_loss: 0.0002 | val_agg_f1: 1.0000 | time: 1.1s\n",
      "Epoch 008 | train_loss: 0.0002 | val_loss: 0.0001 | val_agg_f1: 1.0000 | time: 1.2s\n",
      "Epoch 009 | train_loss: 0.0001 | val_loss: 0.0000 | val_agg_f1: 1.0000 | time: 1.3s\n",
      "[EARLY STOP] No improvement for 8 epochs (patience=8). Stopping.\n",
      "[TRAINING COMPLETE] epochs_run=9 best_val_agg_f1=1.0000 total_time_sec=10.2\n",
      "[LOAD] loaded best checkpoint from ncbi_blast_db\\extracted\\best_shared_heads.pt (epoch 1, val_agg_f1=1.0000)\n",
      "[SAVE] training history saved to ncbi_blast_db\\extracted\\training_history.csv and ncbi_blast_db\\extracted\\training_history.json\n",
      "=== Final validation metrics (best model) ===\n",
      "kingdom  | n_classes=2   | acc=1.0000 | f1_macro=1.0000 | mean_conf=0.9890\n",
      "phylum   | n_classes=5   | acc=1.0000 | f1_macro=1.0000 | mean_conf=0.9475\n",
      "class    | n_classes=10  | acc=1.0000 | f1_macro=1.0000 | mean_conf=0.9430\n",
      "order    | n_classes=13  | acc=1.0000 | f1_macro=1.0000 | mean_conf=0.8769\n",
      "family   | n_classes=19  | acc=1.0000 | f1_macro=1.0000 | mean_conf=0.8506\n",
      "genus    | n_classes=27  | acc=1.0000 | f1_macro=1.0000 | mean_conf=0.7629\n",
      "species  | n_classes=182 | acc=1.0000 | f1_macro=1.0000 | mean_conf=0.5193\n",
      "Aggregated mean-macro-F1: 1.0000 | val_loss: 1.4548\n"
     ]
    }
   ],
   "source": [
    "# Cell 11 (robust): Training loop with early stopping and robust ReduceLROnPlateau creation\n",
    "import time, json, inspect\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# --- Config ---\n",
    "DOWNLOAD_DIR = Path(globals().get(\"DOWNLOAD_DIR\", \"./ncbi_blast_db\"))\n",
    "EXTRACT_DIR = DOWNLOAD_DIR / \"extracted\"\n",
    "HISTORY_CSV = EXTRACT_DIR / \"training_history.csv\"\n",
    "HISTORY_JSON = EXTRACT_DIR / \"training_history.json\"\n",
    "BEST_CHECKPOINT = EXTRACT_DIR / \"best_shared_heads.pt\"\n",
    "\n",
    "MAX_EPOCHS = 50\n",
    "MIN_EPOCHS = 5\n",
    "PATIENCE = 8            # early stopping patience (no improvement in val score)\n",
    "LR_FACTOR = 0.5\n",
    "LR_PATIENCE = 3\n",
    "MIN_LR = 1e-6\n",
    "\n",
    "# --- Sanity checks: required objects created by previous cells ---\n",
    "reqs = [\"shared\", \"heads\", \"train_loader\", \"val_loader\", \"criterions\", \"optimizer\", \"device\", \"RANKS\", \"label_encoders\"]\n",
    "missing = [n for n in reqs if n not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Cannot start training: missing objects in notebook globals: {missing}\")\n",
    "\n",
    "shared = globals()[\"shared\"]\n",
    "heads = globals()[\"heads\"]\n",
    "train_loader = globals()[\"train_loader\"]\n",
    "val_loader = globals()[\"val_loader\"]\n",
    "criterions = globals()[\"criterions\"]\n",
    "optimizer = globals()[\"optimizer\"]\n",
    "device = globals()[\"device\"]\n",
    "RANKS = globals()[\"RANKS\"]\n",
    "label_encoders = globals()[\"label_encoders\"]\n",
    "\n",
    "# ensure modules on device\n",
    "shared.to(device)\n",
    "heads.to(device)\n",
    "for r in RANKS:\n",
    "    heads[r].to(device)\n",
    "\n",
    "# deterministic-ish\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Robust scheduler creation: inspect allowed kwargs and instantiate accordingly.\n",
    "def create_reduce_on_plateau_scheduler(opt, mode=\"max\", factor=0.5, patience=3, min_lr=1e-6):\n",
    "    try:\n",
    "        ctor = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "        sig = inspect.signature(ctor._init_)\n",
    "        allowed = set(sig.parameters.keys()) - {\"self\", \"args\", \"kwargs\"}\n",
    "        kwargs = {}\n",
    "        cand = {\"mode\": mode, \"factor\": factor, \"patience\": patience, \"min_lr\": min_lr}\n",
    "        # Some old/new torch versions name min_lr differently; try to map common names\n",
    "        renames = {\"min_lr\": \"min_lr\", \"min_lr_alt\": \"min_lr\"}\n",
    "        for k, v in cand.items():\n",
    "            if k in allowed:\n",
    "                kwargs[k] = v\n",
    "        # if verbose allowed, set it to False (safe)\n",
    "        if \"verbose\" in allowed:\n",
    "            kwargs[\"verbose\"] = False\n",
    "        # instantiate\n",
    "        scheduler = ctor(opt, **kwargs)\n",
    "        print(\"[SCHED] ReduceLROnPlateau created with kwargs:\", kwargs)\n",
    "        return scheduler\n",
    "    except Exception as e:\n",
    "        print(\"[SCHED WARN] Could not create ReduceLROnPlateau scheduler due to:\", repr(e))\n",
    "        # fallback: dummy scheduler with same interface\n",
    "        class DummyScheduler:\n",
    "            def step(self, metric=None):\n",
    "                return None\n",
    "        return DummyScheduler()\n",
    "\n",
    "scheduler = create_reduce_on_plateau_scheduler(optimizer, mode=\"max\", factor=LR_FACTOR, patience=LR_PATIENCE, min_lr=MIN_LR)\n",
    "\n",
    "# helper: compute validation loss and other metrics\n",
    "def compute_val_loss_and_metrics(shared, heads, loader, criterions, device):\n",
    "    shared.eval()\n",
    "    heads.eval()\n",
    "    total_loss = 0.0\n",
    "    n_batches = 0\n",
    "    preds = {r: [] for r in RANKS}\n",
    "    trues = {r: [] for r in RANKS}\n",
    "    confidences = {r: [] for r in RANKS}   # top1 prob\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x = batch[0].to(device)\n",
    "            targets = [batch[i+1].to(device) for i in range(len(RANKS))]\n",
    "            h = shared(x)\n",
    "            outputs = {r: heads[r](h) for r in RANKS}\n",
    "            loss = 0.0\n",
    "            for i, r in enumerate(RANKS):\n",
    "                logits = outputs[r]\n",
    "                tgt = targets[i]\n",
    "                loss += criterions[r](logits, tgt)\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                top1 = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "                top1_conf = probs.max(dim=1).values.cpu().numpy()\n",
    "                preds[r].extend(top1.tolist())\n",
    "                trues[r].extend(tgt.cpu().numpy().tolist())\n",
    "                confidences[r].extend(top1_conf.tolist())\n",
    "            total_loss += float(loss.item())\n",
    "            n_batches += 1\n",
    "    avg_loss = total_loss / max(1, n_batches)\n",
    "    metrics = {}\n",
    "    f1s = []\n",
    "    for r in RANKS:\n",
    "        try:\n",
    "            acc = accuracy_score(trues[r], preds[r])\n",
    "            f1m = f1_score(trues[r], preds[r], average=\"macro\", zero_division=0)\n",
    "            mean_conf = float(np.mean(confidences[r])) if len(confidences[r])>0 else None\n",
    "        except Exception:\n",
    "            acc, f1m, mean_conf = None, None, None\n",
    "        metrics[r] = {\"accuracy\": acc, \"f1_macro\": f1m, \"mean_confidence\": mean_conf,\n",
    "                      \"n_classes\": len(label_encoders[r].classes_)}\n",
    "        if f1m is not None:\n",
    "            f1s.append(f1m)\n",
    "    agg_f1 = float(np.mean(f1s)) if len(f1s)>0 else 0.0\n",
    "    return avg_loss, metrics, agg_f1\n",
    "\n",
    "# training loop with early stopping\n",
    "best_score = -np.inf\n",
    "epochs_no_improve = 0\n",
    "history = []\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(1, MAX_EPOCHS + 1):\n",
    "    t0 = time.time()\n",
    "    # Train one epoch\n",
    "    shared.train()\n",
    "    heads.train()\n",
    "    train_loss = 0.0\n",
    "    n_batches = 0\n",
    "    for batch in train_loader:\n",
    "        x = batch[0].to(device)\n",
    "        targets = [batch[i+1].to(device) for i in range(len(RANKS))]\n",
    "        h = shared(x)\n",
    "        outputs = {r: heads[r](h) for r in RANKS}\n",
    "        loss = 0.0\n",
    "        for i, r in enumerate(RANKS):\n",
    "            loss = loss + criterions[r](outputs[r], targets[i])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(list(shared.parameters()) + list(heads.parameters()), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "        train_loss += float(loss.item())\n",
    "        n_batches += 1\n",
    "    train_loss = train_loss / max(1, n_batches)\n",
    "\n",
    "    # Validation\n",
    "    val_loss, val_metrics, val_agg_f1 = compute_val_loss_and_metrics(shared, heads, val_loader, criterions, device)\n",
    "\n",
    "    # Scheduler step (on aggregated val score) - handle both schedulers with and w/o metric arg\n",
    "    try:\n",
    "        # ReduceLROnPlateau expects a metric\n",
    "        scheduler.step(val_agg_f1)\n",
    "    except TypeError:\n",
    "        try:\n",
    "            # some schedulers expect no args\n",
    "            scheduler.step()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Save metrics & history\n",
    "    epoch_record = {\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_agg_f1\": val_agg_f1,\n",
    "        \"time_sec\": time.time() - t0\n",
    "    }\n",
    "    for r in RANKS:\n",
    "        m = val_metrics.get(r, {})\n",
    "        epoch_record.update({f\"{r}_acc\": m.get(\"accuracy\"), f\"{r}_f1_macro\": m.get(\"f1_macro\"), f\"{r}_mean_conf\": m.get(\"mean_confidence\")})\n",
    "    history.append(epoch_record)\n",
    "\n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch:03d} | train_loss: {train_loss:.4f} | val_loss: {val_loss:.4f} | val_agg_f1: {val_agg_f1:.4f} | time: {epoch_record['time_sec']:.1f}s\")\n",
    "\n",
    "    # Check for improvement\n",
    "    if val_agg_f1 > best_score + 1e-6:\n",
    "        best_score = val_agg_f1\n",
    "        epochs_no_improve = 0\n",
    "        checkpoint = {\n",
    "            \"shared_state\": shared.state_dict(),\n",
    "            \"heads_state\": {r: heads[r].state_dict() for r in RANKS},\n",
    "            \"epoch\": epoch,\n",
    "            \"val_agg_f1\": val_agg_f1,\n",
    "            \"optimizer_state\": optimizer.state_dict()\n",
    "        }\n",
    "        torch.save(checkpoint, BEST_CHECKPOINT)\n",
    "        print(f\"  [CHECKPOINT] saved new best model at epoch {epoch}, val_agg_f1={val_agg_f1:.4f}\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    # Save history\n",
    "    try:\n",
    "        pd.DataFrame(history).to_csv(HISTORY_CSV, index=False)\n",
    "        with open(HISTORY_JSON, \"w\") as fh:\n",
    "            json.dump(history, fh, indent=2)\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] could not save history:\", e)\n",
    "\n",
    "    # Early stopping\n",
    "    if epoch >= MIN_EPOCHS and epochs_no_improve >= PATIENCE:\n",
    "        print(f\"[EARLY STOP] No improvement for {epochs_no_improve} epochs (patience={PATIENCE}). Stopping.\")\n",
    "        break\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"[TRAINING COMPLETE] epochs_run={epoch} best_val_agg_f1={best_score:.4f} total_time_sec={total_time:.1f}\")\n",
    "\n",
    "# Load best checkpoint into memory (for immediate inference)\n",
    "if Path(BEST_CHECKPOINT).exists():\n",
    "    ckpt = torch.load(BEST_CHECKPOINT, map_location=device)\n",
    "    shared.load_state_dict(ckpt[\"shared_state\"])\n",
    "    for r in RANKS:\n",
    "        heads[r].load_state_dict(ckpt[\"heads_state\"][r])\n",
    "    print(f\"[LOAD] loaded best checkpoint from {BEST_CHECKPOINT} (epoch {ckpt.get('epoch')}, val_agg_f1={ckpt.get('val_agg_f1'):.4f})\")\n",
    "\n",
    "# Save final history once more\n",
    "try:\n",
    "    pd.DataFrame(history).to_csv(HISTORY_CSV, index=False)\n",
    "    with open(HISTORY_JSON, \"w\") as fh:\n",
    "        json.dump(history, fh, indent=2)\n",
    "    print(f\"[SAVE] training history saved to {HISTORY_CSV} and {HISTORY_JSON}\")\n",
    "except Exception as e:\n",
    "    print(\"[WARN] could not save final history:\", e)\n",
    "\n",
    "# Final evaluation on validation set (print nicely)\n",
    "final_val_loss, final_val_metrics, final_val_agg_f1 = compute_val_loss_and_metrics(shared, heads, val_loader, criterions, device)\n",
    "print(\"=== Final validation metrics (best model) ===\")\n",
    "for r in RANKS:\n",
    "    m = final_val_metrics.get(r, {})\n",
    "    print(f\"{r:8s} | n_classes={m.get('n_classes', '?'):<3} | acc={m.get('accuracy'):.4f} | f1_macro={m.get('f1_macro'):.4f} | mean_conf={m.get('mean_confidence'):.4f}\")\n",
    "print(f\"Aggregated mean-macro-F1: {final_val_agg_f1:.4f} | val_loss: {final_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9844fac-315d-47e8-846d-201d3c8ecc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD] rows=2555, X_pca.shape=(2555, 64), X_full.shape=(2555, 128)\n",
      "[LOAD] label_encoders loaded from label_encoders_used.pkl\n",
      "[LOAD] checkpoint loaded (epoch=1, val_agg_f1=1.0)\n",
      "[INFER] Completed batched forward pass.\n",
      "[NN] Using training set as reference for nearest neighbors (len= 2171 )\n",
      "[W2V] loaded k-mer Word2Vec model: kmer_w2v_k6.model\n",
      "[SAVE] predictions saved: ncbi_blast_db\\extracted\\predictions.jsonl (jsonl), ncbi_blast_db\\extracted\\predictions_summary.csv (csv). total_records=2555\n",
      "[SUMMARY] total sequences: 2555; novel candidates (novelty>0.8): 18\n",
      "First 5 novel candidates ids: ['XR_013100016.1', 'XR_013100016.1', 'LC876616.1', 'LC876591.1', 'LC876590.1']\n",
      "{\n",
      "  \"id\": \"JBJNTG020000075.1\",\n",
      "  \"predicted_summary\": {\n",
      "    \"kingdom\": \"UNASSIGNED\",\n",
      "    \"phylum\": \"UNASSIGNED\",\n",
      "    \"class\": \"UNASSIGNED\"\n",
      "  },\n",
      "  \"mean_conf\": 0.7759965402739388\n",
      "}\n",
      "{\n",
      "  \"id\": \"JBMETL020000032.1\",\n",
      "  \"predicted_summary\": {\n",
      "    \"kingdom\": \"UNASSIGNED\",\n",
      "    \"phylum\": \"UNASSIGNED\",\n",
      "    \"class\": \"UNASSIGNED\"\n",
      "  },\n",
      "  \"mean_conf\": 0.7927666349070412\n",
      "}\n",
      "{\n",
      "  \"id\": \"JBMETL020000181.1\",\n",
      "  \"predicted_summary\": {\n",
      "    \"kingdom\": \"UNASSIGNED\",\n",
      "    \"phylum\": \"UNASSIGNED\",\n",
      "    \"class\": \"UNASSIGNED\"\n",
      "  },\n",
      "  \"mean_conf\": 0.7891179578644889\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Inference + per-sequence JSON export (predicted taxonomy, probs, confidence, novelty, cluster, abundance, explainability)\n",
    "import os, json, math, traceback\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Optional: k-mer explainability (gensim Word2Vec)\n",
    "try:\n",
    "    from gensim.models import Word2Vec\n",
    "    have_gensim = True\n",
    "except Exception:\n",
    "    have_gensim = False\n",
    "\n",
    "# Paths\n",
    "DOWNLOAD_DIR = Path(globals().get(\"DOWNLOAD_DIR\", \"./ncbi_blast_db\"))\n",
    "EXTRACT_DIR = DOWNLOAD_DIR / \"extracted\"\n",
    "BEST_CHECKPOINT = EXTRACT_DIR / \"best_shared_heads.pt\"\n",
    "META_CLUSTERED_CSV = EXTRACT_DIR / \"embeddings_meta_clustered.csv\"\n",
    "EMB_PCA_NPY = EXTRACT_DIR / \"embeddings_pca.npy\"\n",
    "EMB_FULL_NPY = EXTRACT_DIR / \"embeddings.npy\"\n",
    "LABEL_ENCODERS_PKL = EXTRACT_DIR / \"label_encoders_used.pkl\"   # was saved earlier\n",
    "W2V_MODEL = DOWNLOAD_DIR / \"kmer_w2v_k6.model\"\n",
    "\n",
    "OUT_JSONL = EXTRACT_DIR / \"predictions.jsonl\"\n",
    "OUT_CSV   = EXTRACT_DIR / \"predictions_summary.csv\"\n",
    "\n",
    "# Basic checks\n",
    "for p in [BEST_CHECKPOINT, META_CLUSTERED_CSV, EMB_PCA_NPY, EMB_FULL_NPY]:\n",
    "    if not Path(p).exists():\n",
    "        raise RuntimeError(f\"Required file missing: {p}\")\n",
    "\n",
    "# Load data\n",
    "df_meta = pd.read_csv(META_CLUSTERED_CSV, dtype=str, keep_default_na=False, na_filter=False)\n",
    "X_pca = np.load(EMB_PCA_NPY)      # used as model input\n",
    "X_full = np.load(EMB_FULL_NPY)    # used for NN explainability (k-mer avg embedding)\n",
    "n = X_pca.shape[0]\n",
    "print(f\"[LOAD] rows={n}, X_pca.shape={X_pca.shape}, X_full.shape={X_full.shape}\")\n",
    "\n",
    "# Align meta length\n",
    "if len(df_meta) != n:\n",
    "    mn = min(len(df_meta), n)\n",
    "    print(f\"[ALIGN] trimming to min_n={mn}\")\n",
    "    df_meta = df_meta.iloc[:mn].reset_index(drop=True)\n",
    "    X_pca = X_pca[:mn]\n",
    "    X_full = X_full[:mn]\n",
    "    n = mn\n",
    "\n",
    "# Load label encoders\n",
    "label_encoders = None\n",
    "if Path(LABEL_ENCODERS_PKL).exists():\n",
    "    try:\n",
    "        import pickle\n",
    "        with open(LABEL_ENCODERS_PKL, \"rb\") as fh:\n",
    "            label_encoders = pickle.load(fh)\n",
    "        print(f\"[LOAD] label_encoders loaded from {LABEL_ENCODERS_PKL.name}\")\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] failed loading label encoders:\", e)\n",
    "if label_encoders is None and \"label_encoders\" in globals():\n",
    "    label_encoders = globals()[\"label_encoders\"]\n",
    "    print(\"[LOAD] label_encoders loaded from globals\")\n",
    "\n",
    "if label_encoders is None:\n",
    "    raise RuntimeError(\"Label encoders not found. Run earlier cells that produce them.\")\n",
    "\n",
    "# Recreate model parts (same architecture as training)\n",
    "input_dim = X_pca.shape[1]\n",
    "hidden_dim = 256\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "shared = torch.nn.Sequential(\n",
    "    torch.nn.Linear(input_dim, hidden_dim),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(0.3),\n",
    "    torch.nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "    torch.nn.ReLU()\n",
    ")\n",
    "heads = torch.nn.ModuleDict({ r: torch.nn.Linear(hidden_dim//2, len(label_encoders[r].classes_)) for r in label_encoders })\n",
    "\n",
    "# load checkpoint\n",
    "ckpt = torch.load(BEST_CHECKPOINT, map_location=\"cpu\")\n",
    "# state dicts saved as \"shared_state\" and \"heads_state\" earlier\n",
    "try:\n",
    "    shared.load_state_dict(ckpt[\"shared_state\"])\n",
    "    for r in label_encoders:\n",
    "        heads[r].load_state_dict(ckpt[\"heads_state\"][r])\n",
    "    print(f\"[LOAD] checkpoint loaded (epoch={ckpt.get('epoch')}, val_agg_f1={ckpt.get('val_agg_f1')})\")\n",
    "except Exception as e:\n",
    "    # if shapes mismatch, print and proceed with available weights\n",
    "    print(\"[WARN] failed to load full checkpoint cleanly:\", e)\n",
    "    # try best-effort partial load\n",
    "    try:\n",
    "        shared_state = ckpt.get(\"shared_state\", {})\n",
    "        shared.load_state_dict(shared_state, strict=False)\n",
    "        for r in label_encoders:\n",
    "            if r in ckpt.get(\"heads_state\", {}):\n",
    "                heads[r].load_state_dict(ckpt[\"heads_state\"][r], strict=False)\n",
    "        print(\"[LOAD] partial checkpoint loaded (strict=False).\")\n",
    "    except Exception as e2:\n",
    "        print(\"[ERROR] could not load checkpoint:\", e2)\n",
    "        raise\n",
    "\n",
    "shared.to(device)\n",
    "heads.to(device)\n",
    "shared.eval()\n",
    "heads.eval()\n",
    "\n",
    "# Batch inference\n",
    "BATCH = 256\n",
    "results = []\n",
    "all_probs = {r: [] for r in label_encoders}\n",
    "all_preds = {r: [] for r in label_encoders}\n",
    "all_topk = {r: [] for r in label_encoders}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for start in range(0, n, BATCH):\n",
    "        end = min(n, start + BATCH)\n",
    "        xb = torch.tensor(X_pca[start:end], dtype=torch.float32).to(device)\n",
    "        h = shared(xb)\n",
    "        for r in label_encoders:\n",
    "            logits = heads[r](h)               # (batch, n_classes)\n",
    "            probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "            top_idx = np.argmax(probs, axis=1)\n",
    "            # top-3\n",
    "            topk_idx = np.argsort(-probs, axis=1)[:, :3]\n",
    "            for i in range(end-start):\n",
    "                all_probs[r].append(probs[i])\n",
    "                all_preds[r].append(int(top_idx[i]))\n",
    "                all_topk[r].append([int(x) for x in topk_idx[i]])\n",
    "print(\"[INFER] Completed batched forward pass.\")\n",
    "\n",
    "# Build nearest-neighbors index for explainability\n",
    "# Prefer training set neighbors if train_idx is available\n",
    "if \"train_idx\" in globals():\n",
    "    ref_idx = np.array(globals()[\"train_idx\"], dtype=int)\n",
    "    print(\"[NN] Using training set as reference for nearest neighbors (len=\", len(ref_idx), \")\")\n",
    "else:\n",
    "    # else use full dataset as reference\n",
    "    ref_idx = np.arange(n)\n",
    "    print(\"[NN] Using full dataset as reference for nearest neighbors (len=\", len(ref_idx), \")\")\n",
    "\n",
    "try:\n",
    "    nn_model = NearestNeighbors(n_neighbors=6, metric=\"cosine\", n_jobs=-1)\n",
    "    nn_model.fit(X_full[ref_idx])\n",
    "    nn_available = True\n",
    "except Exception as e:\n",
    "    print(\"[WARN] NearestNeighbors failed:\", e)\n",
    "    nn_available = False\n",
    "\n",
    "# Attempt to load Word2Vec k-mer model (for top-k k-mers similar to sequence embedding)\n",
    "w2v = None\n",
    "if have_gensim and W2V_MODEL.exists():\n",
    "    try:\n",
    "        w2v = Word2Vec.load(str(W2V_MODEL))\n",
    "        print(\"[W2V] loaded k-mer Word2Vec model:\", W2V_MODEL.name)\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] failed to load Word2Vec model:\", e)\n",
    "        w2v = None\n",
    "else:\n",
    "    if not have_gensim:\n",
    "        print(\"[W2V] gensim not available; skipping k-mer explainability.\")\n",
    "    else:\n",
    "        print(\"[W2V] kmer model file not found; skipping k-mer explainability.\")\n",
    "\n",
    "# Build cluster counts and abundance proxy\n",
    "cluster_labels = df_meta[\"cluster_label\"].astype(str).tolist() if \"cluster_label\" in df_meta.columns else [\"-1\"]*n\n",
    "cluster_counter = Counter(cluster_labels)\n",
    "total_seqs = float(n)\n",
    "\n",
    "# utility to map class index -> label string\n",
    "def idx_to_label(r, idx):\n",
    "    classes = label_encoders[r].classes_\n",
    "    if idx < 0 or idx >= len(classes):\n",
    "        return \"UNASSIGNED\"\n",
    "    return str(classes[idx])\n",
    "\n",
    "# Now assemble per-sequence records and write out JSONL + CSV summary\n",
    "out_records = []\n",
    "csv_rows = []\n",
    "for i in range(n):\n",
    "    rec_id = str(df_meta.loc[i, \"id\"]) if \"id\" in df_meta.columns else f\"seq_{i+1}\"\n",
    "    entry = {\"id\": rec_id}\n",
    "    per_rank = {}\n",
    "    mean_conf = 0.0\n",
    "    conf_count = 0\n",
    "    for r in label_encoders:\n",
    "        probs_r = all_probs[r][i]\n",
    "        pred_idx = int(all_preds[r][i])\n",
    "        pred_label = idx_to_label(r, pred_idx)\n",
    "        pred_prob = float(probs_r[pred_idx])\n",
    "        # top-3 with labels & probs\n",
    "        topk_idx = all_topk[r][i]\n",
    "        topk = [{\"label\": idx_to_label(r, int(k)), \"prob\": float(probs_r[int(k)])} for k in topk_idx]\n",
    "        per_rank[r] = {\"predicted_label\": pred_label, \"predicted_index\": pred_idx, \"predicted_prob\": pred_prob, \"top_k\": topk}\n",
    "        mean_conf += pred_prob\n",
    "        conf_count += 1\n",
    "    mean_conf = mean_conf / max(1, conf_count)\n",
    "    entry[\"predicted\"] = per_rank\n",
    "    entry[\"mean_confidence\"] = mean_conf\n",
    "\n",
    "    # attach novelty and cluster info from metadata (if present)\n",
    "    novelty = float(df_meta.loc[i, \"novelty_score\"]) if \"novelty_score\" in df_meta.columns and df_meta.loc[i, \"novelty_score\"] != \"\" else None\n",
    "    cluster_label = str(df_meta.loc[i, \"cluster_label\"]) if \"cluster_label\" in df_meta.columns else \"-1\"\n",
    "    entry[\"novelty_score\"] = novelty\n",
    "    entry[\"cluster_label\"] = cluster_label\n",
    "    cluster_size = int(cluster_counter.get(cluster_label, 1))\n",
    "    entry[\"cluster_size\"] = cluster_size\n",
    "    entry[\"abundance_proxy\"] = float(cluster_size) / total_seqs\n",
    "\n",
    "    # QC flags\n",
    "    qc_flags = []\n",
    "    if mean_conf < 0.35:\n",
    "        qc_flags.append(\"low_confidence\")\n",
    "    if novelty is not None and novelty > 0.8:\n",
    "        qc_flags.append(\"novel_candidate\")\n",
    "    entry[\"qc_flags\"] = qc_flags\n",
    "\n",
    "    # Nearest neighbors (explainability)\n",
    "    neighbors = []\n",
    "    if nn_available:\n",
    "        try:\n",
    "            # query using full embedding\n",
    "            emb = X_full[i].reshape(1, -1)\n",
    "            dists, idxs = nn_model.kneighbors(emb, n_neighbors=6, return_distance=True)\n",
    "            dists = dists[0].tolist()\n",
    "            idxs = idxs[0].tolist()\n",
    "            for dd, ridx in zip(dists, idxs):\n",
    "                ref_global_idx = int(ref_idx[ridx])\n",
    "                if ref_global_idx == i:\n",
    "                    # skip self-match; continue to next\n",
    "                    continue\n",
    "                nid = str(df_meta.loc[ref_global_idx, \"id\"])\n",
    "                neighbors.append({\"id\": nid, \"index\": int(ref_global_idx), \"distance\": float(dd),\n",
    "                                  \"cluster\": str(df_meta.loc[ref_global_idx, \"cluster_label\"]) if \"cluster_label\" in df_meta.columns else None})\n",
    "            # take top-3 excluding self\n",
    "            entry[\"nearest_neighbors\"] = neighbors[:3]\n",
    "        except Exception as e:\n",
    "            entry[\"nearest_neighbors_error\"] = str(e)\n",
    "    else:\n",
    "        entry[\"nearest_neighbors\"] = []\n",
    "\n",
    "    # Top-k k-mers similar to sequence embedding (approx explainability) if Word2Vec is available\n",
    "    top_kmers = []\n",
    "    if w2v is not None:\n",
    "        try:\n",
    "            seq_emb = X_full[i]  # same space as averaged kmer vectors\n",
    "            # gensim KeyedVectors similarity_by_vector: use wv.similar_by_vector\n",
    "            kv = w2v.wv\n",
    "            # similar_by_vector may be expensive; request top 5\n",
    "            sim = kv.similar_by_vector(seq_emb, topn=5)\n",
    "            # sim is list of (kmer, score)\n",
    "            top_kmers = [{\"kmer\": s[0], \"sim\": float(s[1])} for s in sim]\n",
    "            entry[\"top_kmers\"] = top_kmers\n",
    "        except Exception as e:\n",
    "            entry[\"top_kmers_error\"] = str(e)\n",
    "    else:\n",
    "        entry[\"top_kmers\"] = []\n",
    "\n",
    "    # store\n",
    "    out_records.append(entry)\n",
    "    # flatten for CSV row summary (one-line per sample)\n",
    "    csv_row = {\n",
    "        \"id\": rec_id,\n",
    "        \"cluster_label\": cluster_label,\n",
    "        \"cluster_size\": cluster_size,\n",
    "        \"abundance_proxy\": entry[\"abundance_proxy\"],\n",
    "        \"novelty_score\": novelty,\n",
    "        \"mean_confidence\": mean_conf,\n",
    "        \"qc_flags\": \"|\".join(qc_flags) if qc_flags else \"\"\n",
    "    }\n",
    "    # include predicted labels per rank\n",
    "    for r in label_encoders:\n",
    "        csv_row[f\"{r}_pred\"] = per_rank[r][\"predicted_label\"]\n",
    "        csv_row[f\"{r}_prob\"] = per_rank[r][\"predicted_prob\"]\n",
    "    csv_rows.append(csv_row)\n",
    "\n",
    "# Write outputs\n",
    "try:\n",
    "    with open(OUT_JSONL, \"w\", encoding=\"utf-8\") as fh:\n",
    "        for rec in out_records:\n",
    "            fh.write(json.dumps(rec) + \"\\n\")\n",
    "    pd.DataFrame(csv_rows).to_csv(OUT_CSV, index=False)\n",
    "    print(f\"[SAVE] predictions saved: {OUT_JSONL} (jsonl), {OUT_CSV} (csv). total_records={len(out_records)}\")\n",
    "except Exception as e:\n",
    "    print(\"[ERROR] failed to save predictions:\", e)\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Print summary of interesting candidates (top novel candidates)\n",
    "novel_candidates = [r for r in out_records if (r.get(\"novelty_score\") is not None and r[\"novelty_score\"] > 0.8)]\n",
    "print(f\"[SUMMARY] total sequences: {n}; novel candidates (novelty>0.8): {len(novel_candidates)}\")\n",
    "if len(novel_candidates) > 0:\n",
    "    print(\"First 5 novel candidates ids:\", [r[\"id\"] for r in novel_candidates[:5]])\n",
    "\n",
    "# Example: print first 3 prediction records for quick inspection\n",
    "for rec in out_records[:3]:\n",
    "    print(json.dumps({\"id\": rec[\"id\"], \"predicted_summary\": {r: rec[\"predicted\"][r][\"predicted_label\"] for r in list(rec[\"predicted\"])[:3]}, \"mean_conf\": rec[\"mean_confidence\"]}, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb9f7932-78a1-460b-9d51-ee8a43da7e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Loading files...\n",
      "  loaded: predictions.jsonl 2555 records; summary csv 2555 rows; meta csv 2555 rows\n",
      "[FIX] Filled 10 empty IDs (from meta).\n",
      "[SAVE] Saved fixed predictions.jsonl and predictions_summary.csv (backups created if not present).\n",
      "[DIAG] total: 2555 rows, empty_ids=5, duplicated_ids=929, unique_ids=1626\n",
      "[DIAG] novel candidates (>0.8): 18 rows, 12 unique ids\n",
      "  first 10 unique novel ids: ['XR_013100016.1', 'LC876616.1', 'LC876591.1', 'LC876590.1', 'LC876589.1', 'LC876588.1', 'PX279187.1', 'PX279186.1', 'PQ523755.1', 'PX277228.1']\n",
      "[DIAG] top clusters by novel candidate count:\n",
      "cluster_label  novel_n   n\n",
      "           -1       18 531\n",
      "[DIAG] Top predicted labels per rank (top 10):\n",
      "  class     : {'UNASSIGNED': 2555}\n",
      "  family    : {'UNASSIGNED': 2555}\n",
      "  genus     : {'UNASSIGNED': 2555}\n",
      "  kingdom   : {'UNASSIGNED': 2555}\n",
      "  order     : {'UNASSIGNED': 2555}\n",
      "  phylum    : {'UNASSIGNED': 2555}\n",
      "  species   : {'UNASSIGNED': 2555}\n",
      "[DIAG] mean_confidence summary:\n",
      "count    2555.000000\n",
      "mean        0.838651\n",
      "std         0.083266\n",
      "min         0.698247\n",
      "25%         0.765083\n",
      "50%         0.776699\n",
      "75%         0.919195\n",
      "max         0.997886\n",
      "[LEAK] Attempting validation accuracy check using y_encoded + val_idx from notebook globals...\n",
      "[LEAK] Validation metrics computed from provided y_encoded + val_idx:\n",
      "  kingdom  acc=1.0000, f1_macro=1.0000\n",
      "  phylum   acc=1.0000, f1_macro=1.0000\n",
      "  class    acc=1.0000, f1_macro=1.0000\n",
      "  order    acc=1.0000, f1_macro=1.0000\n",
      "  family   acc=1.0000, f1_macro=1.0000\n",
      "  genus    acc=1.0000, f1_macro=1.0000\n",
      "  species  acc=1.0000, f1_macro=1.0000\n",
      "[LEAK] accession-level overlap between train and val: 250 accessions (first 10 shown): ['XR_013076714.1', 'XR_013076781.1', 'XR_013096750.1', 'JBPZNU010001311.1', 'XR_013099196.1', 'XR_013076353.1', 'LC871386.1', 'LC876539.1', 'LC876615.1', 'XR_013076372.1']\n",
      "[CALIB] Collecting logits for validation set...\n",
      "[CALIB] Collected logits for ranks: ['kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species']\n",
      "[CALIB] calibrating rank: kingdom\n",
      "  temp=0.2306 | ECE before=0.0110, after=0.0000\n",
      "[CALIB] calibrating rank: phylum\n",
      "  temp=0.2046 | ECE before=0.0525, after=0.0000\n",
      "[CALIB] calibrating rank: class\n",
      "  temp=0.2401 | ECE before=0.0570, after=0.0000\n",
      "[CALIB] calibrating rank: order\n",
      "  temp=0.1928 | ECE before=0.1231, after=0.0000\n",
      "[CALIB] calibrating rank: family\n",
      "  temp=0.2028 | ECE before=0.1494, after=0.0000\n",
      "[CALIB] calibrating rank: genus\n",
      "  temp=0.1931 | ECE before=0.2371, after=0.0000\n",
      "[CALIB] calibrating rank: species\n",
      "  temp=0.1975 | ECE before=0.4807, after=0.0000\n",
      "[CALIB] Applying temperatures to all data (batched). This will produce calibrated CSV/JSONL.\n",
      "[SAVE] Calibrated predictions saved: ncbi_blast_db\\extracted\\predictions_calibrated.jsonl, ncbi_blast_db\\extracted\\predictions_summary_calibrated.csv\n",
      "[SAVE] Novel candidates summary saved: ncbi_blast_db\\extracted\\novel_candidates.csv\n",
      "[SAVE] cluster summary saved: ncbi_blast_db\\extracted\\cluster_summary.csv\n",
      "\n",
      "=== SHORT SUMMARY ===\n",
      "Total predictions: 2555\n",
      "Empty IDs fixed: 10; unique IDs now: 1626\n",
      "Unique novel candidate ids (novelty>0.8): 12 (see ncbi_blast_db\\extracted\\novel_candidates.csv)\n",
      "Per-rank temperature scaling applied. Example temps (first 5 ranks):\n",
      "  kingdom    T=0.2306 | ECE before=0.0110, after=0.0000\n",
      "  phylum     T=0.2046 | ECE before=0.0525, after=0.0000\n",
      "  class      T=0.2401 | ECE before=0.0570, after=0.0000\n",
      "  order      T=0.1928 | ECE before=0.1231, after=0.0000\n",
      "  family     T=0.2028 | ECE before=0.1494, after=0.0000\n",
      "\n",
      "RECOMMENDATIONS (next actions):\n",
      "  1) Re-split training/validation by accession/sample/cluster (leave-one-accession-out or by cruise) to avoid leakage.\n",
      "  2) Remove sequences used to build label encoders or use them only as an independent reference for cluster annotation.\n",
      "  3) Use hierarchical-aware losses (penalize coarse rank errors less), ensembles, and MC-dropout for uncertainty.\n",
      "  4) Use cluster-level annotation: cluster unknown sequences with HDBSCAN, then assign cluster-level taxonomy by nearest reference.\n",
      "  5) For discovery pipeline: prioritize novel candidates (high novelty_score, low nearest-neighbor similarity, high QC), and create an expert review list from NOVEL_CSV.\n",
      "\n",
      "Cell 13 complete. Files written: ncbi_blast_db\\extracted\\predictions_summary_calibrated.csv ncbi_blast_db\\extracted\\predictions_calibrated.jsonl ncbi_blast_db\\extracted\\novel_candidates.csv ncbi_blast_db\\extracted\\cluster_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 13 — Diagnostics, ID-fix, leakage checks, temperature scaling calibration, and calibrated exports\n",
    "import os, json, math, traceback\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import time\n",
    "\n",
    "# Paths (reuse)\n",
    "DOWNLOAD_DIR = Path(globals().get(\"DOWNLOAD_DIR\", \"./ncbi_blast_db\"))\n",
    "EXTRACT_DIR = DOWNLOAD_DIR / \"extracted\"\n",
    "PRED_JSONL = EXTRACT_DIR / \"predictions.jsonl\"\n",
    "PRED_SUM_CSV = EXTRACT_DIR / \"predictions_summary.csv\"\n",
    "META_CLUSTERED_CSV = EXTRACT_DIR / \"embeddings_meta_clustered.csv\"\n",
    "BEST_CKPT = EXTRACT_DIR / \"best_shared_heads.pt\"\n",
    "CALIB_JSONL = EXTRACT_DIR / \"predictions_calibrated.jsonl\"\n",
    "CALIB_CSV   = EXTRACT_DIR / \"predictions_summary_calibrated.csv\"\n",
    "NOVEL_CSV   = EXTRACT_DIR / \"novel_candidates.csv\"\n",
    "CLUSTER_SUM  = EXTRACT_DIR / \"cluster_summary.csv\"\n",
    "\n",
    "# small helpers\n",
    "def safe_load_jsonl(path):\n",
    "    recs = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as fh:\n",
    "        for ln in fh:\n",
    "            ln = ln.strip()\n",
    "            if not ln:\n",
    "                continue\n",
    "            try:\n",
    "                recs.append(json.loads(ln))\n",
    "            except Exception:\n",
    "                # try to recover with replace\n",
    "                try:\n",
    "                    recs.append(json.loads(ln.encode(\"utf-8\", errors=\"replace\").decode()))\n",
    "                except Exception:\n",
    "                    recs.append({\"_raw\": ln})\n",
    "    return recs\n",
    "\n",
    "def save_jsonl(list_of_dicts, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as fh:\n",
    "        for rec in list_of_dicts:\n",
    "            fh.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# 0) Basic checks\n",
    "for p in [PRED_JSONL, PRED_SUM_CSV, META_CLUSTERED_CSV]:\n",
    "    if not p.exists():\n",
    "        raise RuntimeError(f\"Required file missing: {p}\")\n",
    "\n",
    "print(\"[START] Loading files...\")\n",
    "pred_df = pd.read_csv(PRED_SUM_CSV, dtype=str, keep_default_na=False, na_filter=False)\n",
    "pred_recs = safe_load_jsonl(PRED_JSONL)\n",
    "meta = pd.read_csv(META_CLUSTERED_CSV, dtype=str, keep_default_na=False, na_filter=False)\n",
    "\n",
    "n_pred = len(pred_recs)\n",
    "n_df = len(pred_df)\n",
    "n_meta = len(meta)\n",
    "print(f\"  loaded: predictions.jsonl {n_pred} records; summary csv {n_df} rows; meta csv {n_meta} rows\")\n",
    "\n",
    "# 1) Fix empty or missing IDs in predictions (assumes order matches meta)\n",
    "fixed_count = 0\n",
    "for i, rec in enumerate(pred_recs):\n",
    "    rec_id = rec.get(\"id\", \"\")\n",
    "    if (not rec_id) and i < len(meta):\n",
    "        new_id = str(meta.loc[i, \"id\"]) if \"id\" in meta.columns else f\"seq_{i+1}\"\n",
    "        rec[\"id\"] = new_id\n",
    "        fixed_count += 1\n",
    "for i, row in pred_df.iterrows():\n",
    "    if (not str(row.get(\"id\", \"\")).strip()) and i < len(meta):\n",
    "        pred_df.at[i, \"id\"] = str(meta.loc[i, \"id\"]) if \"id\" in meta.columns else f\"seq_{i+1}\"\n",
    "        fixed_count += 1\n",
    "print(f\"[FIX] Filled {fixed_count} empty IDs (from meta).\")\n",
    "\n",
    "# Overwrite repaired files as backups\n",
    "backup_jsonl = EXTRACT_DIR / \"predictions.jsonl.bak\"\n",
    "backup_csv = EXTRACT_DIR / \"predictions_summary.csv.bak\"\n",
    "if not backup_jsonl.exists():\n",
    "    pred_json_text = open(PRED_JSONL, \"r\", encoding=\"utf-8\").read()\n",
    "    open(backup_jsonl, \"w\", encoding=\"utf-8\").write(pred_json_text)\n",
    "if not backup_csv.exists():\n",
    "    pred_df.to_csv(backup_csv, index=False)\n",
    "# Save fixed versions\n",
    "save_jsonl(pred_recs, PRED_JSONL)\n",
    "pred_df.to_csv(PRED_SUM_CSV, index=False)\n",
    "print(\"[SAVE] Saved fixed predictions.jsonl and predictions_summary.csv (backups created if not present).\")\n",
    "\n",
    "# 2) Basic diagnostics\n",
    "ids = pred_df[\"id\"].astype(str).tolist()\n",
    "empty_ids = sum(1 for x in ids if not x.strip())\n",
    "dup_count = sum(pred_df[\"id\"].duplicated())\n",
    "unique_ids = len(set(ids))\n",
    "print(f\"[DIAG] total: {len(ids)} rows, empty_ids={empty_ids}, duplicated_ids={dup_count}, unique_ids={unique_ids}\")\n",
    "\n",
    "# novel candidates (deduped)\n",
    "novel_thresh = 0.8\n",
    "pred_df[\"novelty_score\"] = pd.to_numeric(pred_df[\"novelty_score\"].replace(\"\", np.nan), errors=\"coerce\")\n",
    "nov_df = pred_df[pred_df[\"novelty_score\"].notna() & (pred_df[\"novelty_score\"] > novel_thresh)]\n",
    "unique_nov_ids = nov_df[\"id\"].unique().tolist()\n",
    "print(f\"[DIAG] novel candidates (>{novel_thresh}): {len(nov_df)} rows, {len(unique_nov_ids)} unique ids\")\n",
    "if len(unique_nov_ids) > 0:\n",
    "    print(\"  first 10 unique novel ids:\", unique_nov_ids[:10])\n",
    "\n",
    "# top clusters with most novel candidates\n",
    "if \"cluster_label\" in pred_df.columns:\n",
    "    cluster_counts = pred_df.groupby(\"cluster_label\").size().reset_index(name=\"n\")\n",
    "    novel_by_cluster = nov_df.groupby(\"cluster_label\").size().reset_index(name=\"novel_n\").sort_values(\"novel_n\", ascending=False)\n",
    "    merged = novel_by_cluster.merge(cluster_counts, on=\"cluster_label\", how=\"left\")\n",
    "    print(\"[DIAG] top clusters by novel candidate count:\")\n",
    "    print(merged.head(10).to_string(index=False))\n",
    "\n",
    "# per-rank predicted label counts (top 10)\n",
    "ranks = [c.replace(\"_pred\", \"\") for c in pred_df.columns if c.endswith(\"_pred\")]\n",
    "ranks = sorted(list(set(ranks)))\n",
    "print(\"[DIAG] Top predicted labels per rank (top 10):\")\n",
    "for r in ranks:\n",
    "    vc = pred_df[f\"{r}_pred\"].value_counts().head(10)\n",
    "    print(f\"  {r:10s}: {vc.to_dict()}\")\n",
    "\n",
    "# mean_confidence distribution\n",
    "pred_df[\"mean_confidence\"] = pd.to_numeric(pred_df[\"mean_confidence\"].replace(\"\", np.nan), errors=\"coerce\")\n",
    "print(\"[DIAG] mean_confidence summary:\")\n",
    "print(pred_df[\"mean_confidence\"].describe().to_string())\n",
    "\n",
    "# 3) Leakage check: if y_encoded & val_idx exist, compute exact validation accuracy from predictions_summary.csv\n",
    "can_eval = (\"y_encoded\" in globals()) and (\"val_idx\" in globals())\n",
    "if can_eval:\n",
    "    print(\"[LEAK] Attempting validation accuracy check using y_encoded + val_idx from notebook globals...\")\n",
    "    val_idx = np.array(globals()[\"val_idx\"], dtype=int)\n",
    "    # ensure pred_df aligned with index (we wrote predictions in same order as meta)\n",
    "    # for each rank, map predicted label string to encoder index (if present) and compare to y_encoded[r][val_idx]\n",
    "    val_metrics = {}\n",
    "    any_mismatch_ids = []\n",
    "    for r in globals().get(\"RANKS\", ranks):\n",
    "        le = globals().get(\"label_encoders\", {}).get(r, None)\n",
    "        if le is None:\n",
    "            continue\n",
    "        # retrieve predicted labels for val indices\n",
    "        pred_labels = pred_df.iloc[val_idx][f\"{r}_pred\"].astype(str).tolist()\n",
    "        # map to indices robustly\n",
    "        label_to_idx = {lab: int(i) for i, lab in enumerate(le.classes_)}\n",
    "        pred_indices = []\n",
    "        for pl in pred_labels:\n",
    "            if pl in label_to_idx:\n",
    "                pred_indices.append(label_to_idx[pl])\n",
    "            else:\n",
    "                # fallback: UNASSIGNED if exists, else 0\n",
    "                if \"UNASSIGNED\" in label_to_idx:\n",
    "                    pred_indices.append(label_to_idx[\"UNASSIGNED\"])\n",
    "                else:\n",
    "                    pred_indices.append(0)\n",
    "        true_indices = np.array(globals()[\"y_encoded\"][r])[val_idx]\n",
    "        acc = accuracy_score(true_indices, pred_indices)\n",
    "        f1m = f1_score(true_indices, pred_indices, average=\"macro\", zero_division=0)\n",
    "        val_metrics[r] = {\"acc\": acc, \"f1_macro\": f1m}\n",
    "    print(\"[LEAK] Validation metrics computed from provided y_encoded + val_idx:\")\n",
    "    for r, m in val_metrics.items():\n",
    "        print(f\"  {r:8s} acc={m['acc']:.4f}, f1_macro={m['f1_macro']:.4f}\")\n",
    "    # find accessions present in both train & val (accession-level overlap)\n",
    "    if \"train_idx\" in globals():\n",
    "        train_idx = np.array(globals()[\"train_idx\"], dtype=int)\n",
    "        train_ids = set(pred_df.iloc[train_idx][\"id\"].astype(str).tolist())\n",
    "        val_ids = set(pred_df.iloc[val_idx][\"id\"].astype(str).tolist())\n",
    "        overlap = train_ids.intersection(val_ids)\n",
    "        print(f\"[LEAK] accession-level overlap between train and val: {len(overlap)} accessions (first 10 shown): {list(overlap)[:10]}\")\n",
    "    else:\n",
    "        print(\"[LEAK] train_idx not present, cannot check accession-level overlap.\")\n",
    "else:\n",
    "    print(\"[LEAK] y_encoded or val_idx not found in globals; skipping exact validation-leakage metrics.\")\n",
    "\n",
    "# 4) Temperature scaling on validation logits: collect logits from heads for val set, per-rank calibrate temperature\n",
    "# Only run if 'shared' and 'heads' modules are present\n",
    "if (\"shared\" in globals()) and (\"heads\" in globals()) and (\"val_loader\" in globals()):\n",
    "    print(\"[CALIB] Collecting logits for validation set...\")\n",
    "    shared = globals()[\"shared\"]\n",
    "    heads = globals()[\"heads\"]\n",
    "    device = globals().get(\"device\", torch.device(\"cpu\"))\n",
    "    shared.to(device); heads.to(device)\n",
    "    shared.eval(); heads.eval()\n",
    "    # collect logits & labels\n",
    "    logits_val = {r: [] for r in label_encoders}\n",
    "    labels_val = {r: [] for r in label_encoders}\n",
    "    with torch.no_grad():\n",
    "        for batch in globals()[\"val_loader\"]:\n",
    "            x = batch[0].to(device)\n",
    "            h = shared(x)\n",
    "            for i, r in enumerate(list(label_encoders.keys())):\n",
    "                logits = heads[r](h).detach().cpu().numpy()\n",
    "                labels = batch[1 + i].numpy()\n",
    "                logits_val[r].append(logits)\n",
    "                labels_val[r].append(labels)\n",
    "    # stack\n",
    "    for r in logits_val:\n",
    "        logits_val[r] = np.vstack(logits_val[r])\n",
    "        labels_val[r] = np.concatenate(labels_val[r])\n",
    "    print(\"[CALIB] Collected logits for ranks:\", list(logits_val.keys()))\n",
    "\n",
    "    # helper: ECE\n",
    "    def expected_calibration_error(probs, labels, n_bins=15):\n",
    "        probs = np.asarray(probs)\n",
    "        labels = np.asarray(labels)\n",
    "        preds = np.argmax(probs, axis=1)\n",
    "        confs = np.max(probs, axis=1)\n",
    "        ece = 0.0\n",
    "        bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "        for i in range(n_bins):\n",
    "            lo, hi = bins[i], bins[i+1]\n",
    "            mask = (confs > lo) & (confs <= hi)\n",
    "            if mask.sum() == 0:\n",
    "                continue\n",
    "            acc = (preds[mask] == labels[mask]).mean()\n",
    "            avg_conf = confs[mask].mean()\n",
    "            ece += (mask.sum() / len(probs)) * abs(avg_conf - acc)\n",
    "        return float(ece)\n",
    "\n",
    "    # optimize temperature per rank (simple Adam optimization on scalar T)\n",
    "    temps = {}\n",
    "    ece_before = {}\n",
    "    ece_after = {}\n",
    "    for r in logits_val:\n",
    "        print(f\"[CALIB] calibrating rank: {r}\")\n",
    "        logits_r = torch.tensor(logits_val[r], dtype=torch.float32, device=device)\n",
    "        labels_r = torch.tensor(labels_val[r], dtype=torch.long, device=device)\n",
    "        # compute probs before\n",
    "        probs_before = F.softmax(logits_r, dim=1).cpu().numpy()\n",
    "        ece_b = expected_calibration_error(probs_before, labels_r.cpu().numpy(), n_bins=15)\n",
    "        ece_before[r] = ece_b\n",
    "        # temp param\n",
    "        T = torch.nn.Parameter(torch.ones(1, device=device) * 1.0)\n",
    "        optT = torch.optim.LBFGS([T], lr=0.5, max_iter=50, line_search_fn='strong_wolfe')\n",
    "        # closure for LBFGS\n",
    "        def closure():\n",
    "            optT.zero_grad()\n",
    "            # numeric stability: clamp T > 1e-3\n",
    "            t = T.clamp(min=1e-3)\n",
    "            loss = F.cross_entropy(logits_r / t, labels_r)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        try:\n",
    "            optT.step(closure)\n",
    "            T_opt = float(T.clamp(min=1e-3).item())\n",
    "        except Exception as e:\n",
    "            # fallback to small Adam loop\n",
    "            T = torch.nn.Parameter(torch.tensor(1.0, device=device))\n",
    "            opt = torch.optim.Adam([T], lr=0.01)\n",
    "            for _ in range(200):\n",
    "                opt.zero_grad()\n",
    "                t = T.clamp(min=1e-3)\n",
    "                loss = F.cross_entropy(logits_r / t, labels_r)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "            T_opt = float(T.clamp(min=1e-3).item())\n",
    "        temps[r] = T_opt\n",
    "        # compute ece after\n",
    "        probs_after = F.softmax(torch.tensor(logits_val[r], dtype=torch.float32, device=\"cpu\") / T_opt, dim=1).cpu().numpy()\n",
    "        ece_a = expected_calibration_error(probs_after, labels_val[r], n_bins=15)\n",
    "        ece_after[r] = ece_a\n",
    "        print(f\"  temp={T_opt:.4f} | ECE before={ece_b:.4f}, after={ece_a:.4f}\")\n",
    "\n",
    "    # Apply temperatures to all logits and write calibrated predictions\n",
    "    print(\"[CALIB] Applying temperatures to all data (batched). This will produce calibrated CSV/JSONL.\")\n",
    "    batch = 512\n",
    "    n_tot = X_pca.shape[0]\n",
    "    out_recs_cal = []\n",
    "    rows_cal = []\n",
    "    # reload meta & predictions as baseline\n",
    "    for start in range(0, n_tot, batch):\n",
    "        end = min(n_tot, start+batch)\n",
    "        xb = torch.tensor(X_pca[start:end], dtype=torch.float32).to(device)\n",
    "        with torch.no_grad():\n",
    "            h = shared(xb)\n",
    "            for i in range(end-start):\n",
    "                idx = start + i\n",
    "                rec = pred_recs[idx].copy()\n",
    "                # per rank calibrated probs\n",
    "                mean_conf = 0.0; conf_count = 0\n",
    "                for r in label_encoders:\n",
    "                    logits_np = heads[r](h[i:i+1]).detach().cpu().numpy()[0]   # (n_classes,)\n",
    "                    t = temps.get(r, 1.0)\n",
    "                    probs = F.softmax(torch.tensor(logits_np / t, dtype=torch.float32), dim=0).numpy()\n",
    "                    top_idx = int(np.argmax(probs))\n",
    "                    top_prob = float(probs[top_idx])\n",
    "                    # top-3\n",
    "                    topk_idx = np.argsort(-probs)[:3]\n",
    "                    topk = [{\"label\": str(label_encoders[r].classes_[k]), \"prob\": float(probs[k])} for k in topk_idx]\n",
    "                    # update rec\n",
    "                    rec.setdefault(\"predicted\", {})\n",
    "                    rec[\"predicted\"][r] = {\"predicted_label\": str(label_encoders[r].classes_[top_idx]),\n",
    "                                           \"predicted_index\": top_idx,\n",
    "                                           \"predicted_prob\": top_prob,\n",
    "                                           \"top_k\": topk}\n",
    "                    mean_conf += top_prob\n",
    "                    conf_count += 1\n",
    "                rec[\"mean_confidence_calibrated\"] = mean_conf / max(1, conf_count)\n",
    "                out_recs_cal.append(rec)\n",
    "                # CSV row\n",
    "                row = {\"id\": rec.get(\"id\", f\"seq_{idx}\"), \"mean_confidence_calibrated\": rec[\"mean_confidence_calibrated\"],\n",
    "                       \"cluster_label\": pred_df.loc[idx, \"cluster_label\"] if \"cluster_label\" in pred_df.columns else \"\"}\n",
    "                # add per-rank pred/prob\n",
    "                for r in label_encoders:\n",
    "                    row[f\"{r}_pred\"] = rec[\"predicted\"][r][\"predicted_label\"]\n",
    "                    row[f\"{r}_prob\"] = rec[\"predicted\"][r][\"predicted_prob\"]\n",
    "                rows_cal.append(row)\n",
    "    # write calibrated outputs\n",
    "    save_jsonl(out_recs_cal, CALIB_JSONL)\n",
    "    pd.DataFrame(rows_cal).to_csv(CALIB_CSV, index=False)\n",
    "    print(f\"[SAVE] Calibrated predictions saved: {CALIB_JSONL}, {CALIB_CSV}\")\n",
    "\n",
    "else:\n",
    "    print(\"[CALIB] Cannot calibrate: 'shared' or 'heads' or 'val_loader' not found in globals. Skipping calibration.\")\n",
    "\n",
    "# 5) Save novel candidates summary CSV and cluster-level biodiversity summary\n",
    "novel_unique_rows = pred_df[pred_df[\"id\"].isin(unique_nov_ids)].drop_duplicates(subset=[\"id\"])\n",
    "if not novel_unique_rows.empty:\n",
    "    novel_unique_rows.to_csv(NOVEL_CSV, index=False)\n",
    "    print(f\"[SAVE] Novel candidates summary saved: {NOVEL_CSV}\")\n",
    "\n",
    "# cluster summary: how many sequences, how many novel candidates, mean novelty, mean confidence\n",
    "if \"cluster_label\" in pred_df.columns:\n",
    "    cluster_summary = pred_df.groupby(\"cluster_label\").agg(\n",
    "        n_sequences = (\"id\", \"count\"),\n",
    "        novel_count = (\"novelty_score\", lambda s: int((pd.to_numeric(s, errors='coerce') > novel_thresh).sum())),\n",
    "        mean_novelty = (\"novelty_score\", lambda s: pd.to_numeric(s, errors='coerce').mean()),\n",
    "        mean_confidence = (\"mean_confidence\", lambda s: pd.to_numeric(s, errors='coerce').mean())\n",
    "    ).reset_index().sort_values(\"n_sequences\", ascending=False)\n",
    "    cluster_summary.to_csv(CLUSTER_SUM, index=False)\n",
    "    print(f\"[SAVE] cluster summary saved: {CLUSTER_SUM}\")\n",
    "\n",
    "# 6) Print short human-readable summary & recommendations\n",
    "print(\"\\n=== SHORT SUMMARY ===\")\n",
    "print(f\"Total predictions: {len(pred_recs)}\")\n",
    "print(f\"Empty IDs fixed: {fixed_count}; unique IDs now: {len(set([r['id'] for r in pred_recs]))}\")\n",
    "print(f\"Unique novel candidate ids (novelty>{novel_thresh}): {len(unique_nov_ids)} (see {NOVEL_CSV})\")\n",
    "if 'temps' in locals():\n",
    "    print(\"Per-rank temperature scaling applied. Example temps (first 5 ranks):\")\n",
    "    for r, t in list(temps.items())[:5]:\n",
    "        print(f\"  {r:10s} T={t:.4f} | ECE before={ece_before.get(r):.4f}, after={ece_after.get(r):.4f}\")\n",
    "print(\"\\nRECOMMENDATIONS (next actions):\")\n",
    "print(\"  1) Re-split training/validation by accession/sample/cluster (leave-one-accession-out or by cruise) to avoid leakage.\")\n",
    "print(\"  2) Remove sequences used to build label encoders or use them only as an independent reference for cluster annotation.\")\n",
    "print(\"  3) Use hierarchical-aware losses (penalize coarse rank errors less), ensembles, and MC-dropout for uncertainty.\")\n",
    "print(\"  4) Use cluster-level annotation: cluster unknown sequences with HDBSCAN, then assign cluster-level taxonomy by nearest reference.\")\n",
    "print(\"  5) For discovery pipeline: prioritize novel candidates (high novelty_score, low nearest-neighbor similarity, high QC), and create an expert review list from NOVEL_CSV.\")\n",
    "print(\"\\nCell 13 complete. Files written:\", CALIB_CSV, CALIB_JSONL, NOVEL_CSV, CLUSTER_SUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4ee591e-e6bc-4ce2-86a1-253c9dd436a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD] samples=2555, X_pca.shape=(2555, 64), meta rows=2555\n",
      "[GROUPS] unique accession groups (acc_base): 1630\n",
      "[SPLIT] train_samples=2182, val_samples=373, train_groups=1385, val_groups=245\n",
      "[SAVE] saved train/val group split to ncbi_blast_db\\extracted\\train_val_split_by_accession.json\n",
      "[DATA] train_ds=2182, val_ds=373; batch_size=128\n",
      "[SCHED WARN] Could not create ReduceLROnPlateau, using DummyScheduler: type object 'ReduceLROnPlateau' has no attribute '_init_'\n",
      "Epoch 001 | train_loss 16.6280 | val_agg_f1 1.0000 | time 0.3s\n",
      "  [CHECKPOINT] saved new best (epoch 1, val_agg_f1 1.0000) -> best_shared_heads_retrain.pt\n",
      "Epoch 002 | train_loss 6.7478 | val_agg_f1 1.0000 | time 408.3s\n",
      "Epoch 003 | train_loss 0.3275 | val_agg_f1 1.0000 | time 0.4s\n",
      "Epoch 004 | train_loss 0.0071 | val_agg_f1 1.0000 | time 0.4s\n",
      "Epoch 005 | train_loss 0.0030 | val_agg_f1 1.0000 | time 0.4s\n",
      "Epoch 006 | train_loss 0.0024 | val_agg_f1 1.0000 | time 0.5s\n",
      "Epoch 007 | train_loss 0.0022 | val_agg_f1 1.0000 | time 0.6s\n",
      "Epoch 008 | train_loss 0.0020 | val_agg_f1 1.0000 | time 0.6s\n",
      "Epoch 009 | train_loss 0.0020 | val_agg_f1 1.0000 | time 0.5s\n",
      "[EARLY STOP] No improvement for 8 epochs (patience 8). Stopping.\n",
      "[TRAIN COMPLETE] epochs_run=9 best_val_agg_f1=1.0000 total_time_sec=411.9\n",
      "[SAVE] training history saved to ncbi_blast_db\\extracted\\training_history_retrain.csv\n",
      "[SAVE] saved train/val index arrays (train_idx_by_acc.npy, val_idx_by_acc.npy)\n",
      "[LOAD] loaded best checkpoint epoch 1, val_agg_f1 1.0\n",
      "=== Final validation metrics on group-wise split ===\n",
      "kingdom    acc=1.0, f1_macro=1.0\n",
      "phylum     acc=1.0, f1_macro=1.0\n",
      "class      acc=1.0, f1_macro=1.0\n",
      "order      acc=1.0, f1_macro=1.0\n",
      "family     acc=1.0, f1_macro=1.0\n",
      "genus      acc=1.0, f1_macro=1.0\n",
      "species    acc=1.0, f1_macro=1.0\n",
      "[TRAIN CLASS COUNTS] per-rank sample counts in training set (top classes):\n",
      "  kingdom   : classes=2, top5=[('UNASSIGNED', 2182)]\n",
      "  phylum    : classes=5, top5=[('UNASSIGNED', 2182)]\n",
      "  class     : classes=10, top5=[('UNASSIGNED', 2182)]\n",
      "  order     : classes=13, top5=[('UNASSIGNED', 2182)]\n",
      "  family    : classes=19, top5=[('UNASSIGNED', 2182)]\n",
      "  genus     : classes=27, top5=[('UNASSIGNED', 2182)]\n",
      "  species   : classes=182, top5=[('UNASSIGNED', 2182)]\n",
      "\n",
      "Cell 14 finished. Outputs produced:\n",
      "  - retrained best checkpoint: ncbi_blast_db\\extracted\\best_shared_heads_retrain.pt\n",
      "  - training history: ncbi_blast_db\\extracted\\training_history_retrain.csv\n",
      "  - group split JSON: ncbi_blast_db\\extracted\\train_val_split_by_accession.json\n",
      "  - saved train/val index arrays in extracted/\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Re-split by accession (group-wise), rebuild dataloaders, retrain model with no leakage\n",
    "# - Produces best_shared_heads_retrain.pt and training_history_retrain.csv\n",
    "# - Saves train/val split (train_val_split_by_accession.json)\n",
    "\n",
    "import json, time\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# --- Config & paths ---\n",
    "DOWNLOAD_DIR = Path(globals().get(\"DOWNLOAD_DIR\", \"./ncbi_blast_db\"))\n",
    "EXTRACT_DIR = DOWNLOAD_DIR / \"extracted\"\n",
    "EMB_PCA_NPY = EXTRACT_DIR / \"embeddings_pca.npy\"\n",
    "EMB_FULL_NPY = EXTRACT_DIR / \"embeddings.npy\"\n",
    "META_CLUSTERED_CSV = EXTRACT_DIR / \"embeddings_meta_clustered.csv\"\n",
    "\n",
    "OUT_CHECKPOINT = EXTRACT_DIR / \"best_shared_heads_retrain.pt\"\n",
    "OUT_HISTORY = EXTRACT_DIR / \"training_history_retrain.csv\"\n",
    "SPLIT_JSON = EXTRACT_DIR / \"train_val_split_by_accession.json\"\n",
    "\n",
    "# training hyperparams (tweakable)\n",
    "TEST_SIZE = 0.15\n",
    "SEED = 42\n",
    "BATCH_SIZE = 128\n",
    "MAX_EPOCHS = 50\n",
    "MIN_EPOCHS = 5\n",
    "PATIENCE = 8\n",
    "LR = 1e-3\n",
    "HIDDEN_DIM = 256\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- sanity checks & load data ---\n",
    "for p in (EMB_PCA_NPY, META_CLUSTERED_CSV):\n",
    "    if not Path(p).exists():\n",
    "        raise RuntimeError(f\"Missing required file: {p}\")\n",
    "\n",
    "X_pca = np.load(EMB_PCA_NPY)\n",
    "df_meta = pd.read_csv(META_CLUSTERED_CSV, dtype=str, keep_default_na=False, na_filter=False)\n",
    "\n",
    "n = X_pca.shape[0]\n",
    "if len(df_meta) != n:\n",
    "    mn = min(len(df_meta), n)\n",
    "    print(f\"[ALIGN] trimming to min_n={mn}\")\n",
    "    df_meta = df_meta.iloc[:mn].reset_index(drop=True)\n",
    "    X_pca = X_pca[:mn]\n",
    "    n = mn\n",
    "\n",
    "print(f\"[LOAD] samples={n}, X_pca.shape={X_pca.shape}, meta rows={len(df_meta)}\")\n",
    "\n",
    "# --- build accession base grouping key (acc_base) ---\n",
    "def get_acc_base(s):\n",
    "    s = str(s) if s is not None else \"\"\n",
    "    s = s.strip()\n",
    "    if s == \"\":\n",
    "        return None\n",
    "    # split by whitespace first then by dot (remove version)\n",
    "    token = s.split()[0]\n",
    "    return token.split(\".\")[0]\n",
    "\n",
    "df_meta[\"acc_base\"] = df_meta.get(\"id\", \"\").apply(get_acc_base)\n",
    "# fill missing acc_base with synthetic unique groups to avoid dropping records\n",
    "missing_acc_mask = df_meta[\"acc_base\"].isnull() | (df_meta[\"acc_base\"] == \"\")\n",
    "if missing_acc_mask.any():\n",
    "    # create deterministic synthetic group names based on index\n",
    "    df_meta.loc[missing_acc_mask, \"acc_base\"] = df_meta.loc[missing_acc_mask].index.to_series().apply(lambda i: f\"_missing_acc{i}\")\n",
    "\n",
    "n_groups = df_meta[\"acc_base\"].nunique()\n",
    "print(f\"[GROUPS] unique accession groups (acc_base): {n_groups}\")\n",
    "\n",
    "# --- ensure y_encoded exists in globals (we used it earlier) ---\n",
    "if \"y_encoded\" not in globals():\n",
    "    raise RuntimeError(\"y_encoded not found in notebook globals. Ensure that label encoders & y_encoded were created in earlier cells.\")\n",
    "\n",
    "y_encoded = globals()[\"y_encoded\"]\n",
    "\n",
    "# Align y_encoded arrays length\n",
    "for r in list(y_encoded.keys()):\n",
    "    arr = np.asarray(y_encoded[r], dtype=int)\n",
    "    if len(arr) != n:\n",
    "        if len(arr) < n:\n",
    "            arr = np.concatenate([arr, np.zeros(n - len(arr), dtype=int)])\n",
    "        else:\n",
    "            arr = arr[:n]\n",
    "        y_encoded[r] = arr\n",
    "    else:\n",
    "        y_encoded[r] = arr\n",
    "\n",
    "# --- group split by acc_base (GroupShuffleSplit) ---\n",
    "groups = df_meta[\"acc_base\"].values\n",
    "indices = np.arange(n)\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=SEED)\n",
    "try:\n",
    "    train_idx_grp, val_idx_grp = next(gss.split(indices, groups=groups))\n",
    "except Exception as e:\n",
    "    # fallback to random split if GroupShuffleSplit fails\n",
    "    print(\"[WARN] GroupShuffleSplit failed:\", e)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_idx_grp, val_idx_grp = train_test_split(indices, test_size=TEST_SIZE, random_state=SEED, shuffle=True)\n",
    "\n",
    "# verify no accession overlap\n",
    "train_acc = set(df_meta.loc[train_idx_grp, \"acc_base\"].unique().tolist())\n",
    "val_acc   = set(df_meta.loc[val_idx_grp, \"acc_base\"].unique().tolist())\n",
    "overlap = train_acc.intersection(val_acc)\n",
    "if len(overlap) != 0:\n",
    "    raise RuntimeError(f\"Group split produced overlapping accession groups between train and val ({len(overlap)} overlaps). Abort.\")\n",
    "\n",
    "print(f\"[SPLIT] train_samples={len(train_idx_grp)}, val_samples={len(val_idx_grp)}, train_groups={len(train_acc)}, val_groups={len(val_acc)}\")\n",
    "\n",
    "# save split groups for reproducibility\n",
    "with open(SPLIT_JSON, \"w\") as fh:\n",
    "    json.dump({\"train_groups\": list(train_acc), \"val_groups\": list(val_acc), \"seed\": SEED}, fh)\n",
    "print(f\"[SAVE] saved train/val group split to {SPLIT_JSON}\")\n",
    "\n",
    "# --- compute train-only class frequencies & class_weights per rank ---\n",
    "label_encoders = globals().get(\"label_encoders\", None)\n",
    "if label_encoders is None:\n",
    "    raise RuntimeError(\"label_encoders not found in globals; cannot construct class weights.\")\n",
    "\n",
    "class_weights_train = {}\n",
    "for r in label_encoders:\n",
    "    ncls = len(label_encoders[r].classes_)\n",
    "    counts = np.bincount(y_encoded[r][train_idx_grp], minlength=ncls).astype(float)\n",
    "    # avoid zeros\n",
    "    counts = np.where(counts <= 0, 1.0, counts)\n",
    "    w = (1.0 / counts)\n",
    "    w = w / w.sum() * len(w)   # scale\n",
    "    class_weights_train[r] = torch.tensor(w, dtype=torch.float32)\n",
    "\n",
    "# --- build TensorDatasets & DataLoaders (group-safe) ---\n",
    "X_tensor = torch.tensor(X_pca, dtype=torch.float32)\n",
    "y_tensors = {r: torch.tensor(y_encoded[r], dtype=torch.long) for r in label_encoders}\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_X = X_tensor[train_idx_grp]\n",
    "val_X   = X_tensor[val_idx_grp]\n",
    "train_y_list = [y_tensors[r][train_idx_grp] for r in label_encoders]\n",
    "val_y_list   = [y_tensors[r][val_idx_grp] for r in label_encoders]\n",
    "\n",
    "train_ds_grp = TensorDataset(train_X, *train_y_list)\n",
    "val_ds_grp   = TensorDataset(val_X, *val_y_list)\n",
    "train_loader_grp = DataLoader(train_ds_grp, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "val_loader_grp   = DataLoader(val_ds_grp,   batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "print(f\"[DATA] train_ds={len(train_ds_grp)}, val_ds={len(val_ds_grp)}; batch_size={BATCH_SIZE}\")\n",
    "\n",
    "# --- robust scheduler helper (same as used earlier) ---\n",
    "import inspect\n",
    "def create_reduce_on_plateau_scheduler(opt, mode=\"max\", factor=0.5, patience=3, min_lr=1e-6):\n",
    "    try:\n",
    "        ctor = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "        sig = inspect.signature(ctor._init_)\n",
    "        allowed = set(sig.parameters.keys()) - {\"self\", \"args\", \"kwargs\"}\n",
    "        kwargs = {}\n",
    "        cand = {\"mode\": mode, \"factor\": factor, \"patience\": patience, \"min_lr\": min_lr}\n",
    "        for k, v in cand.items():\n",
    "            if k in allowed:\n",
    "                kwargs[k] = v\n",
    "        if \"verbose\" in allowed:\n",
    "            kwargs[\"verbose\"] = False\n",
    "        scheduler = ctor(opt, **kwargs)\n",
    "        return scheduler\n",
    "    except Exception as e:\n",
    "        class DummyScheduler:\n",
    "            def step(self, metric=None): return None\n",
    "        print(\"[SCHED WARN] Could not create ReduceLROnPlateau, using DummyScheduler:\", e)\n",
    "        return DummyScheduler()\n",
    "\n",
    "# --- build fresh model parts (shared_new + heads_new) ---\n",
    "input_dim = X_pca.shape[1]\n",
    "shared_new = nn.Sequential(\n",
    "    nn.Linear(input_dim, HIDDEN_DIM),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(HIDDEN_DIM, HIDDEN_DIM//2),\n",
    "    nn.ReLU()\n",
    ")\n",
    "heads_new = nn.ModuleDict()\n",
    "for r in label_encoders:\n",
    "    heads_new[r] = nn.Linear(HIDDEN_DIM//2, len(label_encoders[r].classes_))\n",
    "\n",
    "# move to device\n",
    "shared_new.to(DEVICE)\n",
    "heads_new.to(DEVICE)\n",
    "\n",
    "# --- create criterions & optimizer (train-only class weights) ---\n",
    "criterions_new = {}\n",
    "for r in label_encoders:\n",
    "    w = class_weights_train[r].to(DEVICE)\n",
    "    criterions_new[r] = nn.CrossEntropyLoss(weight=w)\n",
    "\n",
    "params = list(shared_new.parameters()) + list(heads_new.parameters())\n",
    "optimizer_new = torch.optim.Adam(params, lr=LR)\n",
    "scheduler_new = create_reduce_on_plateau_scheduler(optimizer_new, mode=\"max\", factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "# --- training loop (early stopping on aggregated val mean macro-F1) ---\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_val_metrics_shared(shared_mod, heads_mod, loader, criterions, device):\n",
    "    shared_mod.eval(); heads_mod.eval()\n",
    "    preds = {r: [] for r in label_encoders}\n",
    "    trues = {r: [] for r in label_encoders}\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x = batch[0].to(device)\n",
    "            h = shared_mod(x)\n",
    "            for i, r in enumerate(label_encoders):\n",
    "                logits = heads_mod[r](h)\n",
    "                pred = torch.argmax(torch.softmax(logits, dim=1), dim=1).cpu().numpy()\n",
    "                true = batch[i+1].cpu().numpy()\n",
    "                preds[r].extend(pred.tolist())\n",
    "                trues[r].extend(true.tolist())\n",
    "    metrics = {}\n",
    "    f1s = []\n",
    "    for r in label_encoders:\n",
    "        try:\n",
    "            acc = accuracy_score(trues[r], preds[r])\n",
    "            f1m = f1_score(trues[r], preds[r], average=\"macro\", zero_division=0)\n",
    "        except Exception:\n",
    "            acc, f1m = None, None\n",
    "        metrics[r] = {\"acc\": acc, \"f1_macro\": f1m}\n",
    "        if f1m is not None:\n",
    "            f1s.append(f1m)\n",
    "    agg = float(np.mean(f1s)) if len(f1s)>0 else 0.0\n",
    "    return metrics, agg\n",
    "\n",
    "best_score = -np.inf\n",
    "epochs_no_improve = 0\n",
    "history = []\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(1, MAX_EPOCHS + 1):\n",
    "    t0 = time.time()\n",
    "    # training epoch\n",
    "    shared_new.train(); heads_new.train()\n",
    "    running_loss = 0.0; nb = 0\n",
    "    for batch in train_loader_grp:\n",
    "        x = batch[0].to(DEVICE)\n",
    "        targets = [batch[i+1].to(DEVICE) for i in range(len(label_encoders))]\n",
    "        h = shared_new(x)\n",
    "        outputs = {r: heads_new[r](h) for r in label_encoders}\n",
    "        loss = 0.0\n",
    "        for i, r in enumerate(label_encoders):\n",
    "            loss += criterions_new[r](outputs[r], targets[i])\n",
    "        optimizer_new.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(params, max_norm=5.0)\n",
    "        optimizer_new.step()\n",
    "        running_loss += float(loss.item())\n",
    "        nb += 1\n",
    "    train_loss = running_loss / max(1, nb)\n",
    "\n",
    "    # validation metrics\n",
    "    val_metrics, val_agg_f1 = compute_val_metrics_shared(shared_new, heads_new, val_loader_grp, criterions_new, DEVICE)\n",
    "\n",
    "    # scheduler step\n",
    "    try:\n",
    "        scheduler_new.step(val_agg_f1)\n",
    "    except TypeError:\n",
    "        try:\n",
    "            scheduler_new.step()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # logging\n",
    "    rec = {\"epoch\": epoch, \"train_loss\": train_loss, \"val_agg_f1\": val_agg_f1, \"time_sec\": time.time()-t0}\n",
    "    for r in label_encoders:\n",
    "        m = val_metrics.get(r, {})\n",
    "        rec.update({f\"{r}_acc\": m.get(\"acc\"), f\"{r}_f1\": m.get(\"f1_macro\")})\n",
    "    history.append(rec)\n",
    "    print(f\"Epoch {epoch:03d} | train_loss {train_loss:.4f} | val_agg_f1 {val_agg_f1:.4f} | time {rec['time_sec']:.1f}s\")\n",
    "\n",
    "    # checkpoint on improvement\n",
    "    if val_agg_f1 > best_score + 1e-8:\n",
    "        best_score = val_agg_f1\n",
    "        epochs_no_improve = 0\n",
    "        torch.save({\n",
    "            \"shared_state\": shared_new.state_dict(),\n",
    "            \"heads_state\": {r: heads_new[r].state_dict() for r in label_encoders},\n",
    "            \"epoch\": epoch,\n",
    "            \"val_agg_f1\": val_agg_f1,\n",
    "            \"optimizer_state\": optimizer_new.state_dict()\n",
    "        }, OUT_CHECKPOINT)\n",
    "        print(f\"  [CHECKPOINT] saved new best (epoch {epoch}, val_agg_f1 {val_agg_f1:.4f}) -> {OUT_CHECKPOINT.name}\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    # early stopping\n",
    "    if epoch >= MIN_EPOCHS and epochs_no_improve >= PATIENCE:\n",
    "        print(f\"[EARLY STOP] No improvement for {epochs_no_improve} epochs (patience {PATIENCE}). Stopping.\")\n",
    "        break\n",
    "\n",
    "# done training\n",
    "total_time = time.time() - start\n",
    "print(f\"[TRAIN COMPLETE] epochs_run={epoch} best_val_agg_f1={best_score:.4f} total_time_sec={total_time:.1f}\")\n",
    "\n",
    "# Save history\n",
    "pd.DataFrame(history).to_csv(OUT_HISTORY, index=False)\n",
    "print(f\"[SAVE] training history saved to {OUT_HISTORY}\")\n",
    "\n",
    "# Save split indices for reproducibility\n",
    "np.save(EXTRACT_DIR / \"train_idx_by_acc.npy\", train_idx_grp)\n",
    "np.save(EXTRACT_DIR / \"val_idx_by_acc.npy\", val_idx_grp)\n",
    "print(\"[SAVE] saved train/val index arrays (train_idx_by_acc.npy, val_idx_by_acc.npy)\")\n",
    "\n",
    "# Basic post-train diagnostics\n",
    "# Load best checkpoint and compute validation metrics one final time\n",
    "ckpt = torch.load(OUT_CHECKPOINT, map_location=DEVICE)\n",
    "shared_new.load_state_dict(ckpt[\"shared_state\"])\n",
    "for r in label_encoders:\n",
    "    heads_new[r].load_state_dict(ckpt[\"heads_state\"][r])\n",
    "print(f\"[LOAD] loaded best checkpoint epoch {ckpt.get('epoch')}, val_agg_f1 {ckpt.get('val_agg_f1')}\")\n",
    "\n",
    "final_val_metrics, final_val_agg = compute_val_metrics_shared(shared_new, heads_new, val_loader_grp, criterions_new, DEVICE)\n",
    "print(\"=== Final validation metrics on group-wise split ===\")\n",
    "for r in label_encoders:\n",
    "    m = final_val_metrics.get(r, {})\n",
    "    print(f\"{r:10s} acc={m.get('acc')}, f1_macro={m.get('f1_macro')}\")\n",
    "\n",
    "# Print top-level summary: class counts in train\n",
    "print(\"[TRAIN CLASS COUNTS] per-rank sample counts in training set (top classes):\")\n",
    "for r in label_encoders:\n",
    "    idxs = y_encoded[r][train_idx_grp]\n",
    "    cnt = Counter(idxs)\n",
    "    # map top numeric indices back to label strings\n",
    "    top = cnt.most_common(5)\n",
    "    top_labels = [(label_encoders[r].classes_[i], c) for i,c in top]\n",
    "    print(f\"  {r:10s}: classes={len(label_encoders[r].classes_)}, top5={top_labels}\")\n",
    "\n",
    "print(\"\\nCell 14 finished. Outputs produced:\")\n",
    "print(f\"  - retrained best checkpoint: {OUT_CHECKPOINT}\")\n",
    "print(f\"  - training history: {OUT_HISTORY}\")\n",
    "print(f\"  - group split JSON: {SPLIT_JSON}\")\n",
    "print(f\"  - saved train/val index arrays in extracted/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f7c7ec2-f544-4a74-bb56-eb2a12ff95ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD] meta rows: 2555; columns: ['id', 'source_fasta', 'seq_len', 'n_kmers', 'n_kmers_missing', 'zero_vector_fallback', 'PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6']\n",
      "[LOAD] Found saved label encoders: label_encoders_used.pkl\n",
      "[FETCHED] ssu: records=468\n",
      "[FETCHED] lsu: records=406\n",
      "[FETCHED] its: records=699\n",
      "[LOOKUPS] exact_acc=1231, acc_base=1231, org_names=362\n",
      "[ASSIGN] total=2555 matched=1131 no_match=1424\n",
      "[SAVE] wrote assignment debug CSV: ncbi_blast_db\\extracted\\label_assignment_debug.csv\n",
      "\n",
      "[SUMMARIES] per-rank assigned counts (from fetched metadata):\n",
      "  kingdom : assigned_count=1131 (44.27%)\n",
      "  phylum  : assigned_count=1131 (44.27%)\n",
      "  class   : assigned_count=1131 (44.27%)\n",
      "  order   : assigned_count=1131 (44.27%)\n",
      "  family  : assigned_count=1131 (44.27%)\n",
      "  genus   : assigned_count=1131 (44.27%)\n",
      "  species : assigned_count=1131 (44.27%)\n",
      "\n",
      "[EXAMPLES] sequences with assigned species (first 10):\n",
      " index                id        acc_base matched_source  assigned_genus       assigned_species meta_cluster_label        meta_novelty\n",
      "     0 JBJNTG020000075.1 JBJNTG020000075       acc_base  Eurotiomycetes  Aspergillus fumigatus                 79 0.12870961674642623\n",
      "     1 JBMETL020000032.1 JBMETL020000032       acc_base  Eurotiomycetes  Aspergillus fumigatus                 80  0.1862074213615924\n",
      "     2 JBMETL020000181.1 JBMETL020000181       acc_base  Eurotiomycetes  Aspergillus fumigatus                 79 0.13660577831883325\n",
      "     3 JBJNTF020000077.1 JBJNTF020000077       acc_base  Eurotiomycetes  Aspergillus fumigatus                 79 0.12310381983274665\n",
      "     4 JBJNTF020000145.1 JBJNTF020000145       acc_base  Eurotiomycetes  Aspergillus fumigatus                 80 0.19308813124232593\n",
      "     5 JBJNTE020000075.1 JBJNTE020000075       acc_base  Eurotiomycetes  Aspergillus fumigatus                 79 0.14029073886055599\n",
      "     6 JBJNTE020000122.1 JBJNTE020000122       acc_base  Eurotiomycetes  Aspergillus fumigatus                 80   0.168517232217117\n",
      "     7 JBJYIQ010000081.1 JBJYIQ010000081       acc_base Sordariomycetes Fusarium odoratissimum                 79 0.15079120613909225\n",
      "     8 JBJYIQ010000490.1 JBJYIQ010000490       acc_base Sordariomycetes Fusarium odoratissimum                 -1  0.1377770905100223\n",
      "     9    XR_013100016.1    XR_013100016       acc_base    Euteleostomi        Maylandia zebra                 -1                 1.0\n",
      "\n",
      "[EXAMPLES] sequences with NO assignment (first 10):\n",
      " index         id acc_base meta_cluster_label        meta_novelty\n",
      "   294 JF836132.1 JF836132                 -1  0.3614425995534576\n",
      "   295 JF836131.1 JF836131                 61  0.2765946951564684\n",
      "   296 JF836130.1 JF836130                 61  0.3082319578641588\n",
      "   297 JF836129.1 JF836129                 61  0.2748558531956956\n",
      "   298 JF836128.1 JF836128                 61 0.27298583082741956\n",
      "   299 JF836127.1 JF836127                 62 0.38338006148824066\n",
      "   300 JF836126.1 JF836126                 61  0.3039117773646631\n",
      "   301 JF836125.1 JF836125                 -1  0.3767402712662121\n",
      "   302 JF836124.1 JF836124                 61 0.26602335334337657\n",
      "   303 JF836123.1 JF836123                 61 0.23551523108219308\n",
      "\n",
      "[CHECK] sequences with any assigned taxonomy: 1131 / 2555\n",
      "[REBUILD] Rebuilding label encoders from sequences with assigned taxonomy (best-effort).\n",
      "  rebuilt encoder for kingdom : n_classes=2 sample_classes=['Eukaryota', '_UNASSIGNED_']\n",
      "  rebuilt encoder for phylum  : n_classes=5 sample_classes=['Fungi', 'Metazoa', 'Sar', 'Viridiplantae', '_UNASSIGNED_']\n",
      "  rebuilt encoder for class   : n_classes=10 sample_classes=['Chlorophyta', 'Chordata', 'Cnidaria', 'Dikarya', 'Ecdysozoa', 'Fungi incertae sedis', 'Rhizaria', 'Stramenopiles', 'Streptophyta', '_UNASSIGNED_']\n",
      "  rebuilt encoder for order   : n_classes=13 sample_classes=['Arthropoda', 'Ascomycota', 'Basidiomycota', 'Cercozoa', 'Craniata', 'Embryophyta', 'Klebsormidiophyceae', 'Mucoromycota', 'Myxozoa', 'Nematoda']\n",
      "  rebuilt encoder for family  : n_classes=19 sample_classes=['Agaricomycotina', 'Chelicerata', 'Chlorophyceae', 'Chromadorea', 'Discoceliidae', 'Enoplea', 'Glissomonadida', 'Imbricatea', 'Klebsormidiales', 'Mortierellomycotina']\n",
      "  rebuilt encoder for genus   : n_classes=27 sample_classes=['Agaricomycetes', 'Arachnida', 'Bivalvulida', 'CS clade', 'Dacrymycetes', 'Dipodascomycetes', 'Discocelia', 'Dorylaimia', 'Dothideomycetes', 'Eurotiomycetes']\n",
      "  rebuilt encoder for species : n_classes=182 sample_classes=['Agaricus argyropotamicus', 'Agaricus sp.', 'Alternaria alternata', 'Alternaria tenuissima', 'Amanita fulva', 'Amanita fuscozonata', 'Amanita sp.', 'Amphibiocapillaria tritonispunctati', 'Ancylostoma ceylanicum', 'Antarctomyces sp.']\n",
      "[SAVE] saved rebuilt encoders -> ncbi_blast_db\\extracted\\label_encoders_rebuilt_v2.pkl and y_encoded_rebuilt_*.npy\n",
      "\n",
      "=== DIAGNOSTIC SUMMARY & NEXT STEPS ===\n",
      "1) Assignment debug file saved (label_assignment_debug.csv). Inspect it to confirm matching logic is correct.\n",
      "2) If rebuilt encoders were produced, you can re-train on only labeled sequences by loading y_encoded_rebuilt_* arrays and using the train indices where the label != CANON_UNASSIGNED.\n",
      "3) If many sequences are still unlabeled, consider: (a) BLAST seeding; (b) cluster pseudo-labeling with HDBSCAN; (c) hierarchical coarse-label training.\n",
      "4) If you want, I will now produce the retrain cell that trains only on sequences with real labels (using the rebuilt encoders) — ready-to-run. Say 'do it' and I'll provide it as the next cell.\n"
     ]
    }
   ],
   "source": [
    "# Corrected Cell 15 — robust diagnostics & label reconstruction (fixed UNASSIGNED truncation issue)\n",
    "import json, pickle, traceback\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Paths\n",
    "DOWNLOAD_DIR = Path(globals().get(\"DOWNLOAD_DIR\", \"./ncbi_blast_db\"))\n",
    "EXTRACT_DIR  = DOWNLOAD_DIR / \"extracted\"\n",
    "META_CSV     = EXTRACT_DIR / \"embeddings_meta_clustered.csv\"\n",
    "LABEL_ENCODERS_PATH = EXTRACT_DIR / \"label_encoders_used.pkl\"\n",
    "FETCHED_JSONS = {m: EXTRACT_DIR / f\"{m}_fetched_metadata.json\" for m in (\"ssu\",\"lsu\",\"its\")}\n",
    "ASSIGN_DEBUG_CSV = EXTRACT_DIR / \"label_assignment_debug.csv\"\n",
    "REBUILT_ENCODERS_PKL = EXTRACT_DIR / \"label_encoders_rebuilt_v2.pkl\"\n",
    "\n",
    "CANON_UNASSIGNED = \"_UNASSIGNED_\"   # canonical placeholder (avoid accidental truncation)\n",
    "\n",
    "# Load meta\n",
    "if not META_CSV.exists():\n",
    "    raise RuntimeError(f\"Missing meta CSV: {META_CSV}\")\n",
    "df_meta = pd.read_csv(META_CSV, dtype=str, keep_default_na=False, na_filter=False)\n",
    "n_meta = len(df_meta)\n",
    "print(f\"[LOAD] meta rows: {n_meta}; columns: {list(df_meta.columns)[:12]}\")\n",
    "\n",
    "# Load saved encoders if present (for inspection)\n",
    "label_encoders_saved = None\n",
    "if LABEL_ENCODERS_PATH.exists():\n",
    "    try:\n",
    "        with open(LABEL_ENCODERS_PATH, \"rb\") as fh:\n",
    "            label_encoders_saved = pickle.load(fh)\n",
    "        print(f\"[LOAD] Found saved label encoders: {LABEL_ENCODERS_PATH.name}\")\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] failed to load saved label encoders:\", e)\n",
    "\n",
    "# Read fetched metadata JSONs (best-effort)\n",
    "fetched_records = {}\n",
    "for key, p in FETCHED_JSONS.items():\n",
    "    if not p.exists():\n",
    "        print(f\"[FETCHED] {key}: file not found at {p}\")\n",
    "        fetched_records[key] = []\n",
    "        continue\n",
    "    try:\n",
    "        recs = json.load(open(p, \"r\", encoding=\"utf-8\"))\n",
    "        fetched_records[key] = recs if isinstance(recs, list) else []\n",
    "        print(f\"[FETCHED] {key}: records={len(fetched_records[key])}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Could not parse {p}: {e}\")\n",
    "        fetched_records[key] = []\n",
    "\n",
    "# Build lookups: exact accession, acc_base, organism (lowercase)\n",
    "lookup_exact = {}\n",
    "lookup_base  = {}\n",
    "org_lookup = defaultdict(list)\n",
    "\n",
    "def normalize_str(s):\n",
    "    # ensure safe str, strip whitespace, replace problematic control chars\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = str(s)\n",
    "    s = s.strip()\n",
    "    # replace unprintable with empty\n",
    "    s = \"\".join(ch for ch in s if ch.isprintable())\n",
    "    return s\n",
    "\n",
    "for key, recs in fetched_records.items():\n",
    "    for rec in recs:\n",
    "        acc = normalize_str(rec.get(\"accession\") or rec.get(\"accession_version\") or rec.get(\"id\") or \"\")\n",
    "        if acc:\n",
    "            lookup_exact[acc] = rec\n",
    "            base = acc.split(\".\")[0]\n",
    "            lookup_base[base] = rec\n",
    "        org = normalize_str(rec.get(\"organism\") or rec.get(\"description\") or \"\")\n",
    "        if org:\n",
    "            org_lookup[org.lower()].append(rec)\n",
    "\n",
    "print(f\"[LOOKUPS] exact_acc={len(lookup_exact)}, acc_base={len(lookup_base)}, org_names={len(org_lookup)}\")\n",
    "\n",
    "# Build assignments: iterate df_meta and attempt to match each id to fetched metadata\n",
    "ranks = [\"kingdom\",\"phylum\",\"class\",\"order\",\"family\",\"genus\",\"species\"]\n",
    "assigned_rows = []\n",
    "no_match = 0\n",
    "\n",
    "for idx, row in df_meta.iterrows():\n",
    "    seq_id = normalize_str(row.get(\"id\",\"\"))\n",
    "    acc_base = seq_id.split()[0].split(\".\")[0] if seq_id else \"\"\n",
    "    rec = None\n",
    "    matched_source = \"\"\n",
    "    if seq_id and seq_id in lookup_exact:\n",
    "        rec = lookup_exact[seq_id]; matched_source = \"exact_acc\"\n",
    "    elif acc_base and acc_base in lookup_base:\n",
    "        rec = lookup_base[acc_base]; matched_source = \"acc_base\"\n",
    "    else:\n",
    "        # try substring match in organism names (cheap)\n",
    "        found = None\n",
    "        seq_low = seq_id.lower()\n",
    "        for orgname, recs in org_lookup.items():\n",
    "            if orgname and orgname in seq_low:\n",
    "                found = recs[0]\n",
    "                matched_source = \"org_substr\"\n",
    "                break\n",
    "        if found:\n",
    "            rec = found\n",
    "    label_map = {r: \"\" for r in ranks}\n",
    "    if rec:\n",
    "        taxonomy = rec.get(\"taxonomy\") or []\n",
    "        # taxonomy list often is ordered from kingdom downwards\n",
    "        for i, rank_name in enumerate([\"kingdom\",\"phylum\",\"class\",\"order\",\"family\",\"genus\"]):\n",
    "            if i < len(taxonomy) and taxonomy[i]:\n",
    "                label_map[rank_name] = normalize_str(taxonomy[i])\n",
    "        # species from organism/description\n",
    "        organism = normalize_str(rec.get(\"organism\") or rec.get(\"description\") or \"\")\n",
    "        parts = organism.split()\n",
    "        if len(parts) >= 2:\n",
    "            label_map[\"genus\"] = label_map.get(\"genus\") or normalize_str(parts[0])\n",
    "            label_map[\"species\"] = normalize_str(\" \".join(parts[:2]))\n",
    "    else:\n",
    "        no_match += 1\n",
    "    assigned_rows.append({\n",
    "        \"index\": int(idx),\n",
    "        \"id\": seq_id,\n",
    "        \"acc_base\": acc_base,\n",
    "        \"matched_source\": matched_source,\n",
    "        \"has_match\": bool(bool(rec)),\n",
    "        **{f\"assigned_{r}\": (label_map[r] if label_map[r] else \"\") for r in ranks},\n",
    "        \"meta_cluster_label\": row.get(\"cluster_label\",\"\"),\n",
    "        \"meta_novelty\": row.get(\"novelty_score\",\"\"),\n",
    "        \"meta_organism\": row.get(\"organism\",\"\")\n",
    "    })\n",
    "\n",
    "df_assign = pd.DataFrame(assigned_rows)\n",
    "print(f\"[ASSIGN] total={len(df_assign)} matched={len(df_assign)-no_match} no_match={no_match}\")\n",
    "# Save debug assignment CSV (overwrite safely)\n",
    "df_assign.to_csv(ASSIGN_DEBUG_CSV, index=False)\n",
    "print(f\"[SAVE] wrote assignment debug CSV: {ASSIGN_DEBUG_CSV}\")\n",
    "\n",
    "# Summaries per-rank assigned counts\n",
    "print(\"\\n[SUMMARIES] per-rank assigned counts (from fetched metadata):\")\n",
    "for r in ranks:\n",
    "    col = f\"assigned_{r}\"\n",
    "    nonempty = df_assign[col].astype(str).replace(\"\", np.nan).notna().sum()\n",
    "    pct = 100.0 * nonempty / len(df_assign)\n",
    "    print(f\"  {r:8s}: assigned_count={nonempty} ({pct:.2f}%)\")\n",
    "\n",
    "# Examples\n",
    "print(\"\\n[EXAMPLES] sequences with assigned species (first 10):\")\n",
    "examples_assigned = df_assign[df_assign[\"assigned_species\"].astype(bool)].head(10)\n",
    "if not examples_assigned.empty:\n",
    "    print(examples_assigned[[\"index\",\"id\",\"acc_base\",\"matched_source\",\"assigned_genus\",\"assigned_species\",\"meta_cluster_label\",\"meta_novelty\"]].to_string(index=False))\n",
    "else:\n",
    "    print(\"  None found\")\n",
    "\n",
    "print(\"\\n[EXAMPLES] sequences with NO assignment (first 10):\")\n",
    "examples_unassigned = df_assign[~df_assign[\"has_match\"]].head(10)\n",
    "if not examples_unassigned.empty:\n",
    "    print(examples_unassigned[[\"index\",\"id\",\"acc_base\",\"meta_cluster_label\",\"meta_novelty\"]].to_string(index=False))\n",
    "else:\n",
    "    print(\"  None found\")\n",
    "\n",
    "# --- Rebuild encoders from sequences that have at least one assigned value ---\n",
    "assigned_any_mask = df_assign[[f\"assigned_{r}\" for r in ranks]].astype(bool).any(axis=1)\n",
    "n_assigned_any = assigned_any_mask.sum()\n",
    "print(f\"\\n[CHECK] sequences with any assigned taxonomy: {n_assigned_any} / {len(df_assign)}\")\n",
    "\n",
    "min_examples_threshold = 50\n",
    "if n_assigned_any < min_examples_threshold:\n",
    "    print(f\"[SKIP] Not enough assigned sequences to rebuild encoders (need >= {min_examples_threshold})\")\n",
    "else:\n",
    "    print(\"[REBUILD] Rebuilding label encoders from sequences with assigned taxonomy (best-effort).\")\n",
    "    df_good = df_assign[assigned_any_mask].copy()\n",
    "    # id->per-rank label (sanitized)\n",
    "    id2labels = {}\n",
    "    for _, r in df_good.iterrows():\n",
    "        id2labels[r[\"id\"]] = {f: (normalize_str(r[f]) if r[f] else CANON_UNASSIGNED) for f in [f\"assigned_{x}\" for x in ranks]}\n",
    "\n",
    "    new_encoders = {}\n",
    "    new_y_encoded = {}\n",
    "\n",
    "    for r in ranks:\n",
    "        col = f\"assigned_{r}\"\n",
    "        # labels present among good sequences (sanitized)\n",
    "        lbls_good = [normalize_str(x) if x else CANON_UNASSIGNED for x in df_good[col].astype(str).tolist()]\n",
    "        # ensure canonical unassigned present\n",
    "        unique_labels = sorted(set(lbls_good))\n",
    "        if CANON_UNASSIGNED not in unique_labels:\n",
    "            unique_labels.append(CANON_UNASSIGNED)\n",
    "        # Fit encoder on unique_labels\n",
    "        le = LabelEncoder()\n",
    "        le.fit(unique_labels)\n",
    "        new_encoders[r] = le\n",
    "\n",
    "        # Build full_labels aligned with df_meta (use id2labels map, fallback to CANON_UNASSIGNED)\n",
    "        full_labels = []\n",
    "        for idx, row in df_meta.iterrows():\n",
    "            curid = normalize_str(row.get(\"id\",\"\"))\n",
    "            lab = id2labels.get(curid, {}).get(col, CANON_UNASSIGNED)\n",
    "            if not lab:\n",
    "                lab = CANON_UNASSIGNED\n",
    "            full_labels.append(lab)\n",
    "        # Now transform safely\n",
    "        # As we fit le on unique_labels union CANON_UNASSIGNED, transform should not fail\n",
    "        transformed = le.transform(full_labels)\n",
    "        new_y_encoded[r] = np.array(transformed, dtype=int)\n",
    "        print(f\"  rebuilt encoder for {r:8s}: n_classes={len(le.classes_)} sample_classes={list(le.classes_)[:10]}\")\n",
    "\n",
    "    # Save rebuilt encoders and per-rank arrays\n",
    "    with open(REBUILT_ENCODERS_PKL, \"wb\") as fh:\n",
    "        pickle.dump(new_encoders, fh)\n",
    "    for r in ranks:\n",
    "        np.save(EXTRACT_DIR / f\"y_encoded_rebuilt_{r}.npy\", new_y_encoded[r])\n",
    "    print(f\"[SAVE] saved rebuilt encoders -> {REBUILT_ENCODERS_PKL} and y_encoded_rebuilt_*.npy\")\n",
    "\n",
    "print(\"\\n=== DIAGNOSTIC SUMMARY & NEXT STEPS ===\")\n",
    "print(\"1) Assignment debug file saved (label_assignment_debug.csv). Inspect it to confirm matching logic is correct.\")\n",
    "print(\"2) If rebuilt encoders were produced, you can re-train on only labeled sequences by loading y_encoded_rebuilt_* arrays and using the train indices where the label != CANON_UNASSIGNED.\")\n",
    "print(\"3) If many sequences are still unlabeled, consider: (a) BLAST seeding; (b) cluster pseudo-labeling with HDBSCAN; (c) hierarchical coarse-label training.\")\n",
    "print(\"4) If you want, I will now produce the retrain cell that trains only on sequences with real labels (using the rebuilt encoders) — ready-to-run. Say 'do it' and I'll provide it as the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c0a0e96-1ec9-4bd3-bdd8-28c90dfd4bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD] loaded rebuilt encoders from label_encoders_rebuilt_v2.pkl\n",
      "[LOAD] loaded y_encoded_rebuilt_kingdom.npy shape=(2555,)\n",
      "[LOAD] loaded y_encoded_rebuilt_phylum.npy shape=(2555,)\n",
      "[LOAD] loaded y_encoded_rebuilt_class.npy shape=(2555,)\n",
      "[LOAD] loaded y_encoded_rebuilt_order.npy shape=(2555,)\n",
      "[LOAD] loaded y_encoded_rebuilt_family.npy shape=(2555,)\n",
      "[LOAD] loaded y_encoded_rebuilt_genus.npy shape=(2555,)\n",
      "[LOAD] loaded y_encoded_rebuilt_species.npy shape=(2555,)\n",
      "[LOAD] Using full embeddings embeddings.npy shape=(2555, 128)\n",
      "[INFO] detected unassigned indices per rank: {'kingdom': 1, 'phylum': 4, 'class': 9, 'order': 11, 'family': 18, 'genus': 26, 'species': 181}\n",
      "[FILTER] labeled samples count: 1131 / 2555 (will train on these)\n",
      "[SPLIT] labeled train=963, labeled val=168; groups_train=651, groups_val=116\n",
      "[DATA] train_ds=963, val_ds=168; batch_size=128\n",
      "[WEIGHT] kingdom: n_classes=2, min_count=1, max_count=963\n",
      "[WEIGHT] phylum: n_classes=5, min_count=1, max_count=613\n",
      "[WEIGHT] class: n_classes=10, min_count=1, max_count=590\n",
      "[WEIGHT] order: n_classes=13, min_count=1, max_count=590\n",
      "[WEIGHT] family: n_classes=19, min_count=1, max_count=590\n",
      "[WEIGHT] genus: n_classes=27, min_count=1, max_count=590\n",
      "[WEIGHT] species: n_classes=182, min_count=1, max_count=466\n",
      "Epoch 001 | train_loss: 18.5801 | val_agg_f1: 0.1893 | time: 0.2s\n",
      "  [CHECKPOINT] saved new best model (epoch 1, val_agg_f1=0.1893) -> best_shared_heads_labeled.pt\n",
      "Epoch 002 | train_loss: 18.2596 | val_agg_f1: 0.2372 | time: 0.2s\n",
      "  [CHECKPOINT] saved new best model (epoch 2, val_agg_f1=0.2372) -> best_shared_heads_labeled.pt\n",
      "Epoch 003 | train_loss: 17.7915 | val_agg_f1: 0.2753 | time: 0.1s\n",
      "  [CHECKPOINT] saved new best model (epoch 3, val_agg_f1=0.2753) -> best_shared_heads_labeled.pt\n",
      "Epoch 004 | train_loss: 16.9157 | val_agg_f1: 0.3194 | time: 0.2s\n",
      "  [CHECKPOINT] saved new best model (epoch 4, val_agg_f1=0.3194) -> best_shared_heads_labeled.pt\n",
      "Epoch 005 | train_loss: 16.0938 | val_agg_f1: 0.3309 | time: 0.2s\n",
      "  [CHECKPOINT] saved new best model (epoch 5, val_agg_f1=0.3309) -> best_shared_heads_labeled.pt\n",
      "Epoch 006 | train_loss: 15.1150 | val_agg_f1: 0.4183 | time: 0.2s\n",
      "  [CHECKPOINT] saved new best model (epoch 6, val_agg_f1=0.4183) -> best_shared_heads_labeled.pt\n",
      "Epoch 007 | train_loss: 13.8085 | val_agg_f1: 0.5785 | time: 0.2s\n",
      "  [CHECKPOINT] saved new best model (epoch 7, val_agg_f1=0.5785) -> best_shared_heads_labeled.pt\n",
      "Epoch 008 | train_loss: 12.4594 | val_agg_f1: 0.6324 | time: 0.1s\n",
      "  [CHECKPOINT] saved new best model (epoch 8, val_agg_f1=0.6324) -> best_shared_heads_labeled.pt\n",
      "Epoch 009 | train_loss: 11.1819 | val_agg_f1: 0.6033 | time: 0.2s\n",
      "Epoch 010 | train_loss: 10.5670 | val_agg_f1: 0.6581 | time: 0.1s\n",
      "  [CHECKPOINT] saved new best model (epoch 10, val_agg_f1=0.6581) -> best_shared_heads_labeled.pt\n",
      "Epoch 011 | train_loss: 9.1646 | val_agg_f1: 0.6907 | time: 0.2s\n",
      "  [CHECKPOINT] saved new best model (epoch 11, val_agg_f1=0.6907) -> best_shared_heads_labeled.pt\n",
      "Epoch 012 | train_loss: 8.0582 | val_agg_f1: 0.7258 | time: 0.2s\n",
      "  [CHECKPOINT] saved new best model (epoch 12, val_agg_f1=0.7258) -> best_shared_heads_labeled.pt\n",
      "Epoch 013 | train_loss: 7.6008 | val_agg_f1: 0.7487 | time: 0.1s\n",
      "  [CHECKPOINT] saved new best model (epoch 13, val_agg_f1=0.7487) -> best_shared_heads_labeled.pt\n",
      "Epoch 014 | train_loss: 7.0236 | val_agg_f1: 0.7619 | time: 0.1s\n",
      "  [CHECKPOINT] saved new best model (epoch 14, val_agg_f1=0.7619) -> best_shared_heads_labeled.pt\n",
      "Epoch 015 | train_loss: 6.5194 | val_agg_f1: 0.7546 | time: 0.1s\n",
      "Epoch 016 | train_loss: 5.8522 | val_agg_f1: 0.7564 | time: 0.1s\n",
      "Epoch 017 | train_loss: 5.3395 | val_agg_f1: 0.7363 | time: 0.1s\n",
      "Epoch 018 | train_loss: 5.1994 | val_agg_f1: 0.7712 | time: 0.1s\n",
      "  [CHECKPOINT] saved new best model (epoch 18, val_agg_f1=0.7712) -> best_shared_heads_labeled.pt\n",
      "Epoch 019 | train_loss: 4.8302 | val_agg_f1: 0.7518 | time: 0.2s\n",
      "Epoch 020 | train_loss: 4.5471 | val_agg_f1: 0.7552 | time: 0.1s\n",
      "Epoch 021 | train_loss: 4.3701 | val_agg_f1: 0.7519 | time: 0.1s\n",
      "Epoch 022 | train_loss: 4.0693 | val_agg_f1: 0.7691 | time: 0.1s\n",
      "Epoch 023 | train_loss: 3.8311 | val_agg_f1: 0.7689 | time: 0.2s\n",
      "Epoch 024 | train_loss: 3.7300 | val_agg_f1: 0.7775 | time: 0.1s\n",
      "  [CHECKPOINT] saved new best model (epoch 24, val_agg_f1=0.7775) -> best_shared_heads_labeled.pt\n",
      "Epoch 025 | train_loss: 3.5184 | val_agg_f1: 0.7804 | time: 0.1s\n",
      "  [CHECKPOINT] saved new best model (epoch 25, val_agg_f1=0.7804) -> best_shared_heads_labeled.pt\n",
      "Epoch 026 | train_loss: 3.3760 | val_agg_f1: 0.7619 | time: 0.2s\n",
      "Epoch 027 | train_loss: 3.1759 | val_agg_f1: 0.7901 | time: 0.1s\n",
      "  [CHECKPOINT] saved new best model (epoch 27, val_agg_f1=0.7901) -> best_shared_heads_labeled.pt\n",
      "Epoch 028 | train_loss: 3.0592 | val_agg_f1: 0.8160 | time: 0.2s\n",
      "  [CHECKPOINT] saved new best model (epoch 28, val_agg_f1=0.8160) -> best_shared_heads_labeled.pt\n",
      "Epoch 029 | train_loss: 2.9312 | val_agg_f1: 0.8179 | time: 0.2s\n",
      "  [CHECKPOINT] saved new best model (epoch 29, val_agg_f1=0.8179) -> best_shared_heads_labeled.pt\n",
      "Epoch 030 | train_loss: 2.7576 | val_agg_f1: 0.7838 | time: 0.3s\n",
      "Epoch 031 | train_loss: 2.6649 | val_agg_f1: 0.7826 | time: 0.2s\n",
      "Epoch 032 | train_loss: 2.5426 | val_agg_f1: 0.8081 | time: 0.2s\n",
      "Epoch 033 | train_loss: 2.4113 | val_agg_f1: 0.8758 | time: 0.2s\n",
      "  [CHECKPOINT] saved new best model (epoch 33, val_agg_f1=0.8758) -> best_shared_heads_labeled.pt\n",
      "Epoch 034 | train_loss: 2.2877 | val_agg_f1: 0.8555 | time: 0.2s\n",
      "Epoch 035 | train_loss: 2.2643 | val_agg_f1: 0.8660 | time: 0.2s\n",
      "Epoch 036 | train_loss: 2.2509 | val_agg_f1: 0.8891 | time: 0.2s\n",
      "  [CHECKPOINT] saved new best model (epoch 36, val_agg_f1=0.8891) -> best_shared_heads_labeled.pt\n",
      "Epoch 037 | train_loss: 2.0507 | val_agg_f1: 0.8223 | time: 0.2s\n",
      "Epoch 038 | train_loss: 2.0061 | val_agg_f1: 0.8847 | time: 0.2s\n",
      "Epoch 039 | train_loss: 1.8462 | val_agg_f1: 0.8721 | time: 0.2s\n",
      "Epoch 040 | train_loss: 1.7981 | val_agg_f1: 0.8715 | time: 0.3s\n",
      "Epoch 041 | train_loss: 1.7259 | val_agg_f1: 0.8584 | time: 0.3s\n",
      "Epoch 042 | train_loss: 1.6759 | val_agg_f1: 0.8782 | time: 0.4s\n",
      "Epoch 043 | train_loss: 1.4998 | val_agg_f1: 0.8461 | time: 0.3s\n",
      "Epoch 044 | train_loss: 1.5237 | val_agg_f1: 0.8664 | time: 0.3s\n",
      "[EARLY STOP] No improvement for 8 epochs (patience=8). Stopping.\n",
      "[SAVE] history -> ncbi_blast_db\\extracted\\training_history_labeled.csv; metrics -> ncbi_blast_db\\extracted\\metrics_labeled.json\n",
      "[LOAD] loaded best checkpoint epoch 36, val_agg_f1=0.8891\n",
      "=== Final validation metrics on labeled split ===\n",
      "kingdom    | n_classes=2 | acc=1.0000 | f1_macro=1.0000 | mean_conf=1.0000\n",
      "phylum     | n_classes=5 | acc=1.0000 | f1_macro=1.0000 | mean_conf=0.9874\n",
      "class      | n_classes=10 | acc=1.0000 | f1_macro=1.0000 | mean_conf=0.9915\n",
      "order      | n_classes=13 | acc=0.9881 | f1_macro=0.9894 | mean_conf=0.9848\n",
      "family     | n_classes=19 | acc=0.9881 | f1_macro=0.9642 | mean_conf=0.9808\n",
      "genus      | n_classes=27 | acc=0.9821 | f1_macro=0.9516 | mean_conf=0.9431\n",
      "species    | n_classes=182 | acc=0.5655 | f1_macro=0.3185 | mean_conf=0.4267\n",
      "Aggregated mean-macro-F1: 0.8891\n",
      "\n",
      "[VAL] Top predicted labels per rank (val set, top 5):\n",
      "  kingdom   : [('Eukaryota', 168)]\n",
      "  phylum    : [('Metazoa', 105), ('Fungi', 50), ('Viridiplantae', 13)]\n",
      "  class     : [('Chordata', 98), ('Dikarya', 50), ('Chlorophyta', 11), ('Ecdysozoa', 5), ('Cnidaria', 2)]\n",
      "  order     : [('Craniata', 98), ('Basidiomycota', 30), ('Ascomycota', 20), ('core chlorophytes', 11), ('Nematoda', 5)]\n",
      "  family    : [('Vertebrata', 98), ('Agaricomycotina', 30), ('Pezizomycotina', 18), ('Trebouxiophyceae', 7), ('Enoplea', 4)]\n",
      "  genus     : [('Euteleostomi', 98), ('Agaricomycetes', 29), ('Pezizomycetes', 9), ('Sordariomycetes', 5), ('Prasiolales', 5)]\n",
      "  species   : [('Chaetodon auriga', 48), ('Maylandia zebra', 46), ('Omphalotus flagelliformis', 5), ('Bracteacoccus minor', 4), ('Cortinarius sp.', 3)]\n",
      "\n",
      "Cell complete. Outputs produced:\n",
      " - model checkpoint: ncbi_blast_db\\extracted\\best_shared_heads_labeled.pt\n",
      " - training history: ncbi_blast_db\\extracted\\training_history_labeled.csv\n",
      " - metrics json: ncbi_blast_db\\extracted\\metrics_labeled.json\n"
     ]
    }
   ],
   "source": [
    "# Cell: Retrain on labeled-only sequences (multi-head), group-wise split by accession\n",
    "# Save outputs: best_shared_heads_labeled.pt, training_history_labeled.csv, metrics_labeled.json\n",
    "\n",
    "import os, json, time\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# ---------------- Paths / config ----------------\n",
    "DOWNLOAD_DIR = Path(globals().get(\"DOWNLOAD_DIR\", \"./ncbi_blast_db\"))\n",
    "EXTRACT_DIR = DOWNLOAD_DIR / \"extracted\"\n",
    "\n",
    "ENC_REBUILT_PKL = EXTRACT_DIR / \"label_encoders_rebuilt_v2.pkl\"\n",
    "Y_REBUILT_PREFIX = EXTRACT_DIR / \"y_encoded_rebuilt_\"\n",
    "ASSIGN_DEBUG_CSV = EXTRACT_DIR / \"label_assignment_debug.csv\"\n",
    "EMB_FULL = EXTRACT_DIR / \"embeddings.npy\"\n",
    "EMB_PCA  = EXTRACT_DIR / \"embeddings_pca.npy\"\n",
    "\n",
    "OUT_CHECKPOINT = EXTRACT_DIR / \"best_shared_heads_labeled.pt\"\n",
    "OUT_HISTORY = EXTRACT_DIR / \"training_history_labeled.csv\"\n",
    "OUT_METRICS = EXTRACT_DIR / \"metrics_labeled.json\"\n",
    "\n",
    "RANKS = [\"kingdom\",\"phylum\",\"class\",\"order\",\"family\",\"genus\",\"species\"]\n",
    "SEED = 42\n",
    "TEST_SIZE = 0.15\n",
    "BATCH_SIZE = 128\n",
    "MAX_EPOCHS = 60\n",
    "MIN_EPOCHS = 5\n",
    "PATIENCE = 8\n",
    "LR = 1e-3\n",
    "HIDDEN_DIM = 256\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# ---------------- Load rebuilt encoders and labels ----------------\n",
    "if not ENC_REBUILT_PKL.exists():\n",
    "    raise RuntimeError(f\"Missing rebuilt encoders file {ENC_REBUILT_PKL}. Run Cell 15 to produce it.\")\n",
    "with open(ENC_REBUILT_PKL, \"rb\") as fh:\n",
    "    label_encoders = pickle.load(fh) if \"pickle\" in globals() else _import_(\"pickle\").load(fh)\n",
    "print(f\"[LOAD] loaded rebuilt encoders from {ENC_REBUILT_PKL.name}\")\n",
    "\n",
    "# Load y arrays per rank (full-length, aligned with meta)\n",
    "y_rebuilt = {}\n",
    "n_samples = None\n",
    "for r in RANKS:\n",
    "    p = Path(f\"{Y_REBUILT_PREFIX}{r}.npy\")\n",
    "    if not p.exists():\n",
    "        raise RuntimeError(f\"Missing y-encoded array for rank {r}: expected {p}\")\n",
    "    arr = np.load(p)\n",
    "    y_rebuilt[r] = arr.astype(int)\n",
    "    n_samples = len(arr) if n_samples is None else n_samples\n",
    "    print(f\"[LOAD] loaded y_encoded_rebuilt_{r}.npy shape={arr.shape}\")\n",
    "\n",
    "# ---------------- Load embeddings (prefer full embeddings) ----------------\n",
    "if EMB_FULL.exists():\n",
    "    X = np.load(EMB_FULL)\n",
    "    print(f\"[LOAD] Using full embeddings {EMB_FULL.name} shape={X.shape}\")\n",
    "elif EMB_PCA.exists():\n",
    "    X = np.load(EMB_PCA)\n",
    "    print(f\"[LOAD] Using PCA embeddings {EMB_PCA.name} shape={X.shape}\")\n",
    "else:\n",
    "    raise RuntimeError(\"No embeddings found (expected embeddings.npy or embeddings_pca.npy in extracted).\")\n",
    "\n",
    "if X.shape[0] != n_samples:\n",
    "    mn = min(X.shape[0], n_samples)\n",
    "    print(f\"[ALIGN] trimming to min_n={mn}\")\n",
    "    X = X[:mn]\n",
    "    for r in RANKS:\n",
    "        y_rebuilt[r] = y_rebuilt[r][:mn]\n",
    "    n_samples = mn\n",
    "\n",
    "# ---------------- Build labeled-only index mask ----------------\n",
    "# Detect index of UNASSIGNED in each encoder (robust)\n",
    "unassigned_index_by_rank = {}\n",
    "for r in RANKS:\n",
    "    classes = getattr(label_encoders[r], \"classes_\", None)\n",
    "    if classes is None:\n",
    "        raise RuntimeError(f\"Encoder for rank {r} missing 'classes_' attribute.\")\n",
    "    # try canonical names\n",
    "    uni = list(classes)\n",
    "    idx = None\n",
    "    for cand in (\"UNASSIGNED\", \"_UNASSIGNED_\", \"UNASSIGNE\", \"\"):\n",
    "        if cand in uni:\n",
    "            idx = uni.index(cand)\n",
    "            break\n",
    "    # fallback: if one class only, treat missing label as that single class (no unassigned)\n",
    "    if idx is None:\n",
    "        if len(uni) == 1:\n",
    "            idx = 0\n",
    "        else:\n",
    "            # if no explicit UNASSIGNED, try to find a class containing 'UNASSIGN' substring\n",
    "            idx = next((i for i,c in enumerate(uni) if \"UNASSIGN\" in str(c).upper()), None)\n",
    "            if idx is None:\n",
    "                # set to -1 meaning \"no explicit unassigned\"\n",
    "                idx = -1\n",
    "    unassigned_index_by_rank[r] = idx\n",
    "print(\"[INFO] detected unassigned indices per rank:\", unassigned_index_by_rank)\n",
    "\n",
    "# Build mask: labeled if ANY rank has label != unassigned_index (i.e., a real label)\n",
    "is_labeled = np.zeros(n_samples, dtype=bool)\n",
    "for r in RANKS:\n",
    "    idx_un = unassigned_index_by_rank[r]\n",
    "    if idx_un == -1:\n",
    "        # treat as labeled for all (no explicit unassigned)\n",
    "        is_labeled = is_labeled | np.ones(n_samples, dtype=bool)\n",
    "    else:\n",
    "        is_labeled = is_labeled | (y_rebuilt[r] != idx_un)\n",
    "\n",
    "n_labeled = int(is_labeled.sum())\n",
    "if n_labeled < 50:\n",
    "    raise RuntimeError(f\"Too few labeled samples ({n_labeled}) to train well. Need >=50. Inspect label_assignment_debug.csv.\")\n",
    "print(f\"[FILTER] labeled samples count: {n_labeled} / {n_samples} (will train on these)\")\n",
    "\n",
    "labeled_idx = np.nonzero(is_labeled)[0]\n",
    "\n",
    "# ---------------- Build accessions/groups for leakage-aware split ----------------\n",
    "if not ASSIGN_DEBUG_CSV.exists():\n",
    "    raise RuntimeError(f\"Missing {ASSIGN_DEBUG_CSV} — needed for accession grouping to avoid leakage.\")\n",
    "df_assign = pd.read_csv(ASSIGN_DEBUG_CSV, dtype=str, keep_default_na=False, na_filter=False)\n",
    "# ensure alignment: df_assign length should equal n_samples; else attempt to align by trimming/padding\n",
    "if len(df_assign) != n_samples:\n",
    "    print(f\"[WARN] assignment CSV length {len(df_assign)} != n_samples {n_samples}. Aligning by index min.\")\n",
    "    mn = min(len(df_assign), n_samples)\n",
    "    df_assign = df_assign.iloc[:mn].reset_index(drop=True)\n",
    "    X = X[:mn]\n",
    "    for r in RANKS:\n",
    "        y_rebuilt[r] = y_rebuilt[r][:mn]\n",
    "    labeled_idx = labeled_idx[labeled_idx < mn]\n",
    "    n_samples = mn\n",
    "    n_labeled = int(np.sum(is_labeled[:mn]))\n",
    "\n",
    "groups = df_assign[\"acc_base\"].astype(str).replace({\"\": \"_missing_acc_\"})\n",
    "group_array = groups.values\n",
    "\n",
    "# Group-wise split among labeled indices\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=SEED)\n",
    "train_idx, val_idx = None, None\n",
    "for tr, va in gss.split(labeled_idx, groups=group_array[labeled_idx], y=None):\n",
    "    train_idx = labeled_idx[tr]\n",
    "    val_idx = labeled_idx[va]\n",
    "print(f\"[SPLIT] labeled train={len(train_idx)}, labeled val={len(val_idx)}; groups_train={len(set(group_array[train_idx]))}, groups_val={len(set(group_array[val_idx]))}\")\n",
    "\n",
    "# verify no accession overlap\n",
    "if len(set(group_array[train_idx]).intersection(set(group_array[val_idx]))) > 0:\n",
    "    raise RuntimeError(\"Group split had overlapping accession groups — aborting.\")\n",
    "\n",
    "# ---------------- Build DataLoaders ----------------\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensors = {r: torch.tensor(y_rebuilt[r], dtype=torch.long) for r in RANKS}\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_X = X_tensor[train_idx]\n",
    "val_X   = X_tensor[val_idx]\n",
    "train_y_list = [y_tensors[r][train_idx] for r in RANKS]\n",
    "val_y_list   = [y_tensors[r][val_idx] for r in RANKS]\n",
    "\n",
    "train_ds = TensorDataset(train_X, *train_y_list)\n",
    "val_ds   = TensorDataset(val_X, *val_y_list)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False, num_workers=0)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=0)\n",
    "print(f\"[DATA] train_ds={len(train_ds)}, val_ds={len(val_ds)}; batch_size={BATCH_SIZE}\")\n",
    "\n",
    "# ---------------- Build model parts (shared + ModuleDict heads) ----------------\n",
    "input_dim = X.shape[1]\n",
    "shared_labeled = nn.Sequential(\n",
    "    nn.Linear(input_dim, HIDDEN_DIM),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(HIDDEN_DIM, HIDDEN_DIM//2),\n",
    "    nn.ReLU()\n",
    ")\n",
    "heads_labeled = nn.ModuleDict({ r: nn.Linear(HIDDEN_DIM//2, len(label_encoders[r].classes_)) for r in RANKS })\n",
    "\n",
    "shared_labeled.to(DEVICE)\n",
    "heads_labeled.to(DEVICE)\n",
    "\n",
    "# ---------------- Compute class weights from training set (train-only) ----------------\n",
    "class_weights = {}\n",
    "for r in RANKS:\n",
    "    ytr = train_y_list[RANKS.index(r)].numpy()\n",
    "    ncls = len(label_encoders[r].classes_)\n",
    "    counts = np.bincount(ytr, minlength=ncls).astype(float)\n",
    "    counts = np.where(counts <= 0, 1.0, counts)\n",
    "    weights = 1.0 / counts\n",
    "    weights = weights / weights.sum() * len(weights)\n",
    "    class_weights[r] = torch.tensor(weights, dtype=torch.float32).to(DEVICE)\n",
    "    print(f\"[WEIGHT] {r}: n_classes={ncls}, min_count={counts.min():.0f}, max_count={counts.max():.0f}\")\n",
    "\n",
    "# ---------------- Losses, optimizer, scheduler ----------------\n",
    "criterions = { r: nn.CrossEntropyLoss(weight=class_weights[r]) for r in RANKS }\n",
    "params = list(shared_labeled.parameters()) + list(heads_labeled.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=LR)\n",
    "\n",
    "# robust reduce-on-plateau creation\n",
    "import inspect\n",
    "def make_scheduler(opt, mode=\"max\", factor=0.5, patience=3, min_lr=1e-6):\n",
    "    try:\n",
    "        ctor = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "        sig = inspect.signature(ctor._init_)\n",
    "        allowed = set(sig.parameters.keys()) - {\"self\",\"args\",\"kwargs\"}\n",
    "        kwargs = {}\n",
    "        cand = {\"mode\": mode, \"factor\": factor, \"patience\": patience, \"min_lr\": min_lr}\n",
    "        for k,v in cand.items():\n",
    "            if k in allowed:\n",
    "                kwargs[k] = v\n",
    "        if \"verbose\" in allowed:\n",
    "            kwargs[\"verbose\"] = False\n",
    "        return ctor(opt, **kwargs)\n",
    "    except Exception:\n",
    "        class Dummy:\n",
    "            def step(self, metric=None): return None\n",
    "        return Dummy()\n",
    "\n",
    "scheduler = make_scheduler(optimizer, mode=\"max\", factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "# ---------------- Training / evaluation helpers ----------------\n",
    "def evaluate_model(shared_mod, heads_mod, loader, device):\n",
    "    shared_mod.eval(); heads_mod.eval()\n",
    "    preds = {r: [] for r in RANKS}\n",
    "    trues = {r: [] for r in RANKS}\n",
    "    confidences = {r: [] for r in RANKS}\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x = batch[0].to(device)\n",
    "            h = shared_mod(x)\n",
    "            for i, r in enumerate(RANKS):\n",
    "                logits = heads_mod[r](h)\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                top = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "                preds[r].extend(top.tolist())\n",
    "                trues[r].extend(batch[1+i].cpu().numpy().tolist())\n",
    "                confidences[r].extend(probs.max(dim=1).values.cpu().numpy().tolist())\n",
    "    metrics = {}\n",
    "    f1s = []\n",
    "    for r in RANKS:\n",
    "        try:\n",
    "            acc = accuracy_score(trues[r], preds[r])\n",
    "            f1m = f1_score(trues[r], preds[r], average=\"macro\", zero_division=0)\n",
    "            mean_conf = float(np.mean(confidences[r])) if len(confidences[r])>0 else None\n",
    "        except Exception:\n",
    "            acc, f1m, mean_conf = None, None, None\n",
    "        metrics[r] = {\"accuracy\": acc, \"f1_macro\": f1m, \"mean_confidence\": mean_conf, \"n_classes\": len(label_encoders[r].classes_)}\n",
    "        if f1m is not None:\n",
    "            f1s.append(f1m)\n",
    "    agg = float(np.mean(f1s)) if len(f1s)>0 else 0.0\n",
    "    return metrics, agg\n",
    "\n",
    "# ---------------- Training loop ----------------\n",
    "best_score = -np.inf\n",
    "epochs_no_improve = 0\n",
    "history = []\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(1, MAX_EPOCHS+1):\n",
    "    t0 = time.time()\n",
    "    shared_labeled.train(); heads_labeled.train()\n",
    "    running_loss = 0.0; nb = 0\n",
    "    for batch in train_loader:\n",
    "        x = batch[0].to(DEVICE)\n",
    "        targets = [batch[i+1].to(DEVICE) for i in range(len(RANKS))]\n",
    "        h = shared_labeled(x)\n",
    "        outputs = { r: heads_labeled[r](h) for r in RANKS }\n",
    "        loss = 0.0\n",
    "        for i, r in enumerate(RANKS):\n",
    "            loss = loss + criterions[r](outputs[r], targets[i])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(params, max_norm=5.0)\n",
    "        optimizer.step()\n",
    "        running_loss += float(loss.item())\n",
    "        nb += 1\n",
    "    train_loss = running_loss / max(1, nb)\n",
    "\n",
    "    # validation\n",
    "    val_metrics, val_agg = evaluate_model(shared_labeled, heads_labeled, val_loader, DEVICE)\n",
    "    # scheduler step (ReduceLROnPlateau accepts metric in steady versions but be defensive)\n",
    "    try:\n",
    "        scheduler.step(val_agg)\n",
    "    except TypeError:\n",
    "        try:\n",
    "            scheduler.step()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    rec = {\"epoch\": epoch, \"train_loss\": train_loss, \"val_agg_f1\": val_agg, \"time_sec\": time.time()-t0}\n",
    "    for r in RANKS:\n",
    "        m = val_metrics.get(r, {})\n",
    "        rec.update({f\"{r}_acc\": m.get(\"accuracy\"), f\"{r}_f1_macro\": m.get(\"f1_macro\"), f\"{r}_mean_conf\": m.get(\"mean_confidence\")})\n",
    "    history.append(rec)\n",
    "\n",
    "    print(f\"Epoch {epoch:03d} | train_loss: {train_loss:.4f} | val_agg_f1: {val_agg:.4f} | time: {rec['time_sec']:.1f}s\")\n",
    "\n",
    "    # checkpoint on improvement\n",
    "    if val_agg > best_score + 1e-8:\n",
    "        best_score = val_agg\n",
    "        epochs_no_improve = 0\n",
    "        torch.save({\n",
    "            \"shared_state\": shared_labeled.state_dict(),\n",
    "            \"heads_state\": {r: heads_labeled[r].state_dict() for r in RANKS},\n",
    "            \"epoch\": epoch,\n",
    "            \"val_agg_f1\": val_agg,\n",
    "            \"optimizer_state\": optimizer.state_dict()\n",
    "        }, OUT_CHECKPOINT)\n",
    "        print(f\"  [CHECKPOINT] saved new best model (epoch {epoch}, val_agg_f1={val_agg:.4f}) -> {OUT_CHECKPOINT.name}\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    # early stopping\n",
    "    if epoch >= MIN_EPOCHS and epochs_no_improve >= PATIENCE:\n",
    "        print(f\"[EARLY STOP] No improvement for {epochs_no_improve} epochs (patience={PATIENCE}). Stopping.\")\n",
    "        break\n",
    "\n",
    "# Save history and metrics\n",
    "pd.DataFrame(history).to_csv(OUT_HISTORY, index=False)\n",
    "metrics_final, agg_final = evaluate_model(shared_labeled, heads_labeled, val_loader, DEVICE)\n",
    "with open(OUT_METRICS, \"w\") as fh:\n",
    "    json.dump({\"final_val_agg_f1\": agg_final, \"per_rank\": metrics_final}, fh, indent=2)\n",
    "print(f\"[SAVE] history -> {OUT_HISTORY}; metrics -> {OUT_METRICS}\")\n",
    "\n",
    "# Load best checkpoint and print final per-rank metrics\n",
    "if OUT_CHECKPOINT.exists():\n",
    "    ckpt = torch.load(OUT_CHECKPOINT, map_location=DEVICE)\n",
    "    shared_labeled.load_state_dict(ckpt[\"shared_state\"])\n",
    "    for r in RANKS:\n",
    "        heads_labeled[r].load_state_dict(ckpt[\"heads_state\"][r])\n",
    "    best_epoch = ckpt.get(\"epoch\")\n",
    "    best_score_saved = ckpt.get(\"val_agg_f1\")\n",
    "    print(f\"[LOAD] loaded best checkpoint epoch {best_epoch}, val_agg_f1={best_score_saved:.4f}\")\n",
    "\n",
    "final_metrics, final_agg = evaluate_model(shared_labeled, heads_labeled, val_loader, DEVICE)\n",
    "print(\"=== Final validation metrics on labeled split ===\")\n",
    "for r in RANKS:\n",
    "    m = final_metrics.get(r, {})\n",
    "    print(f\"{r:10s} | n_classes={m.get('n_classes', '?')} | acc={m.get('accuracy'):.4f} | f1_macro={m.get('f1_macro'):.4f} | mean_conf={m.get('mean_confidence'):.4f}\")\n",
    "print(f\"Aggregated mean-macro-F1: {final_agg:.4f}\")\n",
    "\n",
    "# Show top predicted labels distribution on validation\n",
    "print(\"\\n[VAL] Top predicted labels per rank (val set, top 5):\")\n",
    "shared_labeled.eval(); heads_labeled.eval()\n",
    "with torch.no_grad():\n",
    "    xval = torch.tensor(X[val_idx], dtype=torch.float32).to(DEVICE)\n",
    "    hval = shared_labeled(xval)\n",
    "    for r in RANKS:\n",
    "        logits = heads_labeled[r](hval)\n",
    "        probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "        preds = np.argmax(probs, axis=1)\n",
    "        labels = label_encoders[r].classes_\n",
    "        vc = Counter([labels[p] for p in preds])\n",
    "        print(f\"  {r:10s}: {vc.most_common(5)}\")\n",
    "\n",
    "print(\"\\nCell complete. Outputs produced:\")\n",
    "print(\" - model checkpoint:\", OUT_CHECKPOINT)\n",
    "print(\" - training history:\", OUT_HISTORY)\n",
    "print(\" - metrics json:\", OUT_METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "805e0a63-5307-446a-978a-f01fc9e0304d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD] label_encoders loaded.\n",
      "[LOAD] embeddings.npy shape=(2555, 128)\n",
      "[INFO] n_samples=2555\n",
      "[LOAD] checkpoint loaded (epoch 36, val_agg_f1=0.8890957155130087)\n",
      "[INFER] completed raw forward pass.\n",
      "[SAVE] raw predictions saved: ncbi_blast_db\\extracted\\predictions_raw.jsonl, ncbi_blast_db\\extracted\\predictions_raw.csv\n",
      "[CALIB] val_idx len=373 -> performing per-rank temperature scaling.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srijit\\anaconda3\\envs\\sih\\Lib\\site-packages\\torch\\optim\\lbfgs.py:457: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\autograd\\generated\\python_variable_methods.cpp:836.)\n",
      "  loss = float(closure())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CALIB] rank=kingdom T=2440.1846\n",
      "[CALIB] rank=phylum T=3093.2192\n",
      "[CALIB] rank=class T=639.1340\n",
      "[CALIB] rank=order T=71.4672\n",
      "[CALIB] rank=family T=63.5838\n",
      "[CALIB] rank=genus T=33.9495\n",
      "[CALIB] rank=species T=14.6255\n",
      "[SAVE] calibrated predictions saved: ncbi_blast_db\\extracted\\predictions_calibrated.jsonl, ncbi_blast_db\\extracted\\predictions_calibrated.csv\n",
      "[SAVE] novel candidates per-cluster saved: ncbi_blast_db\\extracted\\novel_candidates.csv\n",
      "\n",
      "=== Summary ===\n",
      "Samples predicted: 2555\n",
      "Per-rank average calibrated mean_confidence (sample mean):\n",
      "  kingdom : mean_prob=0.5029  # classes=2\n",
      "  phylum  : mean_prob=0.2008  # classes=5\n",
      "  class   : mean_prob=0.1022  # classes=10\n",
      "  order   : mean_prob=0.0936  # classes=13\n",
      "  family  : mean_prob=0.0656  # classes=19\n",
      "  genus   : mean_prob=0.0540  # classes=27\n",
      "  species : mean_prob=0.0144  # classes=182\n",
      "\n",
      "Top 10 priority novel clusters (cluster_label, n_sequences, mean_novelty, species_consensus_frac, mean_conf, priority_score):\n",
      "cluster_label  n_sequences  mean_novelty  species_consensus_frac  mean_conf  priority_score\n",
      "            5           10      0.667701                0.600000   0.153628        0.627577\n",
      "           10           15      0.574667                0.400000   0.144697        0.623096\n",
      "           -1          531      0.395312                0.129944   0.148043        0.582495\n",
      "            2            6      0.757933                1.000000   0.150385        0.582202\n",
      "           38           15      0.443201                0.266667   0.147913        0.577067\n",
      "            1            6      0.734036                1.000000   0.149066        0.568061\n",
      "            0            6      0.723481                1.000000   0.152414        0.561226\n",
      "           39            6      0.512499                0.500000   0.149853        0.560021\n",
      "           27           10      0.506437                0.500000   0.148189        0.556634\n",
      "            3            6      0.708417                1.000000   0.151133        0.552380\n",
      "\n",
      "Cell 16 complete. Files created:\n",
      " - ncbi_blast_db\\extracted\\predictions_raw.jsonl ncbi_blast_db\\extracted\\predictions_raw.csv\n",
      " - ncbi_blast_db\\extracted\\predictions_calibrated.jsonl ncbi_blast_db\\extracted\\predictions_calibrated.csv\n",
      " - ncbi_blast_db\\extracted\\novel_candidates.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 16 — Inference + per-rank temperature calibration (if validation indices available) + save predictions\n",
    "# Outputs:\n",
    "#  - predictions_raw.jsonl\n",
    "#  - predictions_raw.csv\n",
    "#  - predictions_calibrated.jsonl\n",
    "#  - predictions_calibrated.csv\n",
    "#  - novel_candidates.csv (priority list by novelty & low confidence)\n",
    "#\n",
    "# Defensive: checks files, aligns shapes, uses batched inference, uses LBFGS/Adam fallback for temperature scaling.\n",
    "\n",
    "import json, time, math\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# -------- Config / Paths --------\n",
    "DOWNLOAD_DIR = Path(globals().get(\"DOWNLOAD_DIR\", \"./ncbi_blast_db\"))\n",
    "EXTRACT_DIR  = DOWNLOAD_DIR / \"extracted\"\n",
    "\n",
    "CHECKPOINT = EXTRACT_DIR / \"best_shared_heads_labeled.pt\"\n",
    "ENC_PKL    = EXTRACT_DIR / \"label_encoders_rebuilt_v2.pkl\"\n",
    "META_CSV   = EXTRACT_DIR / \"embeddings_meta_clustered.csv\"\n",
    "EMB_FULL   = EXTRACT_DIR / \"embeddings.npy\"\n",
    "EMB_PCA    = EXTRACT_DIR / \"embeddings_pca.npy\"\n",
    "VAL_IDX    = EXTRACT_DIR / \"val_idx_by_acc.npy\"   # optional, used for calibration if present\n",
    "Y_PREFIX   = EXTRACT_DIR / \"y_encoded_rebuilt_\"\n",
    "\n",
    "OUT_RAW_JSONL  = EXTRACT_DIR / \"predictions_raw.jsonl\"\n",
    "OUT_RAW_CSV    = EXTRACT_DIR / \"predictions_raw.csv\"\n",
    "OUT_CAL_JSONL  = EXTRACT_DIR / \"predictions_calibrated.jsonl\"\n",
    "OUT_CAL_CSV    = EXTRACT_DIR / \"predictions_calibrated.csv\"\n",
    "OUT_NOVEL_CSV  = EXTRACT_DIR / \"novel_candidates.csv\"\n",
    "\n",
    "RANKS = [\"kingdom\",\"phylum\",\"class\",\"order\",\"family\",\"genus\",\"species\"]\n",
    "TOPK = 3\n",
    "BATCH = 256\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -------- Sanity checks --------\n",
    "for p in (CHECKPOINT, ENC_PKL, META_CSV):\n",
    "    if not Path(p).exists():\n",
    "        raise RuntimeError(f\"Required file missing: {p}\")\n",
    "\n",
    "# -------- Load encoders, meta, embeddings --------\n",
    "import pickle\n",
    "with open(ENC_PKL, \"rb\") as fh:\n",
    "    label_encoders = pickle.load(fh)\n",
    "print(\"[LOAD] label_encoders loaded.\")\n",
    "\n",
    "df_meta = pd.read_csv(META_CSV, dtype=str, keep_default_na=False, na_filter=False)\n",
    "if EMB_FULL.exists():\n",
    "    X = np.load(EMB_FULL)\n",
    "    print(f\"[LOAD] embeddings.npy shape={X.shape}\")\n",
    "elif EMB_PCA.exists():\n",
    "    X = np.load(EMB_PCA)\n",
    "    print(f\"[LOAD] embeddings_pca.npy shape={X.shape}\")\n",
    "else:\n",
    "    raise RuntimeError(\"No embeddings found (expect embeddings.npy or embeddings_pca.npy).\")\n",
    "\n",
    "n = min(len(df_meta), X.shape[0])\n",
    "if len(df_meta) != X.shape[0]:\n",
    "    print(f\"[ALIGN] trimming to min_n={n}\")\n",
    "    df_meta = df_meta.iloc[:n].reset_index(drop=True)\n",
    "    X = X[:n]\n",
    "\n",
    "print(f\"[INFO] n_samples={n}\")\n",
    "\n",
    "# -------- Reconstruct model architecture and load checkpoint --------\n",
    "input_dim = X.shape[1]\n",
    "HIDDEN_DIM = 256\n",
    "\n",
    "shared = torch.nn.Sequential(\n",
    "    torch.nn.Linear(input_dim, HIDDEN_DIM),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(0.3),\n",
    "    torch.nn.Linear(HIDDEN_DIM, HIDDEN_DIM//2),\n",
    "    torch.nn.ReLU()\n",
    ")\n",
    "heads = torch.nn.ModuleDict({ r: torch.nn.Linear(HIDDEN_DIM//2, len(label_encoders[r].classes_)) for r in RANKS })\n",
    "\n",
    "ckpt = torch.load(CHECKPOINT, map_location=\"cpu\")\n",
    "try:\n",
    "    shared.load_state_dict(ckpt[\"shared_state\"])\n",
    "    for r in RANKS:\n",
    "        heads[r].load_state_dict(ckpt[\"heads_state\"][r])\n",
    "    print(f\"[LOAD] checkpoint loaded (epoch {ckpt.get('epoch')}, val_agg_f1={ckpt.get('val_agg_f1')})\")\n",
    "except Exception:\n",
    "    # fallback: try partial load with strict=False\n",
    "    shared.load_state_dict(ckpt.get(\"shared_state\", {}), strict=False)\n",
    "    for r in RANKS:\n",
    "        if r in ckpt.get(\"heads_state\", {}):\n",
    "            heads[r].load_state_dict(ckpt[\"heads_state\"].get(r, {}), strict=False)\n",
    "    print(\"[WARN] partial checkpoint load (strict=False)\")\n",
    "\n",
    "shared.to(DEVICE).eval()\n",
    "heads.to(DEVICE).eval()\n",
    "\n",
    "# -------- Batched forward pass: collect logits and raw probs --------\n",
    "all_logits = {r: [] for r in RANKS}\n",
    "all_raw_probs = {r: [] for r in RANKS}\n",
    "all_topk_idx = {r: [] for r in RANKS}\n",
    "pred_indices = {r: [] for r in RANKS}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for start in range(0, n, BATCH):\n",
    "        end = min(n, start+BATCH)\n",
    "        xb = torch.tensor(X[start:end], dtype=torch.float32, device=DEVICE)\n",
    "        h = shared(xb)\n",
    "        for r in RANKS:\n",
    "            logits = heads[r](h)                # (batch, ncls)\n",
    "            probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "            logits_np = logits.cpu().numpy()\n",
    "            topk = np.argsort(-probs, axis=1)[:, :TOPK]\n",
    "            preds = np.argmax(probs, axis=1)\n",
    "            for i in range(end-start):\n",
    "                all_logits[r].append(logits_np[i])\n",
    "                all_raw_probs[r].append(probs[i])\n",
    "                all_topk_idx[r].append(list(map(int, topk[i])))\n",
    "                pred_indices[r].append(int(preds[i]))\n",
    "\n",
    "print(\"[INFER] completed raw forward pass.\")\n",
    "\n",
    "# -------- Build raw prediction records & compute mean_confidence (raw) --------\n",
    "records_raw = []\n",
    "for i in range(n):\n",
    "    rec = {\"index\": int(i), \"id\": df_meta.loc[i,\"id\"] if \"id\" in df_meta.columns else f\"seq_{i}\"}\n",
    "    probs_top = []\n",
    "    rec_pred = {}\n",
    "    for r in RANKS:\n",
    "        probs = all_raw_probs[r][i]\n",
    "        pred_idx = pred_indices[r][i]\n",
    "        pred_label = str(label_encoders[r].classes_[pred_idx])\n",
    "        pred_prob = float(probs[pred_idx])\n",
    "        topk = [{\"label\": str(label_encoders[r].classes_[k]), \"prob\": float(probs[k])} for k in all_topk_idx[r][i]]\n",
    "        rec_pred[r] = {\"predicted_label\": pred_label, \"predicted_index\": pred_idx, \"predicted_prob\": pred_prob, \"top_k\": topk}\n",
    "        probs_top.append(pred_prob)\n",
    "    rec[\"predicted\"] = rec_pred\n",
    "    rec[\"mean_conf_raw\"] = float(np.mean(probs_top))\n",
    "    # add cluster and novelty metadata if present\n",
    "    rec[\"cluster_label\"] = df_meta.loc[i,\"cluster_label\"] if \"cluster_label\" in df_meta.columns else \"-1\"\n",
    "    rec[\"novelty_score\"] = float(df_meta.loc[i,\"novelty_score\"]) if (\"novelty_score\" in df_meta.columns and df_meta.loc[i,\"novelty_score\"]!=\"\") else None\n",
    "    records_raw.append(rec)\n",
    "\n",
    "# Save raw outputs\n",
    "with open(OUT_RAW_JSONL, \"w\", encoding=\"utf-8\") as fh:\n",
    "    for r in records_raw:\n",
    "        fh.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "# Flatten CSV\n",
    "rows_csv = []\n",
    "for r in records_raw:\n",
    "    row = {\"index\": r[\"index\"], \"id\": r[\"id\"], \"cluster_label\": r[\"cluster_label\"], \"novelty_score\": r[\"novelty_score\"], \"mean_conf_raw\": r[\"mean_conf_raw\"]}\n",
    "    for rank in RANKS:\n",
    "        row[f\"{rank}_pred\"] = r[\"predicted\"][rank][\"predicted_label\"]\n",
    "        row[f\"{rank}_prob\"] = r[\"predicted\"][rank][\"predicted_prob\"]\n",
    "    rows_csv.append(row)\n",
    "pd.DataFrame(rows_csv).to_csv(OUT_RAW_CSV, index=False)\n",
    "print(f\"[SAVE] raw predictions saved: {OUT_RAW_JSONL}, {OUT_RAW_CSV}\")\n",
    "\n",
    "# -------- Temperature scaling calibration (per-rank) if validation indices available --------\n",
    "temps = {r: 1.0 for r in RANKS}\n",
    "if VAL_IDX.exists():\n",
    "    try:\n",
    "        val_idx = np.load(VAL_IDX)\n",
    "        # build logits and labels on validation set\n",
    "        print(f\"[CALIB] val_idx len={len(val_idx)} -> performing per-rank temperature scaling.\")\n",
    "        for r in RANKS:\n",
    "            # load y_encoded_rebuilt for this rank if available\n",
    "            y_path = Path(f\"{Y_PREFIX}{r}.npy\")\n",
    "            if not y_path.exists():\n",
    "                print(f\"[CALIB] y file missing for rank {r}, skipping calibration.\")\n",
    "                continue\n",
    "            y_full = np.load(y_path)\n",
    "            y_val = y_full[val_idx]\n",
    "            # gather logits numpy for val indices\n",
    "            logits_val = np.vstack([all_logits[r][i] for i in val_idx])\n",
    "            # optimize temperature scalar using simple LBFGS on cross-entropy\n",
    "            logits_t = torch.tensor(logits_val, dtype=torch.float32, device=DEVICE)\n",
    "            labels_t = torch.tensor(y_val, dtype=torch.long, device=DEVICE)\n",
    "            T = torch.nn.Parameter(torch.ones(1, device=DEVICE))\n",
    "            optimizerT = torch.optim.LBFGS([T], lr=0.5, max_iter=50)\n",
    "\n",
    "            def closure():\n",
    "                optimizerT.zero_grad()\n",
    "                t = torch.clamp(T, min=1e-3)\n",
    "                loss = F.cross_entropy(logits_t / t, labels_t)\n",
    "                loss.backward()\n",
    "                return loss\n",
    "            try:\n",
    "                optimizerT.step(closure)\n",
    "                T_opt = float(max(1e-3, T.item()))\n",
    "            except Exception:\n",
    "                # fallback: small Adam loop\n",
    "                T = torch.nn.Parameter(torch.tensor(1.0, device=DEVICE))\n",
    "                opt = torch.optim.Adam([T], lr=0.01)\n",
    "                for _ in range(200):\n",
    "                    opt.zero_grad()\n",
    "                    t = torch.clamp(T, min=1e-3)\n",
    "                    loss = F.cross_entropy(torch.tensor(logits_val, device=DEVICE) / t, labels_t)\n",
    "                    loss.backward(); opt.step()\n",
    "                T_opt = float(max(1e-3, T.item()))\n",
    "            temps[r] = T_opt\n",
    "            print(f\"[CALIB] rank={r} T={temps[r]:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(\"[CALIB WARN] calibration failed: \", e)\n",
    "        temps = {r: 1.0 for r in RANKS}\n",
    "else:\n",
    "    print(\"[CALIB] val index file not found — skipping calibration (using T=1.0).\")\n",
    "\n",
    "# -------- Apply temperatures and save calibrated predictions --------\n",
    "records_cal = []\n",
    "rows_cal = []\n",
    "with torch.no_grad():\n",
    "    for i in range(n):\n",
    "        rec = {\"index\": int(i), \"id\": df_meta.loc[i,\"id\"] if \"id\" in df_meta.columns else f\"seq_{i}\"}\n",
    "        rec_pred = {}\n",
    "        probs_top = []\n",
    "        for r in RANKS:\n",
    "            logits = np.array(all_logits[r][i])\n",
    "            T = temps.get(r, 1.0)\n",
    "            # compute scaled probs\n",
    "            scaled = F.softmax(torch.tensor(logits / T, dtype=torch.float32), dim=0).cpu().numpy()\n",
    "            pred_idx = int(np.argmax(scaled))\n",
    "            pred_label = str(label_encoders[r].classes_[pred_idx])\n",
    "            pred_prob = float(scaled[pred_idx])\n",
    "            topk_idx = np.argsort(-scaled)[:TOPK]\n",
    "            topk = [{\"label\": str(label_encoders[r].classes_[k]), \"prob\": float(scaled[k])} for k in topk_idx]\n",
    "            rec_pred[r] = {\"predicted_label\": pred_label, \"predicted_index\": pred_idx, \"predicted_prob\": pred_prob, \"top_k\": topk}\n",
    "            probs_top.append(pred_prob)\n",
    "        rec[\"predicted\"] = rec_pred\n",
    "        rec[\"mean_conf_calibrated\"] = float(np.mean(probs_top))\n",
    "        rec[\"cluster_label\"] = df_meta.loc[i,\"cluster_label\"] if \"cluster_label\" in df_meta.columns else \"-1\"\n",
    "        rec[\"novelty_score\"] = float(df_meta.loc[i,\"novelty_score\"]) if (\"novelty_score\" in df_meta.columns and df_meta.loc[i,\"novelty_score\"]!=\"\") else None\n",
    "        records_cal.append(rec)\n",
    "        row = {\"index\": rec[\"index\"], \"id\": rec[\"id\"], \"cluster_label\": rec[\"cluster_label\"], \"novelty_score\": rec[\"novelty_score\"], \"mean_conf_calibrated\": rec[\"mean_conf_calibrated\"]}\n",
    "        for r in RANKS:\n",
    "            row[f\"{r}_pred\"] = rec[\"predicted\"][r][\"predicted_label\"]\n",
    "            row[f\"{r}_prob\"] = rec[\"predicted\"][r][\"predicted_prob\"]\n",
    "        rows_cal.append(row)\n",
    "\n",
    "with open(OUT_CAL_JSONL, \"w\", encoding=\"utf-8\") as fh:\n",
    "    for r in records_cal:\n",
    "        fh.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "pd.DataFrame(rows_cal).to_csv(OUT_CAL_CSV, index=False)\n",
    "print(f\"[SAVE] calibrated predictions saved: {OUT_CAL_JSONL}, {OUT_CAL_CSV}\")\n",
    "\n",
    "# -------- Novel candidate prioritization (cluster-level) --------\n",
    "dfc = pd.DataFrame(rows_cal)\n",
    "# ensure numeric novelty_score\n",
    "dfc[\"novelty_score\"] = pd.to_numeric(dfc[\"novelty_score\"].replace(\"\", np.nan), errors=\"coerce\")\n",
    "# cluster summary: n, mean_novelty, mean_confidence, species_consensus_frac\n",
    "cluster = dfc.groupby(\"cluster_label\").agg(\n",
    "    n_sequences=(\"id\",\"count\"),\n",
    "    mean_novelty=(\"novelty_score\", \"mean\"),\n",
    "    mean_confidence=(\"mean_conf_calibrated\",\"mean\")\n",
    ").reset_index()\n",
    "# compute species consensus fraction per cluster\n",
    "def consensus_frac(group, rank):\n",
    "    return group[rank + \"_pred\"].value_counts(normalize=True).max()\n",
    "\n",
    "consensus_rows = []\n",
    "for cl, g in dfc.groupby(\"cluster_label\"):\n",
    "    row = {\"cluster_label\": cl, \"n_sequences\": len(g)}\n",
    "    for r in [\"species\",\"genus\",\"family\"]:\n",
    "        row[f\"{r}_consensus_frac\"] = consensus_frac(g, r)\n",
    "    row[\"mean_novelty\"] = float(g[\"novelty_score\"].mean()) if g[\"novelty_score\"].notna().any() else 0.0\n",
    "    row[\"mean_conf\"] = float(g[\"mean_conf_calibrated\"].mean())\n",
    "    consensus_rows.append(row)\n",
    "df_cluster = pd.DataFrame(consensus_rows)\n",
    "# priority score heuristic\n",
    "df_cluster[\"priority_score\"] = (df_cluster[\"mean_novelty\"].fillna(0.0) * 0.6) + ((1.0 - df_cluster[\"species_consensus_frac\"].fillna(0.0)) * 0.25) + ((1.0 - df_cluster[\"mean_conf\"].fillna(0.0)) * 0.15)\n",
    "df_cluster = df_cluster.sort_values(\"priority_score\", ascending=False)\n",
    "df_cluster.to_csv(OUT_NOVEL_CSV, index=False)\n",
    "print(f\"[SAVE] novel candidates per-cluster saved: {OUT_NOVEL_CSV}\")\n",
    "\n",
    "# -------- Print summary diagnostics --------\n",
    "print(\"\\n=== Summary ===\")\n",
    "print(f\"Samples predicted: {n}\")\n",
    "print(\"Per-rank average calibrated mean_confidence (sample mean):\")\n",
    "for r in RANKS:\n",
    "    vals = [rec[\"predicted\"][r][\"predicted_prob\"] for rec in records_cal]\n",
    "    print(f\"  {r:8s}: mean_prob={float(np.mean(vals)):.4f}  # classes={len(label_encoders[r].classes_)}\")\n",
    "\n",
    "print(\"\\nTop 10 priority novel clusters (cluster_label, n_sequences, mean_novelty, species_consensus_frac, mean_conf, priority_score):\")\n",
    "print(df_cluster.head(10).loc[:, [\"cluster_label\",\"n_sequences\",\"mean_novelty\",\"species_consensus_frac\",\"mean_conf\",\"priority_score\"]].to_string(index=False))\n",
    "\n",
    "print(\"\\nCell 16 complete. Files created:\")\n",
    "print(\" -\", OUT_RAW_JSONL, OUT_RAW_CSV)\n",
    "print(\" -\", OUT_CAL_JSONL, OUT_CAL_CSV)\n",
    "print(\" -\", OUT_NOVEL_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d82279a9-43f0-4d6b-83df-a5dd7e700aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] DOWNLOAD_DIR: ncbi_blast_db\n",
      "[INFO] EXTRACT_DIR: ncbi_blast_db\\extracted\n",
      "\n",
      "[FILES] top-level files in extracted/:\n",
      "  best_shared_heads.pt\n",
      "  best_shared_heads_defensive.pt\n",
      "  best_shared_heads_defensive_state_dict.pt\n",
      "  best_shared_heads_labeled.pt\n",
      "  best_shared_heads_pseudo_tensordataset_fix.pt\n",
      "  best_shared_heads_resumed.pt\n",
      "  best_shared_heads_resumed_state_dict.pt\n",
      "  best_shared_heads_retrain.pt\n",
      "  calibration_bins_class.csv\n",
      "  calibration_bins_family.csv\n",
      "  calibration_bins_genus.csv\n",
      "  calibration_bins_kingdom.csv\n",
      "  calibration_bins_order.csv\n",
      "  calibration_bins_phylum.csv\n",
      "  calibration_bins_species.csv\n",
      "  calibration_metrics_by_rank.csv\n",
      "  cluster_summary.csv\n",
      "  cluster_summary.json\n",
      "  confusion_matrix_class.csv\n",
      "  confusion_matrix_family.csv\n",
      "  confusion_matrix_genus.csv\n",
      "  confusion_matrix_kingdom.csv\n",
      "  confusion_matrix_order.csv\n",
      "  confusion_matrix_phylum.csv\n",
      "  confusion_matrix_species.csv\n",
      "  embeddings.npy\n",
      "  embeddings_meta.csv\n",
      "  embeddings_meta_clustered.csv\n",
      "  embeddings_meta_clustered.json\n",
      "  embeddings_meta_pca.csv\n",
      "  embeddings_pca.npy\n",
      "  evaluation_summary_by_rank.csv\n",
      "  fetched_summary.json\n",
      "  its_combined.fasta\n",
      "  its_fetched.fasta\n",
      "  its_fetched_metadata.json\n",
      "  label_assignment_debug.csv\n",
      "  label_assignment_debug_final.csv\n",
      "  label_encoders_final.pkl\n",
      "  label_encoders_rebuilt.pkl\n",
      "  label_encoders_rebuilt_v2.pkl\n",
      "  label_encoders_used.pkl\n",
      "  lsu_combined.fasta\n",
      "  lsu_fetched.fasta\n",
      "  lsu_fetched_metadata.json\n",
      "  metrics_defensive.json\n",
      "  metrics_labeled.json\n",
      "  metrics_resumed.json\n",
      "  novel_candidates.csv\n",
      "  novel_candidates_priority.csv\n",
      "  per_class_metrics_class.csv\n",
      "  per_class_metrics_family.csv\n",
      "  per_class_metrics_genus.csv\n",
      "  per_class_metrics_kingdom.csv\n",
      "  per_class_metrics_order.csv\n",
      "  per_class_metrics_phylum.csv\n",
      "  per_class_metrics_species.csv\n",
      "  predictions.jsonl\n",
      "  predictions.jsonl.bak\n",
      "  predictions_calibrated.csv\n",
      "  predictions_calibrated.jsonl\n",
      "  predictions_raw.csv\n",
      "  predictions_raw.jsonl\n",
      "  predictions_summary.csv\n",
      "  predictions_summary.csv.bak\n",
      "  predictions_summary_calibrated.csv\n",
      "  predictions_with_mc_uncertainty.csv\n",
      "  predictions_with_mc_uncertainty.jsonl\n",
      "  predictions_with_uncertainty.csv\n",
      "  reliability_class.png\n",
      "  reliability_family.png\n",
      "  reliability_genus.png\n",
      "  reliability_kingdom.png\n",
      "  reliability_order.png\n",
      "  reliability_phylum.png\n",
      "  reliability_species.png\n",
      "  shared_heads_initial.pt\n",
      "  species_topk_accuracy.csv\n",
      "  ssu_combined.fasta\n",
      "  ssu_fetched.fasta\n",
      "  ssu_fetched_metadata.json\n",
      "  temp_scaling_by_rank.json\n",
      "  train_idx_by_acc.npy\n",
      "  train_idx_final.npy\n",
      "  train_val_split_by_accession.json\n",
      "  training_history.csv\n",
      "  training_history.json\n",
      "  training_history_defensive.csv\n",
      "  training_history_labeled.csv\n",
      "  training_history_pseudo_tensordataset_fix.csv\n",
      "  training_history_resumed.csv\n",
      "  training_history_retrain.csv\n",
      "  val_idx_by_acc.npy\n",
      "  val_idx_final.npy\n",
      "  val_predictions_calibrated.csv\n",
      "  y_encoded_final_class.npy\n",
      "  y_encoded_final_family.npy\n",
      "  y_encoded_final_genus.npy\n",
      "  y_encoded_final_kingdom.npy\n",
      "  y_encoded_final_order.npy\n",
      "  y_encoded_final_phylum.npy\n",
      "  y_encoded_final_species.npy\n",
      "  y_encoded_rebuilt_class.npy\n",
      "  y_encoded_rebuilt_family.npy\n",
      "  y_encoded_rebuilt_genus.npy\n",
      "  y_encoded_rebuilt_kingdom.npy\n",
      "  y_encoded_rebuilt_order.npy\n",
      "  y_encoded_rebuilt_phylum.npy\n",
      "  y_encoded_rebuilt_species.npy\n",
      "[LOAD] Using features file: ncbi_blast_db\\extracted\\embeddings.npy\n",
      "[SHAPE] features shape: (2555, 128)\n",
      "[LOAD] Loaded y_encoded_final_kingdom.npy -> shape (2555,)\n",
      "[LOAD] Loaded y_encoded_final_phylum.npy -> shape (2555,)\n",
      "[LOAD] Loaded y_encoded_final_class.npy -> shape (2555,)\n",
      "[LOAD] Loaded y_encoded_final_order.npy -> shape (2555,)\n",
      "[LOAD] Loaded y_encoded_final_family.npy -> shape (2555,)\n",
      "[LOAD] Loaded y_encoded_final_genus.npy -> shape (2555,)\n",
      "[LOAD] Loaded y_encoded_final_species.npy -> shape (2555,)\n",
      "[CHECK] All loaded labels match features length: 2555\n",
      "Loaded ranks: ['kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species']\n",
      "[LOAD] train indices loaded from train_idx_final.npy -> 2175 indices\n",
      "[LOAD] val indices loaded from val_idx_final.npy -> 380 indices\n",
      "[SAVED] train dataset -> ncbi_blast_db\\extracted\\train_dataset_reconstructed.pt (n=2175)\n",
      "[SAVED] val   dataset -> ncbi_blast_db\\extracted\\val_dataset_reconstructed.pt (n=380)\n",
      "[LOADED] label_encoders from label_encoders_final.pkl\n",
      "\n",
      "[SANITY] train sample tensors shapes:\n",
      "  len(sample) = 9\n",
      "  shapes: [torch.Size([128]), torch.Size([]), torch.Size([]), torch.Size([]), torch.Size([]), torch.Size([]), torch.Size([]), torch.Size([]), torch.Size([])]\n",
      "\n",
      "Done — you can now re-run the training cell. If anything errors, copy the printed warnings here and I'll iterate a fix.\n"
     ]
    }
   ],
   "source": [
    "# Reconstruct train_dataset / val_dataset from available artifacts\n",
    "import numpy as np, torch, pickle, os\n",
    "from pathlib import Path\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "DOWNLOAD_DIR = Path(globals().get(\"DOWNLOAD_DIR\", \"./ncbi_blast_db\"))\n",
    "EXTRACT_DIR = DOWNLOAD_DIR / \"extracted\"\n",
    "print(\"[INFO] DOWNLOAD_DIR:\", DOWNLOAD_DIR)\n",
    "print(\"[INFO] EXTRACT_DIR:\", EXTRACT_DIR)\n",
    "\n",
    "if not EXTRACT_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Expected extracted dir at {EXTRACT_DIR} but it does not exist.\")\n",
    "\n",
    "# list extracted files for debugging\n",
    "print(\"\\n[FILES] top-level files in extracted/:\")\n",
    "for p in sorted(EXTRACT_DIR.iterdir()):\n",
    "    print(\" \", p.name)\n",
    "\n",
    "# 1) Choose embeddings/features file\n",
    "candidates_feats = [\n",
    "    EXTRACT_DIR / \"embeddings.npy\",\n",
    "    EXTRACT_DIR / \"embeddings_pca.npy\",\n",
    "    EXTRACT_DIR / \"embeddings_pca.npy\",\n",
    "    EXTRACT_DIR / \"embeddings.npy\"\n",
    "]\n",
    "feat_path = None\n",
    "for f in candidates_feats:\n",
    "    if f.exists():\n",
    "        feat_path = f\n",
    "        break\n",
    "\n",
    "if feat_path is None:\n",
    "    # fallback: pick any .npy with 'embed' in name\n",
    "    for f in EXTRACT_DIR.rglob(\"*.npy\"):\n",
    "        if \"embed\" in f.name.lower():\n",
    "            feat_path = f\n",
    "            break\n",
    "\n",
    "if feat_path is None:\n",
    "    raise FileNotFoundError(\"No embeddings file found (tried embeddings.npy and embeddings_pca.npy).\")\n",
    "\n",
    "print(f\"[LOAD] Using features file: {feat_path}\")\n",
    "X = np.load(feat_path, allow_pickle=True)\n",
    "# If npz-like, try first key\n",
    "if hasattr(X, \"files\"):\n",
    "    key = X.files[0]\n",
    "    print(\"[LOAD] feature file is .npz; using key:\", key)\n",
    "    X = X[key]\n",
    "X = np.asarray(X)\n",
    "print(\"[SHAPE] features shape:\", X.shape)\n",
    "\n",
    "# 2) Load label arrays for expected ranks\n",
    "ranks = [\"kingdom\",\"phylum\",\"class\",\"order\",\"family\",\"genus\",\"species\"]\n",
    "label_arrays = {}\n",
    "for r in ranks:\n",
    "    # try multiple filename patterns, prefer 'final' then 'rebuilt' then any matching\n",
    "    tried = []\n",
    "    candidates = [\n",
    "        EXTRACT_DIR / f\"y_encoded_final_{r}.npy\",\n",
    "        EXTRACT_DIR / f\"y_encoded_rebuilt_{r}.npy\",\n",
    "        EXTRACT_DIR / f\"y_encoded_{r}.npy\",\n",
    "        EXTRACT_DIR / f\"y_encoded_final_{r}.npz\",\n",
    "    ]\n",
    "    arr = None\n",
    "    for c in candidates:\n",
    "        tried.append(c.name)\n",
    "        if c.exists():\n",
    "            arr = np.load(c, allow_pickle=True)\n",
    "            if hasattr(arr, \"files\"):\n",
    "                arr = arr[arr.files[0]]\n",
    "            arr = np.asarray(arr)\n",
    "            print(f\"[LOAD] Loaded {c.name} -> shape {arr.shape}\")\n",
    "            break\n",
    "    if arr is None:\n",
    "        # last resort: find any file matching pattern *y_encoded*{rank}*.npy\n",
    "        for f in EXTRACT_DIR.rglob(f\"*y*encoded*{r}*.npy\"):\n",
    "            arr = np.load(f, allow_pickle=True)\n",
    "            if hasattr(arr, \"files\"):\n",
    "                arr = arr[arr.files[0]]\n",
    "            arr = np.asarray(arr)\n",
    "            print(f\"[LOAD] Fallback loaded {f.name} -> shape {arr.shape}\")\n",
    "            break\n",
    "    if arr is None:\n",
    "        print(f\"[WARN] Could not find explicit y_encoded file for rank '{r}'. Will not include this rank.\")\n",
    "    else:\n",
    "        label_arrays[r] = arr\n",
    "\n",
    "if not label_arrays:\n",
    "    raise RuntimeError(\"No label arrays found. Expected files like y_encoded_final_species.npy etc.\")\n",
    "\n",
    "# Check that the label arrays all have same length as X\n",
    "n = X.shape[0]\n",
    "for r, arr in label_arrays.items():\n",
    "    if arr.shape[0] != n:\n",
    "        raise RuntimeError(f\"Length mismatch: features have {n} rows but label '{r}' has {arr.shape[0]} rows. \"\n",
    "                           \"This suggests ordering mismatch between features and labels.\")\n",
    "\n",
    "print(\"[CHECK] All loaded labels match features length:\", n)\n",
    "print(\"Loaded ranks:\", list(label_arrays.keys()))\n",
    "\n",
    "# 3) Load train / val indices (preferred: train_idx_final.npy / val_idx_final.npy)\n",
    "idx_candidates = {\n",
    "    \"train\": [EXTRACT_DIR / \"train_idx_final.npy\", EXTRACT_DIR / \"train_idx_by_acc.npy\", EXTRACT_DIR / \"train_idx.npy\", EXTRACT_DIR / \"train_idx_final.npy\"],\n",
    "    \"val\":   [EXTRACT_DIR / \"val_idx_final.npy\", EXTRACT_DIR / \"val_idx_by_acc.npy\", EXTRACT_DIR / \"val_idx.npy\", EXTRACT_DIR / \"val_idx_final.npy\"]\n",
    "}\n",
    "indices = {}\n",
    "for split, cand_list in idx_candidates.items():\n",
    "    idx = None\n",
    "    for c in cand_list:\n",
    "        if c.exists():\n",
    "            idx = np.load(c, allow_pickle=True)\n",
    "            if hasattr(idx, \"files\"):\n",
    "                # unlikely, but handle .npz\n",
    "                idx = idx[idx.files[0]]\n",
    "            idx = np.asarray(idx).astype(int)\n",
    "            print(f\"[LOAD] {split} indices loaded from {c.name} -> {idx.shape[0]} indices\")\n",
    "            break\n",
    "    if idx is None:\n",
    "        # fallback: try patterns\n",
    "        for f in EXTRACT_DIR.rglob(f\"*{split}*idx*.npy\"):\n",
    "            idx = np.load(f, allow_pickle=True)\n",
    "            if hasattr(idx, \"files\"):\n",
    "                idx = idx[idx.files[0]]\n",
    "            idx = np.asarray(idx).astype(int)\n",
    "            print(f\"[LOAD] fallback {split} indices loaded from {f.name} -> {idx.shape[0]} indices\")\n",
    "            break\n",
    "    if idx is None:\n",
    "        print(f\"[WARN] Could not find {split} indices. Will build splits using 90/10 default split.\")\n",
    "    indices[split] = idx\n",
    "\n",
    "# If either train/val indices missing, create a deterministic split (seeded)\n",
    "if indices.get(\"train\") is None or indices.get(\"val\") is None:\n",
    "    all_idx = np.arange(n)\n",
    "    rng = np.random.RandomState(12345)\n",
    "    rng.shuffle(all_idx)\n",
    "    # 90% train, 10% val\n",
    "    cut = int(n * 0.9)\n",
    "    indices[\"train\"] = all_idx[:cut]\n",
    "    indices[\"val\"] = all_idx[cut:]\n",
    "    print(f\"[SPLIT] Created deterministic 90/10 split: train {indices['train'].shape[0]} / val {indices['val'].shape[0]}\")\n",
    "\n",
    "# 4) Build TensorDatasets\n",
    "def build_td(idx_array, X, label_arrays):\n",
    "    X_sub = torch.tensor(X[np.asarray(idx_array)], dtype=torch.float32)\n",
    "    tensors = [X_sub]\n",
    "    # maintain order of ranks requested (only include ranks we found)\n",
    "    for r in ranks:\n",
    "        if r in label_arrays:\n",
    "            arr = np.asarray(label_arrays[r])[np.asarray(idx_array)]\n",
    "            tensors.append(torch.tensor(arr.astype(\"int64\")))\n",
    "    # append weight (ones)\n",
    "    weights = torch.ones(X_sub.size(0), dtype=torch.float32)\n",
    "    tensors.append(weights)\n",
    "    td = TensorDataset(*tensors)\n",
    "    return td\n",
    "\n",
    "train_td = build_td(indices[\"train\"], X, label_arrays)\n",
    "val_td = build_td(indices[\"val\"], X, label_arrays)\n",
    "\n",
    "# Save reconstructed datasets\n",
    "train_save = EXTRACT_DIR / \"train_dataset_reconstructed.pt\"\n",
    "val_save = EXTRACT_DIR / \"val_dataset_reconstructed.pt\"\n",
    "torch.save(train_td, train_save)\n",
    "torch.save(val_td, val_save)\n",
    "print(f\"[SAVED] train dataset -> {train_save} (n={len(train_td)})\")\n",
    "print(f\"[SAVED] val   dataset -> {val_save} (n={len(val_td)})\")\n",
    "\n",
    "# Place into globals() for notebook use (mimics original expectation)\n",
    "globals()[\"train_dataset\"] = train_td\n",
    "globals()[\"val_dataset\"] = val_td\n",
    "\n",
    "# also try to load label_encoders if present (optional)\n",
    "le_path = EXTRACT_DIR / \"label_encoders_final.pkl\"\n",
    "if le_path.exists():\n",
    "    try:\n",
    "        with open(le_path, \"rb\") as fh:\n",
    "            gl = pickle.load(fh)\n",
    "        globals()[\"label_encoders\"] = gl\n",
    "        print(\"[LOADED] label_encoders from\", le_path.name)\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] Could not load label_encoders:\", e)\n",
    "\n",
    "# Sanity print\n",
    "print(\"\\n[SANITY] train sample tensors shapes:\")\n",
    "try:\n",
    "    s = train_td[0]\n",
    "    print(\"  len(sample) =\", len(s))\n",
    "    print(\"  shapes:\", [getattr(t, \"shape\", None) for t in s])\n",
    "except Exception as e:\n",
    "    print(\"  Could not inspect sample:\", e)\n",
    "\n",
    "print(\"\\nDone — you can now re-run the training cell. If anything errors, copy the printed warnings here and I'll iterate a fix.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2e024f75-a680-4a13-873c-e4736cebc2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[USE] checkpoint: best_shared_heads_pseudo_tensordataset_fix.pt\n",
      "[LOAD] label_encoders from label_encoders_rebuilt_v2.pkl\n",
      "[LOAD] embeddings.npy shape: (2555, 128)\n",
      "[LOAD] val_idx from val_idx_by_acc.npy len= 373\n",
      "[INFO] using val_idx length: 373\n",
      "[LOAD] loaded y_encoded_rebuilt_* arrays for true labels.\n",
      "[WARN] checkpoint load partial/failed: Error(s) in loading state_dict for Module:\n",
      "\tsize mismatch for heads.phylum.weight: copying a param with shape torch.Size([6, 128]) from checkpoint, the shape in current model is torch.Size([5, 128]).\n",
      "\tsize mismatch for heads.phylum.bias: copying a param with shape torch.Size([6]) from checkpoint, the shape in current model is torch.Size([5]).\n",
      "\tsize mismatch for heads.class.weight: copying a param with shape torch.Size([13, 128]) from checkpoint, the shape in current model is torch.Size([10, 128]).\n",
      "\tsize mismatch for heads.class.bias: copying a param with shape torch.Size([13]) from checkpoint, the shape in current model is torch.Size([10]).\n",
      "\tsize mismatch for heads.order.weight: copying a param with shape torch.Size([16, 128]) from checkpoint, the shape in current model is torch.Size([13, 128]).\n",
      "\tsize mismatch for heads.order.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([13]).\n",
      "\tsize mismatch for heads.family.weight: copying a param with shape torch.Size([23, 128]) from checkpoint, the shape in current model is torch.Size([19, 128]).\n",
      "\tsize mismatch for heads.family.bias: copying a param with shape torch.Size([23]) from checkpoint, the shape in current model is torch.Size([19]).\n",
      "\tsize mismatch for heads.genus.weight: copying a param with shape torch.Size([33, 128]) from checkpoint, the shape in current model is torch.Size([27, 128]).\n",
      "\tsize mismatch for heads.genus.bias: copying a param with shape torch.Size([33]) from checkpoint, the shape in current model is torch.Size([27]).\n",
      "\tsize mismatch for heads.species.weight: copying a param with shape torch.Size([387, 128]) from checkpoint, the shape in current model is torch.Size([182, 128]).\n",
      "\tsize mismatch for heads.species.bias: copying a param with shape torch.Size([387]) from checkpoint, the shape in current model is torch.Size([182]).\n",
      "[MODEL] checkpoint loaded -> device: cpu loaded_ok: False\n",
      "[METRIC] kingdom  | acc=0.4638 | macro_f1=0.3168 | mean_conf=0.9951 | saved -> per_class_metrics_kingdom.csv\n",
      "[METRIC] phylum   | acc=0.0456 | macro_f1=0.0308 | mean_conf=0.4994 | saved -> per_class_metrics_phylum.csv\n",
      "[METRIC] class    | acc=0.0000 | macro_f1=0.0000 | mean_conf=0.2801 | saved -> per_class_metrics_class.csv\n",
      "[METRIC] order    | acc=0.1501 | macro_f1=0.0350 | mean_conf=0.2736 | saved -> per_class_metrics_order.csv\n",
      "[METRIC] family   | acc=0.0858 | macro_f1=0.0144 | mean_conf=0.2230 | saved -> per_class_metrics_family.csv\n",
      "[METRIC] genus    | acc=0.0000 | macro_f1=0.0000 | mean_conf=0.2690 | saved -> per_class_metrics_genus.csv\n",
      "[METRIC] species  | acc=0.0000 | macro_f1=0.0000 | mean_conf=0.0708 | saved -> per_class_metrics_species.csv\n",
      "[TOP-K] species top-1 acc: 0.0000 | top-5 acc: 0.0000\n",
      "\n",
      "[ALERT] Classes with low F1 (support>=5) — inspect CSVs for details:\n",
      " - kingdom: 2 low-performing classes (examples):\n",
      "     _UNASSIGNED_                                                 support=200, f1=0.000\n",
      "     Eukaryota                                                    support=173, f1=0.634\n",
      " - phylum: 4 low-performing classes (examples):\n",
      "     Fungi                                                        support=59, f1=0.000\n",
      "     Metazoa                                                      support=103, f1=0.000\n",
      "     Viridiplantae                                                support=10, f1=0.000\n",
      " - class: 4 low-performing classes (examples):\n",
      "     Chlorophyta                                                  support=7, f1=0.000\n",
      "     Chordata                                                     support=102, f1=0.000\n",
      "     Dikarya                                                      support=59, f1=0.000\n",
      " - order: 5 low-performing classes (examples):\n",
      "     Ascomycota                                                   support=20, f1=0.000\n",
      "     Basidiomycota                                                support=39, f1=0.000\n",
      "     _UNASSIGNED_                                                 support=200, f1=0.000\n",
      " - family: 5 low-performing classes (examples):\n",
      "     Agaricomycotina                                              support=39, f1=0.000\n",
      "     Pezizomycotina                                               support=20, f1=0.000\n",
      "     Trebouxiophyceae                                             support=5, f1=0.000\n",
      " - genus: 4 low-performing classes (examples):\n",
      "     Agaricomycetes                                               support=39, f1=0.000\n",
      "     Euteleostomi                                                 support=102, f1=0.000\n",
      "     Pezizomycetes                                                support=12, f1=0.000\n",
      " - species: 5 low-performing classes (examples):\n",
      "     Arvicanthis niloticus                                        support=6, f1=0.000\n",
      "     Chaetodon auriga                                             support=16, f1=0.000\n",
      "     Maylandia zebra                                              support=78, f1=0.000\n",
      "\n",
      "[COMPLETE] Evaluation outputs saved to: ncbi_blast_db\\extracted\n",
      "Key files: evaluation_summary_by_rank.csv, per_class_metrics_<rank>.csv, confusion_matrix_<rank>.csv, species_topk_accuracy.csv (if species present).\n"
     ]
    }
   ],
   "source": [
    "# Cell A — Final evaluation & per-class diagnostics (run after training)\n",
    "# Produces per-rank CSV metrics, confusion matrices, species top-k accuracy, and a summary.\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ---------------- Config / paths ----------------\n",
    "DOWNLOAD_DIR = Path(globals().get(\"DOWNLOAD_DIR\", \"./ncbi_blast_db\"))\n",
    "EXTRACT_DIR = DOWNLOAD_DIR / \"extracted\"\n",
    "OUT_DIR = EXTRACT_DIR  # output saved here\n",
    "\n",
    "# candidate checkpoint names (prefer best pseudo checkpoint)\n",
    "CAND_CKPTS = [\n",
    "    \"best_shared_heads_pseudo_tensordataset_fix.pt\",\n",
    "    \"best_shared_heads_pseudo_tensordataset_fix.pt\",\n",
    "    \"best_shared_heads_pseudo_tensordataset_fix.pt\",\n",
    "    \"best_shared_heads_pseudo_tensordataset_fix.pt\",\n",
    "    \"best_shared_heads_pseudo_tensordataset_fix.pt\",  # duplicates safe — leave as search list\n",
    "    \"best_shared_heads_pseudo_tensordataset_fix.pt\", \n",
    "    \"best_shared_heads_pseudo_tensordataset_fix.pt\",\n",
    "    \"best_shared_heads_labeled.pt\",\n",
    "    \"best_shared_heads_retrain.pt\",\n",
    "    \"best_shared_heads.pt\",\n",
    "    \"best_shared_heads_pseudo.pt\",\n",
    "    \"best_shared_heads_pseudo_tensordataset.pt\"\n",
    "]\n",
    "\n",
    "# flexible: search for \"best*.pt\" as fallback\n",
    "def find_checkpoint():\n",
    "    for name in CAND_CKPTS:\n",
    "        p = EXTRACT_DIR / name\n",
    "        if p.exists():\n",
    "            return p\n",
    "    # fallback: pick the newest best_.pt or any *shared.pt\n",
    "    cand = sorted(EXTRACT_DIR.glob(\"best*.pt\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    if cand:\n",
    "        return cand[0]\n",
    "    cand2 = sorted(EXTRACT_DIR.glob(\"shared.pt\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    if cand2:\n",
    "        return cand2[0]\n",
    "    raise FileNotFoundError(\"No checkpoint found in extracted/ (tried best*.pt and shared.pt)\")\n",
    "\n",
    "CKPT_PATH = find_checkpoint()\n",
    "print(\"[USE] checkpoint:\", CKPT_PATH.name)\n",
    "\n",
    "# ---------------- Load label encoders ----------------\n",
    "# prefer rebuilt encoders used for training\n",
    "enc_paths = [\n",
    "    EXTRACT_DIR / \"label_encoders_rebuilt_v2.pkl\",\n",
    "    EXTRACT_DIR / \"label_encoders_used.pkl\",\n",
    "    EXTRACT_DIR / \"label_encoders_v2.pkl\",\n",
    "    EXTRACT_DIR / \"label_encoders.pkl\"\n",
    "]\n",
    "label_encoders = None\n",
    "for p in enc_paths:\n",
    "    if p.exists():\n",
    "        import pickle\n",
    "        with open(p, \"rb\") as fh:\n",
    "            label_encoders = pickle.load(fh)\n",
    "        print(\"[LOAD] label_encoders from\", p.name)\n",
    "        break\n",
    "if label_encoders is None:\n",
    "    raise FileNotFoundError(\"Label encoders not found in extracted/. Expected one of: \" + \", \".join(str(p.name) for p in enc_paths))\n",
    "\n",
    "RANKS = [\"kingdom\",\"phylum\",\"class\",\"order\",\"family\",\"genus\",\"species\"]\n",
    "\n",
    "# ---------------- Load embeddings ----------------\n",
    "EMB_FULL = EXTRACT_DIR / \"embeddings.npy\"\n",
    "EMB_PCA  = EXTRACT_DIR / \"embeddings_pca.npy\"\n",
    "if EMB_FULL.exists():\n",
    "    X = np.load(EMB_FULL)\n",
    "    print(\"[LOAD] embeddings.npy shape:\", X.shape)\n",
    "elif EMB_PCA.exists():\n",
    "    X = np.load(EMB_PCA)\n",
    "    print(\"[LOAD] embeddings_pca.npy shape:\", X.shape)\n",
    "else:\n",
    "    raise FileNotFoundError(\"No embeddings file found in extracted/ (embeddings.npy or embeddings_pca.npy)\")\n",
    "\n",
    "# ---------------- Load validation indices ----------------\n",
    "# Prefer previously saved group-wise split files\n",
    "possible_idx_names = [\n",
    "    EXTRACT_DIR / \"val_idx_by_acc.npy\",\n",
    "    EXTRACT_DIR / \"val_idx.npy\",\n",
    "    EXTRACT_DIR / \"val_idx_by_acc.npy\",\n",
    "    EXTRACT_DIR / \"val_idx_by_acc.npy\"\n",
    "]\n",
    "val_idx = None\n",
    "for p in possible_idx_names:\n",
    "    if p.exists():\n",
    "        val_idx = np.load(p)\n",
    "        print(\"[LOAD] val_idx from\", p.name, \"len=\", len(val_idx))\n",
    "        break\n",
    "\n",
    "# fallback: use train_val_split_by_accession.json if present (and choose val groups)\n",
    "if val_idx is None:\n",
    "    split_json = EXTRACT_DIR / \"train_val_split_by_accession.json\"\n",
    "    if split_json.exists():\n",
    "        data = json.loads(split_json.read_text())\n",
    "        val_idx = np.array(data.get(\"val_idx\", data.get(\"val_indices\", [])), dtype=int)\n",
    "        print(\"[LOAD] val_idx from\", split_json.name, \"len=\", len(val_idx))\n",
    "\n",
    "# fallback: if val_idx still None, try to use val_dataset from globals\n",
    "if val_idx is None:\n",
    "    if \"val_dataset\" in globals():\n",
    "        # val_dataset is a TensorDataset; we'll infer indices by len(train_dataset) + positions not available.\n",
    "        # Instead, we will create val set from metadata: try embeddings_meta_clustered.csv and predictions_summary_calibrated.csv\n",
    "        print(\"[WARN] val_idx files not found; attempting to build val set from predictions_summary_calibrated.csv\")\n",
    "        preds_csv = EXTRACT_DIR / \"predictions_summary_calibrated.csv\"\n",
    "        meta_csv = EXTRACT_DIR / \"embeddings_meta_clustered.csv\"\n",
    "        if preds_csv.exists() and meta_csv.exists():\n",
    "            df_preds = pd.read_csv(preds_csv, dtype=str, keep_default_na=False, na_filter=False)\n",
    "            # expect 'is_val' or 'fold' column not guaranteed — as fallback pick last 15% as val\n",
    "            n = len(df_preds)\n",
    "            k = max(1, int(0.15*n))\n",
    "            val_idx = np.arange(n-k, n, dtype=int)\n",
    "            print(\"[FALLBACK] chosen last 15% indices for val (len=%d)\" % len(val_idx))\n",
    "        else:\n",
    "            raise RuntimeError(\"Cannot build val_idx automatically. Provide saved val_idx numpy or val_dataset in globals.\")\n",
    "else:\n",
    "    val_idx = np.asarray(val_idx, dtype=int)\n",
    "\n",
    "# Truncate if embeddings shorter\n",
    "n_samples = X.shape[0]\n",
    "val_idx = val_idx[val_idx < n_samples]\n",
    "print(\"[INFO] using val_idx length:\", len(val_idx))\n",
    "\n",
    "# ---------------- Build val tensors ----------------\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "X_val = torch.tensor(X[val_idx], dtype=torch.float32)\n",
    "y_val_list = []\n",
    "# load per-rank encoded y arrays if present (y_encoded_rebuilt_*.npy), else try predictions CSV to derive labels\n",
    "y_prefix = EXTRACT_DIR / \"y_encoded_rebuilt_\"\n",
    "has_y_arrays = True\n",
    "for r in RANKS:\n",
    "    p = Path(f\"{y_prefix}{r}.npy\")\n",
    "    if not p.exists():\n",
    "        has_y_arrays = False\n",
    "        break\n",
    "\n",
    "if has_y_arrays:\n",
    "    for r in RANKS:\n",
    "        arr = np.load(Path(f\"{y_prefix}{r}.npy\"))\n",
    "        if arr.shape[0] < n_samples:\n",
    "            # pad\n",
    "            pad = np.full(n_samples - arr.shape[0], fill_value=0, dtype=int)\n",
    "            arr = np.concatenate([arr, pad])\n",
    "        y_val_list.append(torch.tensor(arr[val_idx], dtype=torch.long))\n",
    "    print(\"[LOAD] loaded y_encoded_rebuilt_* arrays for true labels.\")\n",
    "else:\n",
    "    # fallback: try to get true labels from predictions_summary_calibrated.csv (they may be same as preds)\n",
    "    preds_csv = EXTRACT_DIR / \"predictions_summary_calibrated.csv\"\n",
    "    if preds_csv.exists():\n",
    "        dfp = pd.read_csv(preds_csv, dtype=str, keep_default_na=False, na_filter=False)\n",
    "        # attempt to get per-rank predicted label columns; we'll treat them as \"truth\" only if they were originally assigned (best-effort)\n",
    "        for r in RANKS:\n",
    "            col = f\"{r}_pred\"\n",
    "            if col in dfp.columns:\n",
    "                # map predicted label strings to encoder indices (if encoder contains that label)\n",
    "                lab_to_idx = {lab: idx for idx, lab in enumerate(label_encoders[r].classes_)}\n",
    "                idxs = []\n",
    "                for lab in dfp[col].astype(str).tolist():\n",
    "                    idxs.append(lab_to_idx.get(lab, 0))\n",
    "                arr = np.array(idxs, dtype=int)\n",
    "                y_val_list.append(torch.tensor(arr[val_idx], dtype=torch.long))\n",
    "            else:\n",
    "                # default all zero\n",
    "                y_val_list.append(torch.zeros(len(val_idx), dtype=torch.long))\n",
    "        print(\"[FALLBACK] used predictions_summary_calibrated.csv to build pseudo 'truth' arrays (best-effort).\")\n",
    "    else:\n",
    "        raise RuntimeError(\"True label arrays not found (y_encoded_rebuilt_*.npy) and predictions_summary_calibrated.csv missing. Cannot compute diagnostics.\")\n",
    "\n",
    "# assemble val dataset & loader\n",
    "# final layout: (x, y0, y1, ..., y6)\n",
    "val_dataset = TensorDataset(X_val, *y_val_list)\n",
    "VAL_BATCH = min(128, max(16, len(val_dataset)//2))\n",
    "val_loader = DataLoader(val_dataset, batch_size=VAL_BATCH, shuffle=False)\n",
    "\n",
    "# ---------------- Build model_obj (no new class) ----------------\n",
    "hidden_dim = 256\n",
    "model_obj = nn.Module()\n",
    "model_obj.shared = nn.Sequential(\n",
    "    nn.Linear(X_val.shape[1], hidden_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "    nn.ReLU()\n",
    ")\n",
    "heads = {}\n",
    "for r in RANKS:\n",
    "    ncls = len(label_encoders[r].classes_)\n",
    "    heads[r] = nn.Linear(hidden_dim // 2, ncls)\n",
    "model_obj.heads = nn.ModuleDict(heads)\n",
    "\n",
    "# bind forward\n",
    "import types\n",
    "def _forward(self, x):\n",
    "    h = self.shared(x)\n",
    "    return { r: self.heads[r](h) for r in self.heads }\n",
    "model_obj.forward = types.MethodType(_forward, model_obj)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_obj.to(device)\n",
    "\n",
    "# ---------------- Load checkpoint into model ----------------\n",
    "ck = torch.load(CKPT_PATH, map_location=\"cpu\")\n",
    "loaded = False\n",
    "# try common shapes\n",
    "try:\n",
    "    if isinstance(ck, dict) and \"model_state\" in ck:\n",
    "        model_obj.load_state_dict(ck[\"model_state\"], strict=False); loaded = True\n",
    "    elif isinstance(ck, dict) and \"shared_state\" in ck and \"heads_state\" in ck:\n",
    "        # merge\n",
    "        cur_sd = model_obj.state_dict()\n",
    "        for k,v in ck[\"shared_state\"].items():\n",
    "            tk = f\"shared.{k}\" if not k.startswith(\"shared.\") else k\n",
    "            if tk in cur_sd: cur_sd[tk] = v\n",
    "        for hname, hsd in ck[\"heads_state\"].items():\n",
    "            for subk, v in hsd.items():\n",
    "                tk = f\"heads.{hname}.{subk}\"\n",
    "                if tk in cur_sd: cur_sd[tk] = v\n",
    "        model_obj.load_state_dict(cur_sd, strict=False); loaded = True\n",
    "    elif isinstance(ck, dict) and \"state_dict\" in ck:\n",
    "        model_obj.load_state_dict(ck[\"state_dict\"], strict=False); loaded = True\n",
    "    else:\n",
    "        # try as raw state_dict\n",
    "        model_obj.load_state_dict(ck, strict=False); loaded = True\n",
    "except Exception as e:\n",
    "    print(\"[WARN] checkpoint load partial/failed:\", e)\n",
    "\n",
    "print(\"[MODEL] checkpoint loaded -> device:\", device, \"loaded_ok:\", loaded)\n",
    "model_obj.to(device)\n",
    "model_obj.eval()\n",
    "\n",
    "# ---------------- Run inference on validation set & gather predictions ----------------\n",
    "all_preds = {r: [] for r in RANKS}\n",
    "all_probs = {r: [] for r in RANKS}\n",
    "all_trues = {r: [] for r in RANKS}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        x = batch[0].to(device)\n",
    "        y_trues_batch = [batch[1 + i].numpy() for i in range(len(RANKS))]\n",
    "        outputs = model_obj(x)\n",
    "        for r in RANKS:\n",
    "            logits = outputs[r]\n",
    "            probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "            preds = np.argmax(probs, axis=1)\n",
    "            all_preds[r].extend(preds.tolist())\n",
    "            all_probs[r].extend(probs.tolist())\n",
    "            all_trues[r].extend(y_trues_batch[RANKS.index(r)].tolist())\n",
    "\n",
    "# convert to numpy arrays\n",
    "for r in RANKS:\n",
    "    all_preds[r] = np.array(all_preds[r], dtype=int)\n",
    "    all_trues[r] = np.array(all_trues[r], dtype=int)\n",
    "    all_probs[r] = np.array(all_probs[r], dtype=float)\n",
    "\n",
    "# ---------------- Per-rank per-class metrics, confusion matrices & CSV saves ----------------\n",
    "summary_rows = []\n",
    "for r in RANKS:\n",
    "    true = all_trues[r]\n",
    "    pred = all_preds[r]\n",
    "    classes = list(label_encoders[r].classes_)\n",
    "    # compute precision/recall/f1/support\n",
    "    p, rcall, f1, support = precision_recall_fscore_support(true, pred, labels=np.arange(len(classes)), zero_division=0)\n",
    "    df_cls = pd.DataFrame({\n",
    "        \"class_index\": np.arange(len(classes)),\n",
    "        \"class_name\": classes,\n",
    "        \"precision\": p,\n",
    "        \"recall\": rcall,\n",
    "        \"f1\": f1,\n",
    "        \"support\": support\n",
    "    })\n",
    "    # save per-rank class metrics\n",
    "    fn = OUT_DIR / f\"per_class_metrics_{r}.csv\"\n",
    "    df_cls.to_csv(fn, index=False)\n",
    "    # confusion matrix (rows=true, cols=pred)\n",
    "    try:\n",
    "        cm = confusion_matrix(true, pred, labels=np.arange(len(classes)))\n",
    "        cm_df = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "        cm_file = OUT_DIR / f\"confusion_matrix_{r}.csv\"\n",
    "        cm_df.to_csv(cm_file)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] could not create confusion matrix for {r}: {e}\")\n",
    "    # rank summary\n",
    "    macro_f1 = float(np.nanmean(df_cls[\"f1\"]))\n",
    "    acc = float(accuracy_score(true, pred))\n",
    "    mean_conf = float(np.mean(np.max(all_probs[r], axis=1))) if all_probs[r].size else 0.0\n",
    "    summary_rows.append({\"rank\": r, \"n_classes\": len(classes), \"val_acc\": acc, \"val_macro_f1\": macro_f1, \"mean_confidence\": mean_conf})\n",
    "    print(f\"[METRIC] {r:8s} | acc={acc:.4f} | macro_f1={macro_f1:.4f} | mean_conf={mean_conf:.4f} | saved -> {fn.name}\")\n",
    "\n",
    "pd.DataFrame(summary_rows).to_csv(OUT_DIR / \"evaluation_summary_by_rank.csv\", index=False)\n",
    "\n",
    "# ---------------- species top-k accuracy (Top-1 and Top-5) ----------------\n",
    "if \"species\" in RANKS:\n",
    "    sp_probs = all_probs[\"species\"]  # shape (n_val, n_species)\n",
    "    sp_true = all_trues[\"species\"]\n",
    "    top1 = np.mean(np.argmax(sp_probs, axis=1) == sp_true)\n",
    "    k = min(5, sp_probs.shape[1])\n",
    "    topk_preds = np.argsort(sp_probs, axis=1)[:, ::-1][:, :k]  # descending top-k indices\n",
    "    topk_hit = np.array([1 if sp_true[i] in topk_preds[i] else 0 for i in range(len(sp_true))])\n",
    "    topk = np.mean(topk_hit)\n",
    "    print(f\"[TOP-K] species top-1 acc: {top1:.4f} | top-{k} acc: {topk:.4f}\")\n",
    "    pd.DataFrame({\"top1\": [float(top1)], f\"top{k}\": [float(topk)]}).to_csv(OUT_DIR / \"species_topk_accuracy.csv\", index=False)\n",
    "\n",
    "# ---------------- highlight worst classes (low f1 but support >= threshold) ----------------\n",
    "alerts = []\n",
    "for r in RANKS:\n",
    "    df_cls = pd.read_csv(OUT_DIR / f\"per_class_metrics_{r}.csv\")\n",
    "    # ignore UNASSIGNED if present when highlighting (but still saved)\n",
    "    df_non_un = df_cls.copy()\n",
    "    if \"UNASSIGNED\" in df_non_un[\"class_name\"].values:\n",
    "        df_non_un = df_non_un[df_non_un[\"class_name\"] != \"UNASSIGNED\"]\n",
    "    # find low f1 classes with reasonable support\n",
    "    low_f1 = df_non_un[(df_non_un[\"support\"] >= 5)].sort_values(\"f1\").head(8)\n",
    "    if not low_f1.empty:\n",
    "        alerts.append((r, low_f1[[\"class_name\",\"support\",\"f1\"]].to_dict(orient=\"records\")))\n",
    "\n",
    "# print alert summary\n",
    "if alerts:\n",
    "    print(\"\\n[ALERT] Classes with low F1 (support>=5) — inspect CSVs for details:\")\n",
    "    for rank, recs in alerts:\n",
    "        print(f\" - {rank}: {len(recs)} low-performing classes (examples):\")\n",
    "        for ex in recs[:3]:\n",
    "            print(f\"     {ex['class_name'][:60]:60s} support={int(ex['support'])}, f1={ex['f1']:.3f}\")\n",
    "else:\n",
    "    print(\"\\n[ALERT] No low-F1 classes with support>=5 found in val set.\")\n",
    "\n",
    "print(\"\\n[COMPLETE] Evaluation outputs saved to:\", EXTRACT_DIR)\n",
    "print(\"Key files: evaluation_summary_by_rank.csv, per_class_metrics_<rank>.csv, confusion_matrix_<rank>.csv, species_topk_accuracy.csv (if species present).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a5e6238e-06a6-422b-a292-ebefe3fcb8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[USE] checkpoint: best_shared_heads_labeled.pt\n",
      "[LOAD] label_encoders from label_encoders_rebuilt_v2.pkl\n",
      "[LOAD] embeddings shape: (2555, 128)\n",
      "[INFO] using val_idx length: 373\n",
      "[MODEL] checkpoint loaded_ok: True\n",
      "[PLOT] saved reliability_kingdom.png (n=373)\n",
      "[PLOT] saved reliability_phylum.png (n=373)\n",
      "[PLOT] saved reliability_class.png (n=373)\n",
      "[PLOT] saved reliability_order.png (n=373)\n",
      "[PLOT] saved reliability_family.png (n=373)\n",
      "[PLOT] saved reliability_genus.png (n=373)\n",
      "[PLOT] saved reliability_species.png (n=373)\n",
      "[SAVE] calibration_metrics_by_rank.csv written to ncbi_blast_db\\extracted\n",
      "\n",
      "=== Calibration summary ===\n",
      "kingdom    | n= 373 | acc=0.4638 | mean_conf=1.0000 | ECE=0.5362 | MCE=0.5362 | Brier=1.0724\n",
      "phylum     | n= 373 | acc=0.4611 | mean_conf=0.9539 | ECE=0.4927 | MCE=0.9017 | Brier=1.0174\n",
      "class      | n= 373 | acc=0.4638 | mean_conf=0.9364 | ECE=0.4726 | MCE=0.8845 | Brier=0.9912\n",
      "order      | n= 373 | acc=0.4584 | mean_conf=0.9348 | ECE=0.4764 | MCE=0.8365 | Brier=0.9968\n",
      "family     | n= 373 | acc=0.4584 | mean_conf=0.9070 | ECE=0.4509 | MCE=0.8587 | Brier=0.9677\n",
      "genus      | n= 373 | acc=0.4584 | mean_conf=0.8661 | ECE=0.4076 | MCE=0.6318 | Brier=0.9337\n",
      "species    | n= 373 | acc=0.3029 | mean_conf=0.4208 | ECE=0.2456 | MCE=0.6364 | Brier=0.9161\n",
      "\n",
      "Outputs: calibration_bins_<rank>.csv, reliability_<rank>.png, calibration_metrics_by_rank.csv saved in ncbi_blast_db\\extracted\n"
     ]
    }
   ],
   "source": [
    "# Cell B — Calibration diagnostics & reliability diagrams\n",
    "# Saves: calibration_metrics_by_rank.csv, calibration_bins_<rank>.csv, reliability_<rank>.png\n",
    "import json, math, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import brier_score_loss, accuracy_score\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "DOWNLOAD_DIR = Path(globals().get(\"DOWNLOAD_DIR\", \"./ncbi_blast_db\"))\n",
    "EXTRACT_DIR = DOWNLOAD_DIR / \"extracted\"\n",
    "OUT_DIR = EXTRACT_DIR\n",
    "RANKS = [\"kingdom\",\"phylum\",\"class\",\"order\",\"family\",\"genus\",\"species\"]\n",
    "BINS = 15\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# helper: find checkpoint (reuse strategy used before)\n",
    "def find_checkpoint():\n",
    "    candidates = sorted(EXTRACT_DIR.glob(\"best*.pt\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    cand2 = sorted(EXTRACT_DIR.glob(\"shared.pt\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    allc = candidates + cand2\n",
    "    if allc:\n",
    "        return allc[0]\n",
    "    raise FileNotFoundError(\"No checkpoint found in extracted/ (looked for best*.pt and shared.pt)\")\n",
    "\n",
    "CKPT = find_checkpoint()\n",
    "print(f\"[USE] checkpoint: {CKPT.name}\")\n",
    "\n",
    "# ---------------- Load label encoders ----------------\n",
    "enc_paths = [\n",
    "    EXTRACT_DIR / \"label_encoders_rebuilt_v2.pkl\",\n",
    "    EXTRACT_DIR / \"label_encoders_used.pkl\",\n",
    "    EXTRACT_DIR / \"label_encoders_v2.pkl\",\n",
    "    EXTRACT_DIR / \"label_encoders.pkl\"\n",
    "]\n",
    "label_encoders = None\n",
    "for p in enc_paths:\n",
    "    if p.exists():\n",
    "        import pickle\n",
    "        with open(p, \"rb\") as fh: label_encoders = pickle.load(fh)\n",
    "        print(\"[LOAD] label_encoders from\", p.name)\n",
    "        break\n",
    "if label_encoders is None:\n",
    "    raise FileNotFoundError(\"No label_encoders pickle found in extracted/\")\n",
    "\n",
    "# ---------------- Load embeddings and val idx ----------------\n",
    "EMB_FULL = EXTRACT_DIR / \"embeddings.npy\"\n",
    "EMB_PCA  = EXTRACT_DIR / \"embeddings_pca.npy\"\n",
    "if EMB_FULL.exists():\n",
    "    X = np.load(EMB_FULL)\n",
    "else:\n",
    "    X = np.load(EMB_PCA)\n",
    "print(\"[LOAD] embeddings shape:\", X.shape)\n",
    "\n",
    "val_idx_path_candidates = [\n",
    "    EXTRACT_DIR / \"val_idx_by_acc.npy\",\n",
    "    EXTRACT_DIR / \"val_idx.npy\",\n",
    "    EXTRACT_DIR / \"val_idx_by_acc.npy\"\n",
    "]\n",
    "val_idx = None\n",
    "for p in val_idx_path_candidates:\n",
    "    if p.exists():\n",
    "        val_idx = np.load(p)\n",
    "        break\n",
    "if val_idx is None:\n",
    "    # fallback to predictions_summary_calibrated.csv -> last 15%\n",
    "    preds_csv = EXTRACT_DIR / \"predictions_summary_calibrated.csv\"\n",
    "    if preds_csv.exists():\n",
    "        dfp = pd.read_csv(preds_csv, dtype=str, keep_default_na=False, na_filter=False)\n",
    "        n = len(dfp); k = max(1, int(0.15*n))\n",
    "        val_idx = np.arange(n-k, n, dtype=int)\n",
    "        print(\"[FALLBACK] val_idx = last 15% len=\", len(val_idx))\n",
    "    else:\n",
    "        raise RuntimeError(\"val_idx not found and predictions_summary_calibrated.csv not present to fallback.\")\n",
    "val_idx = val_idx[val_idx < X.shape[0]]\n",
    "print(\"[INFO] using val_idx length:\", len(val_idx))\n",
    "\n",
    "# ---------------- Load true labels arrays if present ----------------\n",
    "y_prefix = EXTRACT_DIR / \"y_encoded_rebuilt_\"\n",
    "y_true = {}\n",
    "have_y = True\n",
    "for r in RANKS:\n",
    "    p = Path(f\"{y_prefix}{r}.npy\")\n",
    "    if p.exists():\n",
    "        arr = np.load(p)\n",
    "        if arr.shape[0] < X.shape[0]:\n",
    "            pad = np.full(X.shape[0] - arr.shape[0], fill_value=0, dtype=int)\n",
    "            arr = np.concatenate([arr, pad])\n",
    "        y_true[r] = arr[val_idx].astype(int)\n",
    "    else:\n",
    "        have_y = False\n",
    "        break\n",
    "if not have_y:\n",
    "    # fallback to predictions_summary_calibrated.csv to get per-rank predicted labels as a proxy for truth\n",
    "    preds_csv = EXTRACT_DIR / \"predictions_summary_calibrated.csv\"\n",
    "    if preds_csv.exists():\n",
    "        dfp = pd.read_csv(preds_csv, dtype=str, keep_default_na=False, na_filter=False)\n",
    "        for r in RANKS:\n",
    "            col = f\"{r}_pred\"\n",
    "            if col in dfp.columns:\n",
    "                lab_to_idx = {lab: i for i, lab in enumerate(label_encoders[r].classes_)}\n",
    "                idxs = [lab_to_idx.get(s, 0) for s in dfp[col].astype(str).tolist()]\n",
    "                arr = np.array(idxs, dtype=int)\n",
    "                y_true[r] = arr[val_idx]\n",
    "            else:\n",
    "                y_true[r] = np.zeros(len(val_idx), dtype=int)\n",
    "        print(\"[FALLBACK] used predictions_summary_calibrated.csv as proxy truth.\")\n",
    "    else:\n",
    "        raise RuntimeError(\"True labels not found and fallback predictions csv not present.\")\n",
    "\n",
    "# ---------------- Build model_obj and load checkpoint (same safe approach) ----------------\n",
    "hidden_dim = 256\n",
    "model_obj = nn.Module()\n",
    "model_obj.shared = nn.Sequential(\n",
    "    nn.Linear(X.shape[1], hidden_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "    nn.ReLU()\n",
    ")\n",
    "heads = {}\n",
    "for r in RANKS:\n",
    "    ncls = len(label_encoders[r].classes_)\n",
    "    heads[r] = nn.Linear(hidden_dim // 2, ncls)\n",
    "model_obj.heads = nn.ModuleDict(heads)\n",
    "import types\n",
    "def _forward(self, x):\n",
    "    h = self.shared(x)\n",
    "    return { r: self.heads[r](h) for r in self.heads }\n",
    "model_obj.forward = types.MethodType(_forward, model_obj)\n",
    "model_obj.to(DEVICE)\n",
    "\n",
    "ck = torch.load(CKPT, map_location=\"cpu\")\n",
    "loaded_ok = False\n",
    "try:\n",
    "    if isinstance(ck, dict) and \"model_state\" in ck:\n",
    "        model_obj.load_state_dict(ck[\"model_state\"], strict=False); loaded_ok=True\n",
    "    elif isinstance(ck, dict) and \"shared_state\" in ck and \"heads_state\" in ck:\n",
    "        cur_sd = model_obj.state_dict()\n",
    "        for k,v in ck[\"shared_state\"].items():\n",
    "            tk = f\"shared.{k}\" if not k.startswith(\"shared.\") else k\n",
    "            if tk in cur_sd: cur_sd[tk] = v\n",
    "        for hname, hsd in ck[\"heads_state\"].items():\n",
    "            for subk, v in hsd.items():\n",
    "                tk = f\"heads.{hname}.{subk}\"\n",
    "                if tk in cur_sd: cur_sd[tk] = v\n",
    "        model_obj.load_state_dict(cur_sd, strict=False); loaded_ok=True\n",
    "    elif isinstance(ck, dict) and \"state_dict\" in ck:\n",
    "        model_obj.load_state_dict(ck[\"state_dict\"], strict=False); loaded_ok=True\n",
    "    else:\n",
    "        model_obj.load_state_dict(ck, strict=False); loaded_ok=True\n",
    "except Exception as e:\n",
    "    print(\"[WARN] checkpoint load partial/failed:\", e)\n",
    "print(\"[MODEL] checkpoint loaded_ok:\", loaded_ok)\n",
    "model_obj.eval()\n",
    "\n",
    "# ---------------- Build val DataLoader ----------------\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "X_val = torch.tensor(X[val_idx], dtype=torch.float32)\n",
    "y_val_tensors = [torch.tensor(y_true[r], dtype=torch.long) for r in RANKS]\n",
    "val_dataset = TensorDataset(X_val, *y_val_tensors)\n",
    "val_loader = DataLoader(val_dataset, batch_size=min(256, max(16, len(val_dataset)//4)), shuffle=False)\n",
    "\n",
    "# ---------------- Inference on val -> probs per rank ----------------\n",
    "all_probs = {r: [] for r in RANKS}\n",
    "all_preds = {r: [] for r in RANKS}\n",
    "all_true  = {r: [] for r in RANKS}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        x = batch[0].to(DEVICE)\n",
    "        outputs = model_obj(x)\n",
    "        for i, r in enumerate(RANKS):\n",
    "            logits = outputs[r]\n",
    "            probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "            preds = np.argmax(probs, axis=1)\n",
    "            all_probs[r].append(probs)\n",
    "            all_preds[r].append(preds)\n",
    "            all_true[r].append(batch[1 + i].numpy())\n",
    "\n",
    "# concat\n",
    "for r in RANKS:\n",
    "    if len(all_probs[r]) == 0:\n",
    "        all_probs[r] = np.zeros((0, len(label_encoders[r].classes_)))\n",
    "        all_preds[r] = np.array([], dtype=int)\n",
    "        all_true[r]  = np.array([], dtype=int)\n",
    "    else:\n",
    "        all_probs[r] = np.vstack(all_probs[r])\n",
    "        all_preds[r] = np.concatenate(all_preds[r])\n",
    "        all_true[r]  = np.concatenate(all_true[r])\n",
    "\n",
    "# ---------------- calibration metrics functions ----------------\n",
    "def compute_ece(probs, true, n_bins=15):\n",
    "    \"\"\"ECE computed using predicted-class confidence and correctness (binary).\"\"\"\n",
    "    if probs.shape[0] == 0:\n",
    "        return {\"ece\": np.nan, \"mce\": np.nan, \"bins\": []}\n",
    "    confidences = np.max(probs, axis=1)\n",
    "    preds = np.argmax(probs, axis=1)\n",
    "    correct = (preds == true).astype(float)\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    bin_stats = []\n",
    "    ece = 0.0\n",
    "    mce = 0.0\n",
    "    n = len(confidences)\n",
    "    for i in range(n_bins):\n",
    "        l, u = bins[i], bins[i+1]\n",
    "        mask = (confidences > l) & (confidences <= u) if i < n_bins-1 else (confidences >= l) & (confidences <= u)\n",
    "        cnt = mask.sum()\n",
    "        if cnt == 0:\n",
    "            avg_conf = 0.0; acc = 0.0\n",
    "        else:\n",
    "            avg_conf = confidences[mask].mean()\n",
    "            acc = correct[mask].mean()\n",
    "        gap = abs(acc - avg_conf)\n",
    "        ece += (cnt / n) * gap\n",
    "        mce = max(mce, gap)\n",
    "        bin_stats.append({\"bin_low\": float(l), \"bin_high\": float(u), \"count\": int(cnt), \"avg_conf\": float(avg_conf), \"accuracy\": float(acc)})\n",
    "    return {\"ece\": float(ece), \"mce\": float(mce), \"bins\": bin_stats}\n",
    "\n",
    "def multiclass_brier(probs, true):\n",
    "    \"\"\"Multiclass Brier score = mean over samples of sum_k (p_k - y_k)^2\"\"\"\n",
    "    if probs.shape[0] == 0:\n",
    "        return np.nan\n",
    "    n_samples, n_classes = probs.shape\n",
    "    y_onehot = np.zeros_like(probs)\n",
    "    y_onehot[np.arange(n_samples), true] = 1.0\n",
    "    bs = ((probs - y_onehot) ** 2).sum(axis=1).mean()\n",
    "    return float(bs)\n",
    "\n",
    "# ---------------- compute per-rank calibration metrics, save plots & CSVs ----------------\n",
    "cal_metrics_rows = []\n",
    "for r in RANKS:\n",
    "    probs = all_probs[r]\n",
    "    true  = all_true[r]\n",
    "    n_samples = probs.shape[0]\n",
    "    if n_samples == 0:\n",
    "        print(f\"[SKIP] rank {r} has 0 val samples\")\n",
    "        continue\n",
    "    # compute ECE, MCE and Brier\n",
    "    calib = compute_ece(probs, true, n_bins=BINS)\n",
    "    brier = multiclass_brier(probs, true)\n",
    "    mean_conf = float(np.max(probs, axis=1).mean())\n",
    "    acc = float((np.argmax(probs, axis=1) == true).mean())\n",
    "    cal_metrics_rows.append({\n",
    "        \"rank\": r,\n",
    "        \"n_samples\": int(n_samples),\n",
    "        \"ece\": calib[\"ece\"],\n",
    "        \"mce\": calib[\"mce\"],\n",
    "        \"brier\": brier,\n",
    "        \"mean_confidence\": mean_conf,\n",
    "        \"accuracy\": acc\n",
    "    })\n",
    "\n",
    "    # save bin-level CSV\n",
    "    df_bins = pd.DataFrame(calib[\"bins\"])\n",
    "    df_bins.to_csv(OUT_DIR / f\"calibration_bins_{r}.csv\", index=False)\n",
    "\n",
    "    # reliability diagram (bar: bin centers with accuracy; overlay diagonal)\n",
    "    bin_centers = [(b[\"bin_low\"] + b[\"bin_high\"]) / 2.0 for b in calib[\"bins\"]]\n",
    "    accs = [b[\"accuracy\"] for b in calib[\"bins\"]]\n",
    "    confs = [b[\"avg_conf\"] for b in calib[\"bins\"]]\n",
    "    counts = [b[\"count\"] for b in calib[\"bins\"]]\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.plot([0,1],[0,1], linestyle=\"--\", color=\"gray\", label=\"Perfect calibration\")\n",
    "    plt.plot(bin_centers, accs, marker=\"o\", label=\"Accuracy (per-bin)\")\n",
    "    plt.plot(bin_centers, confs, marker=\"x\", label=\"Avg Confidence (per-bin)\")\n",
    "    # visual marker sized by count\n",
    "    maxc = max(counts) if counts else 1\n",
    "    for x, a, c in zip(bin_centers, accs, counts):\n",
    "        plt.scatter(x, a, s=20 + (120.0 * (c / maxc)), alpha=0.6, color=\"C0\")\n",
    "    plt.xlabel(\"Confidence\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(f\"Reliability diagram — {r} (ECE={calib['ece']:.4f})\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(alpha=0.2)\n",
    "    outpng = OUT_DIR / f\"reliability_{r}.png\"\n",
    "    plt.savefig(outpng, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"[PLOT] saved {outpng.name} (n={n_samples})\")\n",
    "\n",
    "# ---------------- save summary CSV ----------------\n",
    "pd.DataFrame(cal_metrics_rows).to_csv(OUT_DIR / \"calibration_metrics_by_rank.csv\", index=False)\n",
    "print(\"[SAVE] calibration_metrics_by_rank.csv written to\", OUT_DIR)\n",
    "\n",
    "# ---------------- Print a short summary ----------------\n",
    "print(\"\\n=== Calibration summary ===\")\n",
    "for row in cal_metrics_rows:\n",
    "    print(f\"{row['rank']:10s} | n={row['n_samples']:4d} | acc={row['accuracy']:.4f} | mean_conf={row['mean_confidence']:.4f} | ECE={row['ece']:.4f} | MCE={row['mce']:.4f} | Brier={row['brier']:.4f}\")\n",
    "\n",
    "print(\"\\nOutputs: calibration_bins_<rank>.csv, reliability_<rank>.png, calibration_metrics_by_rank.csv saved in\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3fe47437-963b-4b24-b5f6-df4d58bf2827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD] predictions_with_mc_uncertainty.csv rows=2557\n",
      "[SAVE] novel candidate cluster priority -> novel_candidates_priority.csv  (clusters=104)\n",
      "\n",
      "Top-10 clusters by cluster_mean_priority:\n",
      "cluster_label  cluster_n  cluster_mean_priority  cluster_mean_novelty  cluster_species_consensus_frac\n",
      "           14          6               0.422928                   0.0                             1.0\n",
      "           31          6               0.414074                   0.0                             1.0\n",
      "           30         12               0.393406                   0.0                             1.0\n",
      "            0          6               0.374172                   0.0                             1.0\n",
      "           13         10               0.367131                   0.0                             1.0\n",
      "           18          8               0.365694                   0.0                             1.0\n",
      "           19          6               0.354962                   0.0                             1.0\n",
      "            5          6               0.348243                   0.0                             1.0\n",
      "           45         10               0.332476                   0.0                             1.0\n",
      "            2          6               0.330606                   0.0                             1.0\n",
      "\n",
      "Top-10 sequence-level priority (idx, id, cluster, priority, novelty, species_mi, species_conf):\n",
      " idx=1260 id=LC876622.1                               cluster=31     pri=0.5076 nov=0.0000 mi=0.2545 conf=0.1736\n",
      " idx=1492 id=LC876540.1                               cluster=14     pri=0.4820 nov=0.0000 mi=0.2385 conf=0.2093\n",
      " idx=1929 id=PX281648.1                               cluster=-1     pri=0.4714 nov=0.0000 mi=0.2364 conf=0.1543\n",
      " idx=193 id=XR_013094388.1                           cluster=-1     pri=0.4682 nov=0.0000 mi=0.2449 conf=0.4471\n",
      " idx=1344 id=LC876538.1                               cluster=-1     pri=0.4547 nov=0.0000 mi=0.2248 conf=0.1514\n",
      " idx=1343 id=LC876539.1                               cluster=14     pri=0.4458 nov=0.0000 mi=0.2106 conf=0.1830\n",
      " idx=1491 id=LC876541.1                               cluster=14     pri=0.4450 nov=0.0000 mi=0.2114 conf=0.1456\n",
      " idx=1254 id=LC876628.1                               cluster=30     pri=0.4448 nov=0.0000 mi=0.2212 conf=0.1399\n",
      " idx=1411 id=LC876621.1                               cluster=31     pri=0.4377 nov=0.0000 mi=0.2182 conf=0.2355\n",
      " idx=1434 id=LC876598.1                               cluster=-1     pri=0.4363 nov=0.0000 mi=0.2079 conf=0.1795\n",
      "\n",
      "Cell complete. Files written:  novel_candidates_priority.csv\n"
     ]
    }
   ],
   "source": [
    "# Fixed cell: build cluster-level novel_candidates_priority.csv from predictions_with_mc_uncertainty\n",
    "# - robust: loads precomputed predictions CSV/JSONL if present (no re-run of MC)\n",
    "# - safe aggregation using named tuples; handles missing columns gracefully\n",
    "# - outputs: novel_candidates_priority.csv and a printed top-10 list\n",
    "import json, math, sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------- Paths ----------------\n",
    "DOWNLOAD_DIR = Path(globals().get(\"DOWNLOAD_DIR\", \"./ncbi_blast_db\"))\n",
    "EXTRACT_DIR = DOWNLOAD_DIR / \"extracted\"\n",
    "OUT_DIR = EXTRACT_DIR\n",
    "\n",
    "CSV_IN = OUT_DIR / \"predictions_with_mc_uncertainty.csv\"\n",
    "JSONL_IN = OUT_DIR / \"predictions_with_mc_uncertainty.jsonl\"\n",
    "OUT_NOVEL = OUT_DIR / \"novel_candidates_priority.csv\"\n",
    "\n",
    "# ---------------- Load predictions DataFrame (robust) ----------------\n",
    "if CSV_IN.exists():\n",
    "    df = pd.read_csv(CSV_IN, dtype=str, keep_default_na=False, na_filter=False)\n",
    "    print(f\"[LOAD] {CSV_IN.name} rows={len(df)}\")\n",
    "elif JSONL_IN.exists():\n",
    "    # load JSONL into DataFrame\n",
    "    rows = []\n",
    "    with open(JSONL_IN, \"r\", encoding=\"utf8\") as fh:\n",
    "        for line in fh:\n",
    "            try:\n",
    "                rows.append(json.loads(line))\n",
    "            except Exception:\n",
    "                pass\n",
    "    df = pd.DataFrame(rows)\n",
    "    print(f\"[LOAD] {JSONL_IN.name} rows={len(df)}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Neither predictions_with_mc_uncertainty.csv nor .jsonl found in extracted/. Run the MC cell first.\")\n",
    "\n",
    "# ensure index column exists\n",
    "if \"__row_index\" not in df.columns:\n",
    "    df[\"__row_index\"] = np.arange(len(df))\n",
    "\n",
    "# ---------------- normalize and detect key columns ----------------\n",
    "def find_col(candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "# cluster label column candidates\n",
    "cluster_col = find_col([\"cluster_label\", \"meta_cluster_label\", \"cluster\", \"cluster_id\"])\n",
    "if cluster_col is None:\n",
    "    # create a fallback column (all -1)\n",
    "    df[\"cluster_label\"] = \"-1\"\n",
    "    cluster_col = \"cluster_label\"\n",
    "    print(\"[WARN] cluster column not found - using fallback '-1' for all rows\")\n",
    "else:\n",
    "    # ensure string dtype\n",
    "    df[cluster_col] = df[cluster_col].astype(str)\n",
    "\n",
    "# novelty score column candidates\n",
    "nov_col = find_col([\"novelty_score\", \"novelty\", \"meta_novelty\", \"novel_score\"])\n",
    "if nov_col is None:\n",
    "    df[\"novelty_score\"] = 0.0\n",
    "    nov_col = \"novelty_score\"\n",
    "    print(\"[WARN] novelty column not found - defaulting to 0.0\")\n",
    "else:\n",
    "    # coerce to numeric\n",
    "    df[nov_col] = pd.to_numeric(df[nov_col], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "# priority / mutual-info / mean_conf candidates\n",
    "priority_col = find_col([\"priority_score\", \"priority\"])\n",
    "mi_col = find_col([\"species_mutual_info_mc\", \"species_mutual_info\", \"mutual_info\", \"mi\"])\n",
    "conf_col = find_col([\"species_mean_conf_mc\", \"species_mean_conf\", \"mean_confidence\", \"species_mean_conf\"])\n",
    "\n",
    "# if priority doesn't exist but mi & novelty exist, compute a fallback priority\n",
    "if priority_col is None:\n",
    "    if mi_col is not None:\n",
    "        df[\"priority_score\"] = (0.5 * df[nov_col].astype(float) + 0.5 * pd.to_numeric(df[mi_col], errors=\"coerce\").fillna(0.0))\n",
    "    else:\n",
    "        df[\"priority_score\"] = pd.to_numeric(df[nov_col], errors=\"coerce\").fillna(0.0)\n",
    "    priority_col = \"priority_score\"\n",
    "    print(\"[INFO] priority_score not found - computed fallback from novelty and mutual-info where possible\")\n",
    "\n",
    "# species label column detection for consensus fraction\n",
    "species_label_col = find_col([\"species_pred_label_mc\", \"species_pred_label\", \"species_pred\", \"species_pred_label\"])\n",
    "if species_label_col is None:\n",
    "    # fallback: attempt genus or species columns that exist\n",
    "    species_label_col = find_col([\"genus_pred_label_mc\", \"genus_pred_label\", \"genus_pred\"])\n",
    "    if species_label_col is None:\n",
    "        # create a column of NaNs so aggregation can run\n",
    "        df[\"species_pred_label_mc\"] = pd.NA\n",
    "        species_label_col = \"species_pred_label_mc\"\n",
    "        print(\"[WARN] species label column not found - creating empty column for consensus frac\")\n",
    "\n",
    "# ensure numeric columns are numeric\n",
    "df[nov_col] = pd.to_numeric(df[nov_col], errors=\"coerce\").fillna(0.0)\n",
    "df[priority_col] = pd.to_numeric(df[priority_col], errors=\"coerce\").fillna(0.0)\n",
    "# mutual-info & mean_conf (optional)\n",
    "if mi_col:\n",
    "    df[mi_col] = pd.to_numeric(df[mi_col], errors=\"coerce\").fillna(0.0)\n",
    "if conf_col:\n",
    "    df[conf_col] = pd.to_numeric(df[conf_col], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "# ---------------- Groupby aggregation (safe named tuples) ----------------\n",
    "# We'll use safe column names for aggregation targets; if a column is missing pandas will fill NaN\n",
    "agg_map = {\n",
    "    \"cluster_n\": (\"__row_index\", \"count\"),\n",
    "    \"cluster_mean_novelty\": (nov_col, \"mean\"),\n",
    "    \"cluster_mean_priority\": (priority_col, \"mean\"),\n",
    "}\n",
    "\n",
    "# add mutual-info and mean_conf if present\n",
    "if mi_col:\n",
    "    agg_map[\"cluster_mean_species_mi\"] = (mi_col, \"mean\")\n",
    "else:\n",
    "    df[\"cluster_mean_species_mi\"] = 0.0\n",
    "    agg_map[\"cluster_mean_species_mi\"] = (\"cluster_mean_species_mi\", \"mean\")\n",
    "\n",
    "if conf_col:\n",
    "    agg_map[\"cluster_mean_species_conf\"] = (conf_col, \"mean\")\n",
    "else:\n",
    "    df[\"cluster_mean_species_conf\"] = 0.0\n",
    "    agg_map[\"cluster_mean_species_conf\"] = (\"cluster_mean_species_conf\", \"mean\")\n",
    "\n",
    "# consensus fraction computed from species label column using tuple (col, function)\n",
    "def consensus_frac_series(s):\n",
    "    # s may be object dtype strings; consider non-empty / notna as consensus presence\n",
    "    try:\n",
    "        return float(s.notna().sum()) / float(len(s)) if len(s) > 0 else 0.0\n",
    "    except Exception:\n",
    "        # fallback simple\n",
    "        return float(np.count_nonzero(~pd.isna(s))) / float(len(s)) if len(s) > 0 else 0.0\n",
    "\n",
    "# If pandas version supports lambda inside named aggregation tuple, use it; otherwise compute separately\n",
    "try:\n",
    "    grp = df.groupby(cluster_col).agg(\n",
    "        **agg_map,\n",
    "        cluster_species_consensus_frac = (species_label_col, lambda s: float(s.notna().sum())/float(len(s)) if len(s)>0 else 0.0)\n",
    "    ).reset_index()\n",
    "except Exception as e:\n",
    "    # fallback: compute groupby agg with agg_map, then compute consensus separately\n",
    "    print(\"[FALLBACK] Named-lambda aggregation not supported in this pandas version:\", e)\n",
    "    grp = df.groupby(cluster_col).agg(**agg_map).reset_index()\n",
    "    # compute consensus separately:\n",
    "    cons = df.groupby(cluster_col)[species_label_col].apply(lambda s: float(s.notna().sum())/float(len(s)) if len(s)>0 else 0.0).reset_index(name=\"cluster_species_consensus_frac\")\n",
    "    grp = grp.merge(cons, how=\"left\", left_on=cluster_col, right_on=cluster_col)\n",
    "\n",
    "# ---------------- Post-process & ranking ----------------\n",
    "# clean cluster label column name in grp if necessary\n",
    "if cluster_col != \"cluster_label\":\n",
    "    # ensure we still have a cluster column in output named 'cluster_label'\n",
    "    grp = grp.rename(columns={cluster_col: \"cluster_label\"})\n",
    "\n",
    "# fill NaNs\n",
    "for c in [\"cluster_mean_novelty\",\"cluster_mean_priority\",\"cluster_mean_species_mi\",\"cluster_mean_species_conf\",\"cluster_species_consensus_frac\"]:\n",
    "    if c in grp.columns:\n",
    "        grp[c] = pd.to_numeric(grp[c], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "# compute priority_score if not present (use cluster_mean_priority if exists)\n",
    "if \"cluster_mean_priority\" not in grp.columns or grp[\"cluster_mean_priority\"].isnull().all():\n",
    "    grp[\"cluster_mean_priority\"] = grp[\"cluster_mean_novelty\"] * 0.6 + grp[\"cluster_mean_species_mi\"] * 0.4\n",
    "\n",
    "# add additional derived columns for sorting\n",
    "grp[\"priority_rank\"] = grp[\"cluster_mean_priority\"].rank(method=\"first\", ascending=False).astype(int)\n",
    "grp = grp.sort_values([\"cluster_mean_priority\",\"cluster_mean_novelty\"], ascending=[False, False]).reset_index(drop=True)\n",
    "\n",
    "# ---------------- Save outputs ----------------\n",
    "grp.to_csv(OUT_NOVEL, index=False)\n",
    "print(f\"[SAVE] novel candidate cluster priority -> {OUT_NOVEL.name}  (clusters={len(grp)})\")\n",
    "\n",
    "# ---------------- Print top-10 cluster-level candidates & top-10 sequence-level for review ----------------\n",
    "print(\"\\nTop-10 clusters by cluster_mean_priority:\")\n",
    "top_clusters = grp.head(10)\n",
    "print(top_clusters[[\"cluster_label\",\"cluster_n\",\"cluster_mean_priority\",\"cluster_mean_novelty\",\"cluster_species_consensus_frac\"]].to_string(index=False))\n",
    "\n",
    "# Also show top-10 sequence-level items (already prioritized by priority_score)\n",
    "# ensure priority_score column exists in df\n",
    "if \"priority_score\" not in df.columns:\n",
    "    df[\"priority_score\"] = pd.to_numeric(df[priority_col], errors=\"coerce\").fillna(0.0)\n",
    "top_seq = df.sort_values(\"priority_score\", ascending=False).head(10)\n",
    "print(\"\\nTop-10 sequence-level priority (idx, id, cluster, priority, novelty, species_mi, species_conf):\")\n",
    "for _, row in top_seq.iterrows():\n",
    "    seqid = row.get(\"sequence_id\", \"\") or row.get(\"id\",\"\") or \"\"\n",
    "    print(f\" idx={int(row['__row_index'])} id={str(seqid)[:40]:40s} cluster={str(row.get(cluster_col,'-1')):6s} pri={float(row.get('priority_score',0.0)):.4f} nov={float(row.get(nov_col,0.0)):.4f} mi={float(row.get(mi_col or '0',0.0)):.4f} conf={float(row.get(conf_col or '0',0.0)):.4f}\")\n",
    "\n",
    "print(\"\\nCell complete. Files written: \", OUT_NOVEL.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6c5ee553-05a6-4f20-91ba-731c98606de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell D — Production inference wrapper: infer_fasta(fasta_path, out_prefix, ...)\n",
    "# Outputs:\n",
    "#  - <out_prefix>_predictions.jsonl\n",
    "#  - <out_prefix>_predictions_summary.csv\n",
    "#  - optionally: <out_prefix>_mc.jsonl / _mc.csv with MC metrics\n",
    "# Requirements: gensim, torch, sklearn, numpy, pandas\n",
    "# Usage example (run after cell): infer_fasta(\"new_seqs.fasta\", out_prefix=\"ncbi_blast_db/extracted/infer_new\", n_mc=30)\n",
    "\n",
    "import os, json, time, math, pickle\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.models import Word2Vec\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "DOWNLOAD_DIR = Path(globals().get(\"DOWNLOAD_DIR\", \"./ncbi_blast_db\"))\n",
    "EXTRACT_DIR = DOWNLOAD_DIR / \"extracted\"\n",
    "W2V_PATH_CANDIDATES = [EXTRACT_DIR / \"kmer_w2v_k6.model\", EXTRACT_DIR / \"kmer_w2v.model\", DOWNLOAD_DIR / \"kmer_w2v_k6.model\"]\n",
    "PCA_MODEL_PATH = EXTRACT_DIR / \"pca_model.pkl\"\n",
    "EMB_FULL = EXTRACT_DIR / \"embeddings.npy\"   # used to fit PCA if missing\n",
    "CHECKPOINT_GLOBS = [EXTRACT_DIR / \"best_shared_heads_retrain.pt\",\n",
    "                    EXTRACT_DIR / \"best_shared_heads_labeled.pt\",\n",
    "                    EXTRACT_DIR / \"best_shared_heads_pt.pt\",\n",
    "                    EXTRACT_DIR / \"best_shared_heads_pseudo_tensordataset_fix.pt\",\n",
    "                    EXTRACT_DIR / \"best_shared_heads.pt\",\n",
    "                    EXTRACT_DIR / \"best_shared_heads_retrain.pt\"]\n",
    "LABEL_ENCODER_CANDIDATES = [EXTRACT_DIR / \"label_encoders_rebuilt_v2.pkl\",\n",
    "                            EXTRACT_DIR / \"label_encoders_used.pkl\",\n",
    "                            EXTRACT_DIR / \"label_encoders_v2.pkl\",\n",
    "                            EXTRACT_DIR / \"label_encoders.pkl\"]\n",
    "DEFAULT_KMER = 6\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "HIDDEN_DIM = 256\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def find_file(cands):\n",
    "    for p in cands:\n",
    "        if p is None:\n",
    "            continue\n",
    "        if Path(p).exists():\n",
    "            return Path(p)\n",
    "    return None\n",
    "\n",
    "def parse_fasta(path):\n",
    "    \"\"\"Simple fasta parser yielding (header, seq)\"\"\"\n",
    "    header = None\n",
    "    seq_parts = []\n",
    "    with open(path, \"r\", encoding=\"utf8\", errors=\"ignore\") as fh:\n",
    "        for line in fh:\n",
    "            line = line.rstrip(\"\\n\\r\")\n",
    "            if not line:\n",
    "                continue\n",
    "            if line[0] == \">\":\n",
    "                if header is not None:\n",
    "                    yield header, \"\".join(seq_parts)\n",
    "                header = line[1:].strip()\n",
    "                seq_parts = []\n",
    "            else:\n",
    "                seq_parts.append(line.strip())\n",
    "        if header is not None:\n",
    "            yield header, \"\".join(seq_parts)\n",
    "\n",
    "def kmers_from_seq(seq, k=6):\n",
    "    s = seq.upper()\n",
    "    n = len(s)\n",
    "    if n < k:\n",
    "        return []\n",
    "    return [s[i:i+k] for i in range(0, n - k + 1)]\n",
    "\n",
    "def load_word2vec(path_candidates):\n",
    "    p = find_file(path_candidates)\n",
    "    if p is None:\n",
    "        raise FileNotFoundError(\"Word2Vec k-mer model not found. Looked in: \" + \", \".join(str(x) for x in path_candidates))\n",
    "    print(\"[LOAD] Word2Vec:\", p.name)\n",
    "    return Word2Vec.load(str(p))\n",
    "\n",
    "def infer_input_dim_from_ckpt(ck):\n",
    "    \"\"\"Robustly inspect checkpoint dict or state_dict to find expected input dim (second dim of the first linear weight).\"\"\"\n",
    "    # ck may be dict or state_dict\n",
    "    cand = None\n",
    "    if isinstance(ck, dict):\n",
    "        # check 'model_state' or 'state_dict'\n",
    "        for key in (\"model_state\", \"state_dict\"):\n",
    "            if key in ck and isinstance(ck[key], dict):\n",
    "                sd = ck[key]\n",
    "                for k, v in sd.items():\n",
    "                    if k.endswith(\".weight\"):\n",
    "                        arr = v.numpy() if isinstance(v, torch.Tensor) else np.array(v)\n",
    "                        if arr.ndim == 2 and arr.shape[1] < 5000:\n",
    "                            return int(arr.shape[1])\n",
    "        # check shared_state pattern\n",
    "        if \"shared_state\" in ck and isinstance(ck[\"shared_state\"], dict):\n",
    "            for k, v in ck[\"shared_state\"].items():\n",
    "                arr = v.numpy() if isinstance(v, torch.Tensor) else np.array(v)\n",
    "                if arr.ndim == 2:\n",
    "                    return int(arr.shape[1])\n",
    "    # if ck itself is a state_dict-like\n",
    "    if isinstance(ck, dict):\n",
    "        for k, v in ck.items():\n",
    "            if k.endswith(\".weight\"):\n",
    "                arr = v.numpy() if isinstance(v, torch.Tensor) else np.array(v)\n",
    "                if arr.ndim == 2 and arr.shape[1] < 5000:\n",
    "                    return int(arr.shape[1])\n",
    "    return None\n",
    "\n",
    "def build_model(input_dim, label_encoders, hidden_dim=HIDDEN_DIM):\n",
    "    model = nn.Module()\n",
    "    model.shared = nn.Sequential(\n",
    "        nn.Linear(input_dim, hidden_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "    heads = {}\n",
    "    for r in label_encoders.keys():\n",
    "        ncls = len(label_encoders[r].classes_)\n",
    "        heads[r] = nn.Linear(hidden_dim // 2, ncls)\n",
    "    model.heads = nn.ModuleDict(heads)\n",
    "    # attach forward\n",
    "    def _forward(self, x):\n",
    "        h = self.shared(x)\n",
    "        return {r: self.heads[r](h) for r in self.heads}\n",
    "    import types\n",
    "    model.forward = types.MethodType(_forward, model)\n",
    "    return model\n",
    "\n",
    "def load_label_encoders(candidates):\n",
    "    p = find_file(candidates)\n",
    "    if p is None:\n",
    "        raise FileNotFoundError(\"label_encoders pickle not found; looked in: \" + \", \".join(str(x) for x in candidates))\n",
    "    with open(p, \"rb\") as fh:\n",
    "        enc = pickle.load(fh)\n",
    "    print(\"[LOAD] label_encoders:\", p.name)\n",
    "    return enc\n",
    "\n",
    "def safe_load_ckpt(candidates):\n",
    "    p = find_file(candidates)\n",
    "    if p is None:\n",
    "        raise FileNotFoundError(\"Checkpoint not found; looked in: \" + \", \".join(str(x) for x in candidates))\n",
    "    ck = torch.load(p, map_location=\"cpu\")\n",
    "    print(\"[LOAD] checkpoint:\", p.name)\n",
    "    return ck\n",
    "\n",
    "# ---------- main inference function ----------\n",
    "def infer_fasta(fasta_path, out_prefix=None, kmer=DEFAULT_KMER, n_mc=0, batch_size=256, apply_temp_scaling=False, temps=None):\n",
    "    \"\"\"\n",
    "    fasta_path: path to fasta file\n",
    "    out_prefix: prefix for output files (if None -> <fasta stem> in extracted/)\n",
    "    n_mc: number of MC-dropout forward passes (0 = deterministic only)\n",
    "    apply_temp_scaling: if True and temps dict provided mapping rank->T, will divide logits by T\n",
    "    temps: dict rank->temperature, optional\n",
    "    \"\"\"\n",
    "    tstart = time.time()\n",
    "    fasta_path = Path(fasta_path)\n",
    "    if not fasta_path.exists():\n",
    "        raise FileNotFoundError(f\"FASTA not found: {fasta_path}\")\n",
    "    out_prefix = Path(out_prefix) if out_prefix else (EXTRACT_DIR / (fasta_path.stem + \"_infer\"))\n",
    "    out_prefix = out_prefix.with_suffix(\"\")  # remove extension\n",
    "    out_jsonl = out_prefix.name + \"_predictions.jsonl\"\n",
    "    out_csv = out_prefix.name + \"_predictions_summary.csv\"\n",
    "    out_jsonl = out_prefix.with_suffix(\"\").with_name(out_prefix.name + \"_predictions.jsonl\")\n",
    "    out_csv = out_prefix.with_suffix(\"\").with_name(out_prefix.name + \"_predictions_summary.csv\")\n",
    "    out_jsonl = Path(out_jsonl)\n",
    "    out_csv = Path(out_csv)\n",
    "    print(f\"[INFER] fasta={fasta_path.name} -> outputs: {out_jsonl.name}, {out_csv.name}\")\n",
    "\n",
    "    # load resources\n",
    "    w2v = load_word2vec(W2V_PATH_CANDIDATES)\n",
    "    enc = load_label_encoders(LABEL_ENCODER_CANDIDATES)\n",
    "    ck = safe_load_ckpt(CHECKPOINT_GLOBS)\n",
    "\n",
    "    # determine input_dim expected by model\n",
    "    input_dim = infer_input_dim_from_ckpt(ck)\n",
    "    if input_dim is None:\n",
    "        # fallback to Word2Vec vector_size\n",
    "        input_dim = getattr(w2v.wv, \"vector_size\", None) or getattr(w2v, \"vector_size\", 128)\n",
    "    print(f\"[INFO] chosen input_dim={input_dim}\")\n",
    "\n",
    "    # if input_dim < w2v.vec_size, ensure PCA available or fit\n",
    "    w2v_dim = w2v.wv.vector_size\n",
    "    pca_model = None\n",
    "    if input_dim != w2v_dim:\n",
    "        # try load saved PCA\n",
    "        if PCA_MODEL_PATH.exists():\n",
    "            with open(PCA_MODEL_PATH, \"rb\") as fh:\n",
    "                pca_model = pickle.load(fh)\n",
    "            print(\"[LOAD] PCA model from\", PCA_MODEL_PATH.name)\n",
    "        else:\n",
    "            # fit PCA on saved full embeddings.npy (if present)\n",
    "            if EMB_FULL.exists():\n",
    "                emb = np.load(EMB_FULL)\n",
    "                print(f\"[FIT] Fitting PCA to reduce {emb.shape[1]} -> {input_dim} using saved embeddings.npy (n={emb.shape[0]})\")\n",
    "                pca = PCA(n_components=input_dim, random_state=42)\n",
    "                pca.fit(emb)\n",
    "                pca_model = pca\n",
    "                with open(PCA_MODEL_PATH, \"wb\") as fh:\n",
    "                    pickle.dump(pca_model, fh)\n",
    "                print(\"[SAVE] PCA model written to\", PCA_MODEL_PATH.name)\n",
    "            else:\n",
    "                raise RuntimeError(\"Model expects input_dim != w2v vector size, but no PCA model and no embeddings.npy to fit PCA.\")\n",
    "    else:\n",
    "        print(\"[INFO] No PCA required (model input matches Word2Vec dim).\")\n",
    "\n",
    "    # build model with the input_dim and load weights\n",
    "    model = build_model(input_dim, enc, hidden_dim=HIDDEN_DIM)\n",
    "    # load checkpoint state into model (robust)\n",
    "    try:\n",
    "        if isinstance(ck, dict) and \"model_state\" in ck:\n",
    "            model.load_state_dict(ck[\"model_state\"], strict=False)\n",
    "        elif isinstance(ck, dict) and \"state_dict\" in ck:\n",
    "            model.load_state_dict(ck[\"state_dict\"], strict=False)\n",
    "        elif isinstance(ck, dict) and \"shared_state\" in ck and \"heads_state\" in ck:\n",
    "            # merge into model.state_dict\n",
    "            cur = model.state_dict()\n",
    "            for k,v in ck[\"shared_state\"].items():\n",
    "                tk = f\"shared.{k}\" if not k.startswith(\"shared.\") else k\n",
    "                if tk in cur:\n",
    "                    cur[tk] = v\n",
    "            for hname, hsd in ck[\"heads_state\"].items():\n",
    "                for subk, v in hsd.items():\n",
    "                    tk = f\"heads.{hname}.{subk}\"\n",
    "                    if tk in cur:\n",
    "                        cur[tk] = v\n",
    "            model.load_state_dict(cur, strict=False)\n",
    "        else:\n",
    "            # attempt to load ck itself as state_dict\n",
    "            model.load_state_dict(ck, strict=False)\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] checkpoint partial load:\", e)\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    # parse FASTA -> sequences list\n",
    "    seqs = []\n",
    "    headers = []\n",
    "    for h, s in parse_fasta(fasta_path):\n",
    "        headers.append(h)\n",
    "        seqs.append(s)\n",
    "    if len(seqs) == 0:\n",
    "        raise RuntimeError(\"No sequences parsed from FASTA.\")\n",
    "\n",
    "    # vectorize by averaging k-mer vectors\n",
    "    n = len(seqs)\n",
    "    seq_emb = np.zeros((n, w2v_dim), dtype=np.float32)\n",
    "    for i, s in enumerate(seqs):\n",
    "        kmer_list = kmers_from_seq(s, k=kmer)\n",
    "        vecs = []\n",
    "        for kmer_token in kmer_list:\n",
    "            if kmer_token in w2v.wv:\n",
    "                vecs.append(w2v.wv[kmer_token])\n",
    "        if len(vecs) == 0:\n",
    "            # fallback: try single characters? or zero-vector\n",
    "            seq_emb[i] = np.zeros((w2v_dim,), dtype=np.float32)\n",
    "        else:\n",
    "            seq_emb[i] = np.mean(vecs, axis=0)\n",
    "\n",
    "    # apply PCA if needed\n",
    "    if pca_model is not None:\n",
    "        seq_emb_in = pca_model.transform(seq_emb)\n",
    "    else:\n",
    "        seq_emb_in = seq_emb  # shape (n, input_dim)\n",
    "\n",
    "    # inference function (single pass)\n",
    "    def forward_batch(X_batch, temp_dict=None):\n",
    "        xb = torch.tensor(X_batch, dtype=torch.float32).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            logits_map = model(xb)\n",
    "        out_probs = {}\n",
    "        for r, logits in logits_map.items():\n",
    "            # apply temperature scaling if provided\n",
    "            l = logits\n",
    "            if (apply_temp_scaling and temp_dict and r in temp_dict and float(temp_dict[r]) > 0.0):\n",
    "                T = float(temp_dict[r])\n",
    "                l = l / T\n",
    "            probs = F.softmax(l, dim=1).cpu().numpy()\n",
    "            out_probs[r] = probs\n",
    "        return out_probs\n",
    "\n",
    "    # deterministic predictions\n",
    "    B = min(batch_size, max(1, n))\n",
    "    all_probs = {r: [] for r in enc.keys()}\n",
    "    for i in range(0, n, B):\n",
    "        batch = seq_emb_in[i:i+B]\n",
    "        probs_map = forward_batch(batch, temp_dict=temps if apply_temp_scaling else None)\n",
    "        for r, p in probs_map.items():\n",
    "            all_probs[r].append(p)\n",
    "    for r in list(all_probs.keys()):\n",
    "        all_probs[r] = np.vstack(all_probs[r]) if len(all_probs[r]) else np.zeros((n, len(enc[r].classes_)))\n",
    "\n",
    "    # MC-dropout if requested\n",
    "    mc_metrics = {}\n",
    "    if n_mc and n_mc > 0:\n",
    "        model.train()  # enable dropout\n",
    "        sum_probs = {r: np.zeros((n, len(enc[r].classes_)), dtype=np.float64) for r in enc.keys()}\n",
    "        sum_probs_sq = {r: np.zeros_like(sum_probs[r]) for r in enc.keys()}\n",
    "        # run passes\n",
    "        for t in range(n_mc):\n",
    "            for i in range(0, n, B):\n",
    "                batch = torch.tensor(seq_emb_in[i:i+B], dtype=torch.float32).to(DEVICE)\n",
    "                with torch.no_grad():\n",
    "                    logits_map = model(batch)\n",
    "                for r, logits in logits_map.items():\n",
    "                    probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "                    sum_probs[r][i:i+len(probs)] += probs\n",
    "                    sum_probs_sq[r][i:i+len(probs)] += (probs * probs)\n",
    "        # compute metrics\n",
    "        for r in enc.keys():\n",
    "            mean_probs = sum_probs[r] / float(n_mc)\n",
    "            mean_probs_sq = sum_probs_sq[r] / float(n_mc)\n",
    "            var_probs = np.clip(mean_probs_sq - (mean_probs ** 2), 0.0, None)\n",
    "            pred_entropy = -np.sum(mean_probs * np.log(mean_probs + 1e-12), axis=1)\n",
    "            exp_entropy = ( -np.sum((sum_probs[r]/float(n_mc)) * np.log((sum_probs[r]/float(n_mc)) + 1e-12), axis=1) ) # not quite E[H], but approximate; better below:\n",
    "            # recompute expected entropy properly: approximate by average of entropies; but we didn't store entropies per pass above; recompute by re-running passes? \n",
    "            # Instead compute expected entropy using mean of per-pass entropies:\n",
    "            # we can approximate expected_entropy = mean over passes of H(p_t) = (sum_probs_sq term used for var)\n",
    "            # We'll compute expected_entropy by reusing mean_probs and var approx:\n",
    "            # For reliability we compute mutual_info = pred_entropy - mean_of_entropy. We'll approximate mean_of_entropy by: sum_over_k (mean_p_k^2?) Not perfect but acceptable\n",
    "            # To avoid re-running passes (time), we approximate expected entropy via: expected_entropy ≈ -sum_k mean_p_k * log(mean_p_k) - sum_k var_k / (2 * mean_p_k + eps) ... this is messy.\n",
    "            # Practical compromise: compute predictive entropy (H[p_mean]) and use var of predicted-class as uncertainty metric.\n",
    "            pred_class_idx = np.argmax(mean_probs, axis=1)\n",
    "            mean_conf = np.max(mean_probs, axis=1)\n",
    "            # std of max confidence via var of max approximated by var_probs at predicted class\n",
    "            std_pred_class = np.sqrt(var_probs[np.arange(n), pred_class_idx])\n",
    "            mc_metrics[r] = {\n",
    "                \"mean_probs\": mean_probs,\n",
    "                \"predictive_entropy\": pred_entropy,\n",
    "                \"std_pred_class\": std_pred_class,\n",
    "                \"mean_conf\": mean_conf,\n",
    "                \"pred_class_idx\": pred_class_idx\n",
    "            }\n",
    "        model.eval()\n",
    "\n",
    "    # Build output records\n",
    "    records = []\n",
    "    summary_rows = []\n",
    "    for i in range(n):\n",
    "        rec = {}\n",
    "        rec[\"id\"] = headers[i]\n",
    "        rec[\"seq_len\"] = len(seqs[i])\n",
    "        rec[\"k\"] = kmer\n",
    "        # deterministic ranks\n",
    "        mean_conf_sum = 0.0\n",
    "        for r in enc.keys():\n",
    "            probs = all_probs[r][i]\n",
    "            pred_idx = int(np.argmax(probs))\n",
    "            pred_label = enc[r].classes_[pred_idx] if pred_idx < len(enc[r].classes_) else \"\"\n",
    "            conf = float(np.max(probs))\n",
    "            rec[f\"{r}_pred\"] = pred_label\n",
    "            rec[f\"{r}_pred_idx\"] = pred_idx\n",
    "            rec[f\"{r}_mean_conf\"] = conf\n",
    "            mean_conf_sum += conf\n",
    "            # MC metrics if present\n",
    "            if n_mc > 0 and r in mc_metrics:\n",
    "                rec[f\"{r}_mc_std_predprob\"] = float(mc_metrics[r][\"std_pred_class\"][i])\n",
    "                rec[f\"{r}_mc_pred_entropy\"] = float(mc_metrics[r][\"predictive_entropy\"][i])\n",
    "                rec[f\"{r}_mc_mean_conf\"] = float(mc_metrics[r][\"mean_conf\"][i])\n",
    "        # aggregated summary\n",
    "        rec[\"mean_conf_mean_ranks\"] = mean_conf_sum / max(1, len(enc.keys()))\n",
    "        records.append(rec)\n",
    "\n",
    "        # summary row small\n",
    "        srow = {\"id\": headers[i], \"seq_len\": len(seqs[i])}\n",
    "        for r in enc.keys():\n",
    "            srow[f\"{r}_pred\"] = rec.get(f\"{r}_pred\", \"\")\n",
    "            srow[f\"{r}_mean_conf\"] = rec.get(f\"{r}_mean_conf\", \"\")\n",
    "            if n_mc > 0:\n",
    "                srow[f\"{r}_mc_mean_conf\"] = rec.get(f\"{r}_mc_mean_conf\", \"\")\n",
    "                srow[f\"{r}_mc_pred_entropy\"] = rec.get(f\"{r}_mc_pred_entropy\", \"\")\n",
    "        summary_rows.append(srow)\n",
    "\n",
    "    # Save JSONL and CSV\n",
    "    with open(out_jsonl, \"w\", encoding=\"utf8\") as fh:\n",
    "        for r in records:\n",
    "            fh.write(json.dumps(r) + \"\\n\")\n",
    "    df_summary = pd.DataFrame(summary_rows)\n",
    "    df_summary.to_csv(out_csv, index=False)\n",
    "    elapsed = time.time() - tstart\n",
    "    print(f\"[DONE] wrote {out_jsonl.name} ({len(records)} rows) and {out_csv.name} in {elapsed:.1f}s\")\n",
    "    return out_jsonl, out_csv\n",
    "\n",
    "# ---------------- Example invocation (uncomment and edit FASTA path to run) ----------------\n",
    "# infer_fasta(\"path/to/your/new_sequences.fasta\",\n",
    "#             out_prefix=\"ncbi_blast_db/extracted/infer_new\",\n",
    "#             kmer=6, n_mc=30, batch_size=128, apply_temp_scaling=False, temps=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33f58ccd-6028-48c2-a964-e5a6b986a47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PWD: C:\\Users\\Srijit\\sih\n",
      "Checking expected dirs:\n",
      "  DOWNLOAD_DIR: C:\\Users\\Srijit\\sih\\ncbi_blast_db\n",
      "  EXTRACT_DIR : C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\n",
      "\n",
      "[OK] Found 6 FASTA files directly in ncbi_blast_db\\extracted:\n",
      "  - its_combined.fasta\n",
      "  - its_fetched.fasta\n",
      "  - lsu_combined.fasta\n",
      "  - lsu_fetched.fasta\n",
      "  - ssu_combined.fasta\n",
      "  - ssu_fetched.fasta\n",
      "\n",
      "[INFO] Found 3 tarball(s) (candidate sources):\n",
      "  - C:\\Users\\Srijit\\sih\\ncbi_blast_db\\ITS_eukaryote_sequences.tar.gz\n",
      "  - C:\\Users\\Srijit\\sih\\ncbi_blast_db\\LSU_eukaryote_rRNA.tar.gz\n",
      "  - C:\\Users\\Srijit\\sih\\ncbi_blast_db\\SSU_eukaryote_rRNA.tar.gz\n",
      "\n",
      "[INFO] Word2Vec file not found at expected location: ncbi_blast_db\\extracted\\kmer_w2v_k6.model\n",
      "\n",
      "============================================================\n",
      "NEXT STEPS (choose what applies):\n",
      "\n",
      "A) FASTAs are now present in extracted/. Run your Word2Vec training cell to create:\n",
      "   ncbi_blast_db/extracted/kmer_w2v_k6.model\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Robust locator + safe extractor for real FASTA/tarballs (improved, streaming, progress)\n",
    "import os, sys, tarfile, gzip, io, shutil, time\n",
    "from pathlib import Path\n",
    "import textwrap\n",
    "\n",
    "DOWNLOAD_DIR = Path(\"./ncbi_blast_db\")\n",
    "EXTRACT_DIR = DOWNLOAD_DIR / \"extracted\"\n",
    "\n",
    "# --- Helpers ---------------------------------------------------------------\n",
    "def find_fastas(base_dir):\n",
    "    exts = [\".fasta\", \".fa\", \".fna\", \".fasta.gz\", \".fa.gz\", \".fna.gz\"]\n",
    "    found = []\n",
    "    if base_dir.exists():\n",
    "        for e in exts:\n",
    "            found.extend(list(base_dir.glob(f\"*{e}\")))     # non-recursive\n",
    "            found.extend(list(base_dir.rglob(f\"*{e}\")))    # recursive\n",
    "    found = sorted({p.resolve() for p in found})\n",
    "    return found\n",
    "\n",
    "def find_tarballs(base_dir):\n",
    "    if not base_dir.exists():\n",
    "        return []\n",
    "    patterns = [\"*.tar.gz\", \"*.tgz\", \"*.tar\"]\n",
    "    found = []\n",
    "    for p in patterns:\n",
    "        found.extend(list(base_dir.glob(p)))\n",
    "        found.extend(list(base_dir.rglob(p)))\n",
    "    found = sorted({p.resolve() for p in found})\n",
    "    return found\n",
    "\n",
    "def safe_name_for_out(tar_path, member_name, prefix_with_tar=True):\n",
    "    # use basename, but prefix with tar name (without ext) to avoid collisions\n",
    "    base = Path(member_name).name\n",
    "    if prefix_with_tar:\n",
    "        tarstem = Path(tar_path).stem\n",
    "        # if tar was .tar.gz the stem is e.g. \"file.tar\" so remove extra .tar\n",
    "        if tarstem.endswith(\".tar\"):\n",
    "            tarstem = tarstem[:-4]\n",
    "        return f\"{tarstem}__{base}\"\n",
    "    return base\n",
    "\n",
    "def stream_decompress_gz_member(member_fileobj, out_path):\n",
    "    # member_fileobj is a file-like object (from tar.extractfile)\n",
    "    # open gzip.GzipFile on top and stream-copy to disk\n",
    "    try:\n",
    "        with gzip.GzipFile(fileobj=member_fileobj) as gz:\n",
    "            with open(out_path, \"wb\") as w:\n",
    "                shutil.copyfileobj(gz, w)\n",
    "        return True\n",
    "    except (OSError, EOFError):\n",
    "        # not a gzip stream or decompression failed\n",
    "        return False\n",
    "\n",
    "def safe_extract_fastas_from_tar(tar_path, out_dir, verbose=True):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    count = 0\n",
    "    start = time.time()\n",
    "    try:\n",
    "        # streaming mode avoids building the full member list\n",
    "        with tarfile.open(tar_path, \"r|*\") as tf:\n",
    "            if verbose:\n",
    "                print(f\"Processing tar (stream mode): {tar_path.name}\")\n",
    "            for m in tf:\n",
    "                # skip if not regular file\n",
    "                if not m.isfile():\n",
    "                    continue\n",
    "                name = Path(m.name).name\n",
    "                if not name:\n",
    "                    continue\n",
    "                lname = name.lower()\n",
    "                if not (lname.endswith(\".fasta\") or lname.endswith(\".fa\") or lname.endswith(\".fna\")\n",
    "                        or lname.endswith(\".fasta.gz\") or lname.endswith(\".fa.gz\") or lname.endswith(\".fna.gz\")):\n",
    "                    # skip non-FASTA-like members\n",
    "                    continue\n",
    "\n",
    "                outname = safe_name_for_out(tar_path, name, prefix_with_tar=True)\n",
    "                out_path = out_dir / outname\n",
    "                # open member fileobj and stream to disk (decompress if gz)\n",
    "                try:\n",
    "                    fobj = tf.extractfile(m)\n",
    "                    if fobj is None:\n",
    "                        print(f\"  [WARN] could not open member stream: {m.name}\")\n",
    "                        continue\n",
    "\n",
    "                    # If name endswith .gz, attempt streaming gzip decompression\n",
    "                    if lname.endswith(\".gz\"):\n",
    "                        ok = stream_decompress_gz_member(fobj, out_path.with_suffix(''))  # remove .gz in output\n",
    "                        if ok:\n",
    "                            print(f\"  extracted (decompressed) -> {out_path.with_suffix('').name}\", flush=True)\n",
    "                        else:\n",
    "                            # fallback: write raw bytes without loading all at once\n",
    "                            with open(out_path, \"wb\") as w:\n",
    "                                shutil.copyfileobj(fobj, w)\n",
    "                            print(f\"  extracted (saved .gz raw) -> {out_path.name} (decompress failed)\", flush=True)\n",
    "                    else:\n",
    "                        # stream copy plain member to disk\n",
    "                        with open(out_path, \"wb\") as w:\n",
    "                            shutil.copyfileobj(fobj, w)\n",
    "                        print(f\"  extracted -> {out_path.name}\", flush=True)\n",
    "\n",
    "                    count += 1\n",
    "                except Exception as ex:\n",
    "                    print(f\"  [WARN] could not extract member {m.name}: {ex}\", flush=True)\n",
    "    except Exception as ex:\n",
    "        print(f\"[WARN] failed to open {tar_path.name}: {ex}\")\n",
    "    elapsed = time.time() - start\n",
    "    if elapsed > 0:\n",
    "        print(f\"  -> done {count} files from {tar_path.name} in {elapsed:.1f}s\")\n",
    "    return count\n",
    "\n",
    "# --- environment summary ---------------------------------------------------\n",
    "print(\"PWD:\", Path.cwd())\n",
    "print(\"Checking expected dirs:\")\n",
    "print(\"  DOWNLOAD_DIR:\", DOWNLOAD_DIR.resolve())\n",
    "print(\"  EXTRACT_DIR :\", EXTRACT_DIR.resolve())\n",
    "EXTRACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1) look for FASTAs in extracted/\n",
    "fastas_in_extracted = find_fastas(EXTRACT_DIR)\n",
    "if fastas_in_extracted:\n",
    "    print(f\"\\n[OK] Found {len(fastas_in_extracted)} FASTA files directly in {EXTRACT_DIR}:\")\n",
    "    for p in fastas_in_extracted[:50]:\n",
    "        print(\"  -\", p.name)\n",
    "else:\n",
    "    print(f\"\\n[INFO] No FASTA files found in {EXTRACT_DIR}.\")\n",
    "\n",
    "# 2) find tarballs (only limited, avoid scanning entire home recursively if you don't want it)\n",
    "tar_candidates = []\n",
    "tar_candidates += find_tarballs(DOWNLOAD_DIR)\n",
    "tar_candidates += find_tarballs(Path.cwd())\n",
    "# (optionally) include home if you explicitly want to scan it - can be slow on large home dirs\n",
    "# tar_candidates += find_tarballs(Path.home())\n",
    "\n",
    "tar_candidates = sorted({p.resolve() for p in tar_candidates})\n",
    "if tar_candidates:\n",
    "    print(f\"\\n[INFO] Found {len(tar_candidates)} tarball(s) (candidate sources):\")\n",
    "    for t in tar_candidates[:20]:\n",
    "        print(\"  -\", t)\n",
    "else:\n",
    "    print(\"\\n[INFO] No tarballs (*.tar.gz, *.tgz, *.tar) found in ncbi_blast_db/ or cwd (home scan disabled).\")\n",
    "\n",
    "# 3) If FASTAs not present but tarballs found, attempt extraction of FASTA members into EXTRACT_DIR\n",
    "if (not fastas_in_extracted) and tar_candidates:\n",
    "    print(\"\\n[STEP] Attempting to extract FASTA-like members from the tarball(s) into\", EXTRACT_DIR)\n",
    "    total = 0\n",
    "    for t in tar_candidates:\n",
    "        extracted = safe_extract_fastas_from_tar(t, EXTRACT_DIR)\n",
    "        total += extracted\n",
    "    if total > 0:\n",
    "        print(f\"[OK] Extracted {total} FASTA-like files into {EXTRACT_DIR}. Re-checking files now...\")\n",
    "        fastas_in_extracted = find_fastas(EXTRACT_DIR)\n",
    "        print(f\"[OK] Now found {len(fastas_in_extracted)} FASTA files.\")\n",
    "        for p in fastas_in_extracted[:50]:\n",
    "            print(\"  -\", p.name)\n",
    "    else:\n",
    "        print(\"[WARN] No FASTA-like members were extracted from the tarballs found. Inspect the tarballs with 'tar -tf <tarfile> | head' to see contents.\")\n",
    "\n",
    "# 4) look for word2vec path (same as before)\n",
    "w2v_path = EXTRACT_DIR / \"kmer_w2v_k6.model\"\n",
    "if w2v_path.exists():\n",
    "    print(f\"\\n[OK] Found k-mer Word2Vec model: {w2v_path}\")\n",
    "else:\n",
    "    print(f\"\\n[INFO] Word2Vec file not found at expected location: {w2v_path}\")\n",
    "\n",
    "# 5) nearby fastas if none in extracted\n",
    "if not fastas_in_extracted:\n",
    "    nearby = []\n",
    "    for d in [Path.cwd(), Path.cwd().parent, Path.cwd().parent.parent]:\n",
    "        nearby.extend(find_fastas(d))\n",
    "    nearby = sorted({p.resolve() for p in nearby})\n",
    "    if nearby:\n",
    "        print(f\"\\n[FOUND] FASTA files in nearby locations (not in extracted/). Consider moving them to {EXTRACT_DIR}:\")\n",
    "        for p in nearby[:50]:\n",
    "            print(\"  -\", p)\n",
    "    else:\n",
    "        print(\"\\n[INFO] No FASTA files found nearby either.\")\n",
    "\n",
    "# 6) next steps text\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NEXT STEPS (choose what applies):\\n\")\n",
    "if fastas_in_extracted:\n",
    "    print(\"A) FASTAs are now present in extracted/. Run your Word2Vec training cell to create:\")\n",
    "    print(\"   ncbi_blast_db/extracted/kmer_w2v_k6.model\")\n",
    "else:\n",
    "    print(\"A) No FASTAs detected. If you have the original tar.gz files, place them into:\")\n",
    "    print(f\"   {DOWNLOAD_DIR.resolve()}\")\n",
    "    print(\"   Then re-run this cell. To inspect tar contents before extraction:\")\n",
    "    print(\"   tar -tf <tarfile> | head -n 40\")\n",
    "print(\"\\n\" + \"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf203a14-70d3-43e0-8334-b3b91e908f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PWD: C:\\Users\\Srijit\\sih\n",
      "Listing C:\\Users\\Srijit\\sih\\ncbi_blast_db:\n",
      "  blastdb_raw - 0 bytes\n",
      "  extracted - 49152 bytes\n",
      "  ITS_eukaryote_sequences-nucl-metadata.json - 446 bytes\n",
      "  ITS_eukaryote_sequences.tar.gz - 74475777 bytes\n",
      "  kmer_w2v_k6.model - 5141323 bytes\n",
      "  LSU_eukaryote_rRNA-nucl-metadata.json - 465 bytes\n",
      "  LSU_eukaryote_rRNA.tar.gz - 59387878 bytes\n",
      "  SSU_eukaryote_rRNA-nucl-metadata.json - 465 bytes\n",
      "  SSU_eukaryote_rRNA.tar.gz - 59776957 bytes\n",
      "----------------------------------------\n",
      "\n",
      "Found 3 tarball(s):\n",
      " - SSU_eukaryote_rRNA.tar.gz\n",
      " - LSU_eukaryote_rRNA.tar.gz\n",
      " - ITS_eukaryote_sequences.tar.gz\n",
      "\n",
      "Tar: SSU_eukaryote_rRNA.tar.gz (59776957 bytes)\n",
      "  total members: 12\n",
      "     1. taxdb.btd\n",
      "     2. taxdb.bti\n",
      "     3. taxonomy4blast.sqlite3\n",
      "     4. SSU_eukaryote_rRNA.nin\n",
      "     5. SSU_eukaryote_rRNA.nhr\n",
      "     6. SSU_eukaryote_rRNA.nsq\n",
      "     7. SSU_eukaryote_rRNA.nog\n",
      "     8. SSU_eukaryote_rRNA.ndb\n",
      "     9. SSU_eukaryote_rRNA.nos\n",
      "    10. SSU_eukaryote_rRNA.not\n",
      "    11. SSU_eukaryote_rRNA.ntf\n",
      "    12. SSU_eukaryote_rRNA.nto\n",
      "  detected member extensions (sample counts):\n",
      "    .btd : 1\n",
      "    .bti : 1\n",
      "    .sqlite3 : 1\n",
      "    .nin : 1\n",
      "    .nhr : 1\n",
      "    .nsq : 1\n",
      "    .nog : 1\n",
      "    .ndb : 1\n",
      "    .nos : 1\n",
      "    .not : 1\n",
      "    .ntf : 1\n",
      "    .nto : 1\n",
      "\n",
      "Tar: LSU_eukaryote_rRNA.tar.gz (59387878 bytes)\n",
      "  total members: 12\n",
      "     1. taxdb.btd\n",
      "     2. taxdb.bti\n",
      "     3. taxonomy4blast.sqlite3\n",
      "     4. LSU_eukaryote_rRNA.nin\n",
      "     5. LSU_eukaryote_rRNA.nhr\n",
      "     6. LSU_eukaryote_rRNA.nsq\n",
      "     7. LSU_eukaryote_rRNA.nog\n",
      "     8. LSU_eukaryote_rRNA.ndb\n",
      "     9. LSU_eukaryote_rRNA.nos\n",
      "    10. LSU_eukaryote_rRNA.not\n",
      "    11. LSU_eukaryote_rRNA.ntf\n",
      "    12. LSU_eukaryote_rRNA.nto\n",
      "  detected member extensions (sample counts):\n",
      "    .btd : 1\n",
      "    .bti : 1\n",
      "    .sqlite3 : 1\n",
      "    .nin : 1\n",
      "    .nhr : 1\n",
      "    .nsq : 1\n",
      "    .nog : 1\n",
      "    .ndb : 1\n",
      "    .nos : 1\n",
      "    .not : 1\n",
      "    .ntf : 1\n",
      "    .nto : 1\n",
      "\n",
      "Tar: ITS_eukaryote_sequences.tar.gz (74475777 bytes)\n",
      "  total members: 12\n",
      "     1. ITS_eukaryote_sequences.ndb\n",
      "     2. ITS_eukaryote_sequences.nhr\n",
      "     3. ITS_eukaryote_sequences.nin\n",
      "     4. ITS_eukaryote_sequences.nog\n",
      "     5. ITS_eukaryote_sequences.nos\n",
      "     6. ITS_eukaryote_sequences.not\n",
      "     7. ITS_eukaryote_sequences.nsq\n",
      "     8. ITS_eukaryote_sequences.ntf\n",
      "     9. ITS_eukaryote_sequences.nto\n",
      "    10. taxdb.btd\n",
      "    11. taxdb.bti\n",
      "    12. taxonomy4blast.sqlite3\n",
      "  detected member extensions (sample counts):\n",
      "    .ndb : 1\n",
      "    .nhr : 1\n",
      "    .nin : 1\n",
      "    .nog : 1\n",
      "    .nos : 1\n",
      "    .not : 1\n",
      "    .nsq : 1\n",
      "    .ntf : 1\n",
      "    .nto : 1\n",
      "    .btd : 1\n",
      "    .bti : 1\n",
      "    .sqlite3 : 1\n",
      "\n",
      "Done. After you paste the result here, I will give the next cell tailored to what we find:\n",
      " - if FASTA members exist -> Word2Vec training cell\n",
      " - if BLAST DB binaries (.nsq/.nin/.nhr/.psq etc) -> blastdbcmd extraction cell\n",
      " - else -> efetch / alternative instructions\n"
     ]
    }
   ],
   "source": [
    "# Cell: Inspect ncbi_blast_db tarball contents safely (corrected - no syntax errors)\n",
    "import os, sys, tarfile, traceback\n",
    "from pathlib import Path\n",
    "\n",
    "DOWNLOAD_DIR = Path(\"./ncbi_blast_db\")\n",
    "expected_names = [\n",
    "    \"SSU_eukaryote_rRNA.tar.gz\",\n",
    "    \"LSU_eukaryote_rRNA.tar.gz\",\n",
    "    \"ITS_eukaryote_sequences.tar.gz\",\n",
    "]\n",
    "\n",
    "def list_dir(path):\n",
    "    if not path.exists():\n",
    "        print(f\"[MISSING] {path} does not exist.\")\n",
    "        return\n",
    "    print(f\"Listing {path.resolve()}:\")\n",
    "    for p in sorted(path.iterdir()):\n",
    "        try:\n",
    "            print(\" \", p.name, \"-\", p.stat().st_size, \"bytes\")\n",
    "        except Exception:\n",
    "            print(\" \", p.name)\n",
    "    print(\"-\"*40)\n",
    "\n",
    "def find_tarballs(base_dir):\n",
    "    tar_candidates = []\n",
    "    if not base_dir.exists():\n",
    "        return tar_candidates\n",
    "    # exact match (case insensitive)\n",
    "    for name in expected_names:\n",
    "        for p in base_dir.iterdir():\n",
    "            if p.is_file() and p.name.lower() == name.lower():\n",
    "                tar_candidates.append(p.resolve())\n",
    "    # fallback: glob\n",
    "    if not tar_candidates:\n",
    "        tar_candidates = sorted(list(base_dir.glob(\".tar.gz\")) + list(base_dir.glob(\".tgz\")) + list(base_dir.glob(\"*.tar\")))\n",
    "    # unique preserves order\n",
    "    seen = []\n",
    "    for p in tar_candidates:\n",
    "        if p not in seen:\n",
    "            seen.append(p)\n",
    "    return seen\n",
    "\n",
    "def show_tar_members(tar_path, max_show=200):\n",
    "    print(f\"\\nTar: {tar_path.name} ({tar_path.stat().st_size} bytes)\")\n",
    "    try:\n",
    "        with tarfile.open(tar_path, \"r:*\") as tf:\n",
    "            members = tf.getmembers()\n",
    "            print(f\"  total members: {len(members)}\")\n",
    "            exts = {}\n",
    "            for i, m in enumerate(members[:max_show]):\n",
    "                name = m.name\n",
    "                print(f\"   {i+1:3d}. {name}\")\n",
    "                _, ext = os.path.splitext(name.lower())\n",
    "                exts.setdefault(ext, 0)\n",
    "                exts[ext] += 1\n",
    "            if len(members) > max_show:\n",
    "                print(f\"   ... (showing first {max_show} members)\")\n",
    "            if exts:\n",
    "                print(\"  detected member extensions (sample counts):\")\n",
    "                for k,v in sorted(exts.items(), key=lambda x:-x[1])[:20]:\n",
    "                    print(f\"    {k or '(no ext)'} : {v}\")\n",
    "    except Exception as e:\n",
    "        print(\"  [ERROR] failed to read tar members:\", e)\n",
    "        traceback.print_exc()\n",
    "\n",
    "# main\n",
    "print(\"PWD:\", Path.cwd())\n",
    "list_dir(DOWNLOAD_DIR)\n",
    "tarballs = find_tarballs(DOWNLOAD_DIR)\n",
    "if not tarballs:\n",
    "    print(\"\\n[NO TARBALLS] No tarballs found in\", DOWNLOAD_DIR.resolve())\n",
    "    print(\"If the three tar.gz files are present, ensure they are in ncbi_blast_db/ and named correctly.\")\n",
    "else:\n",
    "    print(f\"\\nFound {len(tarballs)} tarball(s):\")\n",
    "    for t in tarballs:\n",
    "        print(\" -\", t.name)\n",
    "    # list members for each\n",
    "    for t in tarballs:\n",
    "        show_tar_members(t, max_show=200)\n",
    "\n",
    "print(\"\\nDone. After you paste the result here, I will give the next cell tailored to what we find:\")\n",
    "print(\" - if FASTA members exist -> Word2Vec training cell\")\n",
    "print(\" - if BLAST DB binaries (.nsq/.nin/.nhr/.psq etc) -> blastdbcmd extraction cell\")\n",
    "print(\" - else -> efetch / alternative instructions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efedf67-0548-4082-817c-d099dee48c9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell: FIXED — detect DB prefixes correctly and (if available) run blastdbcmd to dump FASTA\n",
    "import os, traceback, subprocess\n",
    "from pathlib import Path\n",
    "from shutil import which\n",
    "\n",
    "DOWNLOAD_DIR = Path(\"./ncbi_blast_db\")\n",
    "RAW_DB_DIR = DOWNLOAD_DIR / \"blastdb_raw\"\n",
    "EXTRACT_DIR = DOWNLOAD_DIR / \"extracted\"\n",
    "RAW_DB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EXTRACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Working dirs:\")\n",
    "print(\" DOWNLOAD_DIR:\", DOWNLOAD_DIR.resolve())\n",
    "print(\" RAW_DB_DIR  :\", RAW_DB_DIR.resolve())\n",
    "print(\" EXTRACT_DIR :\", EXTRACT_DIR.resolve())\n",
    "print()\n",
    "\n",
    "# list raw db files\n",
    "raw_files = sorted([p.name for p in RAW_DB_DIR.iterdir() if p.is_file()]) if RAW_DB_DIR.exists() else []\n",
    "print(\"Raw DB files found:\", len(raw_files))\n",
    "for fn in raw_files[:200]:\n",
    "    print(\" \", fn)\n",
    "print()\n",
    "\n",
    "# group files by prefix (prefix = name before first dot)\n",
    "prefix_map = {}\n",
    "for fn in raw_files:\n",
    "    pref = fn.split(\".\", 1)[0]\n",
    "    prefix_map.setdefault(pref, []).append(fn)\n",
    "\n",
    "if not prefix_map:\n",
    "    print(\"[ERROR] No raw DB files found in\", RAW_DB_DIR)\n",
    "    print(\"If you previously extracted tarballs, ensure those extracted files are in:\", RAW_DB_DIR)\n",
    "    raise RuntimeError(\"No BLAST DB raw files found. Re-run extraction cell or place files in blastdb_raw/\")\n",
    "\n",
    "# display candidate prefixes & sample members\n",
    "print(\"Detected file prefixes (sample members shown):\")\n",
    "for pref, members in prefix_map.items():\n",
    "    print(\" -\", pref, \"| members:\", len(members), \"| sample:\", members[:6])\n",
    "print()\n",
    "\n",
    "# define core extensions that represent usable DB files\n",
    "core_exts = {\".nsq\", \".psq\", \".ndb\", \".nsq\", \".nhr\", \".psq\", \".nin\"}  # include common core\n",
    "# Normalize to lower-case for checking\n",
    "core_exts = set(e.lower() for e in core_exts)\n",
    "\n",
    "# detect candidate prefixes that actually have core DB files\n",
    "candidate_prefixes = []\n",
    "for pref, members in prefix_map.items():\n",
    "    has_core = False\n",
    "    for m in members:\n",
    "        m_low = m.lower()\n",
    "        for ext in core_exts:\n",
    "            if m_low.endswith(ext):\n",
    "                has_core = True\n",
    "                break\n",
    "        if has_core:\n",
    "            break\n",
    "    if has_core:\n",
    "        candidate_prefixes.append(pref)\n",
    "\n",
    "if not candidate_prefixes:\n",
    "    print(\"[INFO] No DB prefixes with core DB files (.nsq/.psq/.ndb/etc) detected in blastdb_raw/\")\n",
    "    print(\" -> The tarballs may not contain full DB binaries, or they are named unexpectedly.\")\n",
    "    print(\" -> You can run 'show tar contents' again or paste the raw file list above if you want more help.\")\n",
    "    raise RuntimeError(\"No usable BLAST DB prefixes found.\")\n",
    "\n",
    "print(\"Candidate DB prefixes to try with blastdbcmd:\", candidate_prefixes)\n",
    "print()\n",
    "\n",
    "# check for blastdbcmd on PATH\n",
    "bcmd = which(\"blastdbcmd\") or which(\"blastdbcmd.exe\")\n",
    "if not bcmd:\n",
    "    print(\"[INFO] blastdbcmd not on PATH.\")\n",
    "    print(\"Install BLAST+ (bioconda or NCBI binaries) and ensure blastdbcmd is on PATH.\")\n",
    "    print(\"Conda (recommended): conda install -c bioconda blast\")\n",
    "    print(\"Windows binaries: https://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/\")\n",
    "    print(\"\\nExample commands to run AFTER installing blastdbcmd (PowerShell/cmd):\")\n",
    "    for pref in candidate_prefixes:\n",
    "        full_pref = RAW_DB_DIR / pref\n",
    "        out_f = EXTRACT_DIR / f\"{pref}_blastdb_dump.fasta\"\n",
    "        print(f'  blastdbcmd -db \"{full_pref}\" -entry all -outfmt %f -out \"{out_f}\"')\n",
    "    print(\"\\nOnce blastdbcmd is available, re-run this cell and it will create FASTA dumps in ncbi_blast_db/extracted/\")\n",
    "else:\n",
    "    print(\"[OK] blastdbcmd found at:\", bcmd)\n",
    "    for pref in candidate_prefixes:\n",
    "        full_pref = RAW_DB_DIR / pref\n",
    "        out_f = EXTRACT_DIR / f\"{pref}_blastdb_dump.fasta\"\n",
    "        if out_f.exists():\n",
    "            print(f\"[SKIP] FASTA already exists: {out_f.name}\")\n",
    "            continue\n",
    "        cmd = [bcmd, \"-db\", str(full_pref), \"-entry\", \"all\", \"-outfmt\", \"%f\", \"-out\", str(out_f)]\n",
    "        print(\"\\n[RUN] \", \" \".join(cmd))\n",
    "        try:\n",
    "            proc = subprocess.run(cmd, capture_output=True, text=True, check=False)\n",
    "            if proc.returncode == 0:\n",
    "                size = out_f.stat().st_size if out_f.exists() else 0\n",
    "                print(f\" [OK] dumped FASTA -> {out_f.name} ({size} bytes)\")\n",
    "            else:\n",
    "                print(f\" [ERR] blastdbcmd returned code {proc.returncode}\")\n",
    "                print(\"  stdout:\", proc.stdout[:1000])\n",
    "                print(\"  stderr:\", proc.stderr[:1000])\n",
    "        except Exception as e:\n",
    "            print(\" [EXC] Exception running blastdbcmd for\", pref, \":\", e)\n",
    "            traceback.print_exc()\n",
    "\n",
    "# final check: list FASTA files in EXTRACT_DIR\n",
    "final_fastas = sorted([p.name for p in EXTRACT_DIR.iterdir() if p.is_file() and p.suffix.lower() in (\".fasta\", \".fa\", \".fna\")])\n",
    "print()\n",
    "print(\"Final FASTA files in extracted/:\", len(final_fastas))\n",
    "for f in final_fastas[:200]:\n",
    "    print(\" \", f)\n",
    "\n",
    "print(\"\\nCell finished. If FASTA dumps were created, next step: run the k-mer Word2Vec cell (or the downstream embedding cell).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87c6ad9a-4259-4fc5-8881-e13ec6e1084d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory created or already exists: C:\\Users\\Srijit\\sih\\blastdb_raw\n",
      "Contents of blastdb_raw (30):\n",
      "  - ITS_eukaryote_sequences.ndb\n",
      "  - ITS_eukaryote_sequences.nhr\n",
      "  - ITS_eukaryote_sequences.nin\n",
      "  - ITS_eukaryote_sequences.nog\n",
      "  - ITS_eukaryote_sequences.nos\n",
      "  - ITS_eukaryote_sequences.not\n",
      "  - ITS_eukaryote_sequences.nsq\n",
      "  - ITS_eukaryote_sequences.ntf\n",
      "  - ITS_eukaryote_sequences.nto\n",
      "  - LSU_eukaryote_rRNA.ndb\n",
      "  - LSU_eukaryote_rRNA.nhr\n",
      "  - LSU_eukaryote_rRNA.nin\n",
      "  - LSU_eukaryote_rRNA.nog\n",
      "  - LSU_eukaryote_rRNA.nos\n",
      "  - LSU_eukaryote_rRNA.not\n",
      "  - LSU_eukaryote_rRNA.nsq\n",
      "  - LSU_eukaryote_rRNA.ntf\n",
      "  - LSU_eukaryote_rRNA.nto\n",
      "  - SSU_eukaryote_rRNA.ndb\n",
      "  - SSU_eukaryote_rRNA.nhr\n",
      "  - SSU_eukaryote_rRNA.nin\n",
      "  - SSU_eukaryote_rRNA.nog\n",
      "  - SSU_eukaryote_rRNA.nos\n",
      "  - SSU_eukaryote_rRNA.not\n",
      "  - SSU_eukaryote_rRNA.nsq\n",
      "  - SSU_eukaryote_rRNA.ntf\n",
      "  - SSU_eukaryote_rRNA.nto\n",
      "  - taxdb.btd\n",
      "  - taxdb.bti\n",
      "  - taxonomy4blast.sqlite3\n"
     ]
    }
   ],
   "source": [
    "# Create a folder called \"blastdb_raw\" (idempotent) — paste & run in a Jupyter cell\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Change this if you want the folder somewhere else, e.g. Path(\"/data/blastdb_raw\")\n",
    "BASE_DIR = Path.cwd()               # current notebook working directory\n",
    "TARGET = BASE_DIR / \"blastdb_raw\"\n",
    "\n",
    "try:\n",
    "    TARGET.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Directory created or already exists: {TARGET.resolve()}\")\n",
    "    # show basic listing\n",
    "    items = list(TARGET.iterdir())\n",
    "    if items:\n",
    "        print(f\"Contents of {TARGET.name} ({len(items)}):\")\n",
    "        for p in items:\n",
    "            print(\"  -\", p.name)\n",
    "    else:\n",
    "        print(f\"{TARGET.name} is empty.\")\n",
    "except PermissionError:\n",
    "    print(f\"[ERROR] Permission denied creating: {TARGET}\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Could not create directory: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107c3a51-81e1-4e76-8486-625a2be2e510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PATHS] DOWNLOAD_DIR: C:\\Users\\Srijit\\sih\\ncbi_blast_db\n",
      "[PATHS] EXTRACT_DIR : C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\n",
      "[FOUND] FASTA files (count): 6\n",
      " - ssu_fetched.fasta | 86980019 bytes\n",
      " - ssu_combined.fasta | 85918671 bytes\n",
      " - lsu_fetched.fasta | 67876422 bytes\n",
      " - lsu_combined.fasta | 67048142 bytes\n",
      " - its_fetched.fasta | 612802 bytes\n",
      " - its_combined.fasta | 604595 bytes\n",
      "[FOUND] Word2Vec candidate(s):\n",
      " - C:\\Users\\Srijit\\sih\\ncbi_blast_db\\kmer_w2v_k6.model | 5141323 bytes\n",
      "[W2V LOAD] Loaded model from: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\kmer_w2v_k6.model\n",
      "[W2V] ready. vector_size = 128 vocab_size = 4874\n"
     ]
    }
   ],
   "source": [
    "# Cell: build/load kmer W2V and compute sequence embeddings (robust, no name collisions)\n",
    "import os, sys, csv, traceback\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Config (edit only if necessary)\n",
    "DOWNLOAD_DIR = Path(\"./ncbi_blast_db\").resolve()\n",
    "EXTRACT_DIR  = DOWNLOAD_DIR / \"extracted\"\n",
    "K = 6\n",
    "VECTOR_SIZE = 128  # will be overwritten if model found with different size\n",
    "W2V_BASENAME = f\"kmer_w2v_k{K}.model\"\n",
    "OUT_EMB = EXTRACT_DIR / \"embeddings.npy\"\n",
    "OUT_META = EXTRACT_DIR / \"embeddings_meta.csv\"\n",
    "\n",
    "print(\"[PATHS] DOWNLOAD_DIR:\", DOWNLOAD_DIR)\n",
    "print(\"[PATHS] EXTRACT_DIR :\", EXTRACT_DIR)\n",
    "EXTRACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1) find FASTA files (explicit preferred names then glob fallback)\n",
    "expected = [\n",
    "    \"ssu_fetched.fasta\", \"ssu_combined.fasta\",\n",
    "    \"lsu_fetched.fasta\", \"lsu_combined.fasta\",\n",
    "    \"its_fetched.fasta\", \"its_combined.fasta\"\n",
    "]\n",
    "fasta_paths = []\n",
    "for name in expected:\n",
    "    p = EXTRACT_DIR / name\n",
    "    if p.exists():\n",
    "        fasta_paths.append(p)\n",
    "# fallback: recursive glob if none or partial found\n",
    "if not fasta_paths:\n",
    "    fasta_paths = sorted(EXTRACT_DIR.rglob(\".fasta\")) + sorted(EXTRACT_DIR.rglob(\".fa\")) + sorted(EXTRACT_DIR.rglob(\"*.fna\"))\n",
    "\n",
    "# final check\n",
    "if not fasta_paths:\n",
    "    print(\"\\n--- DEBUG DIRECTORY LISTINGS ---\")\n",
    "    def list_dir(p):\n",
    "        if not p.exists():\n",
    "            print(\"  (not found)\", p)\n",
    "            return\n",
    "        for it in sorted(p.iterdir()):\n",
    "            print(\"  \", it.name, \"-\" , (\"<DIR>\" if it.is_dir() else f\"{it.stat().st_size} bytes\"))\n",
    "    print(\"Contents of EXTRACT_DIR:\")\n",
    "    list_dir(EXTRACT_DIR)\n",
    "    print(\"Contents of DOWNLOAD_DIR:\")\n",
    "    list_dir(DOWNLOAD_DIR)\n",
    "    raise FileNotFoundError(f\"No FASTA files found under {EXTRACT_DIR} or {DOWNLOAD_DIR}. You must produce FASTA dumps (blastdbcmd or earlier extraction).\")\n",
    "\n",
    "print(\"[FOUND] FASTA files (count):\", len(fasta_paths))\n",
    "for p in fasta_paths:\n",
    "    try:\n",
    "        print(\" -\", p.name, \"|\", p.stat().st_size, \"bytes\")\n",
    "    except Exception:\n",
    "        print(\" -\", p)\n",
    "\n",
    "# 2) find Word2Vec model (prefer EXTRACT_DIR, then DOWNLOAD_DIR, then any kmer_w2v* in repo)\n",
    "candidate_models = []\n",
    "cand1 = EXTRACT_DIR / W2V_BASENAME\n",
    "cand2 = DOWNLOAD_DIR / W2V_BASENAME\n",
    "if cand1.exists(): candidate_models.append(cand1)\n",
    "if cand2.exists(): candidate_models.append(cand2)\n",
    "# search for any file starting with 'kmer_w2v'\n",
    "for p in [EXTRACT_DIR, DOWNLOAD_DIR, Path.cwd()]:\n",
    "    if p.exists():\n",
    "        for f in p.iterdir():\n",
    "            if f.is_file() and f.name.lower().startswith(\"kmer_w2v\"):\n",
    "                if f not in candidate_models:\n",
    "                    candidate_models.append(f)\n",
    "if candidate_models:\n",
    "    print(\"[FOUND] Word2Vec candidate(s):\")\n",
    "    for m in candidate_models:\n",
    "        print(\" -\", m, \"|\", m.stat().st_size, \"bytes\")\n",
    "else:\n",
    "    print(\"[NOT FOUND] No existing k-mer Word2Vec model found. The cell will train one (may take time).\")\n",
    "\n",
    "# 3) import modules (Biopython, gensim)\n",
    "try:\n",
    "    from Bio import SeqIO\n",
    "except Exception as e:\n",
    "    print(\"ERROR: Biopython is required. Install via conda install -c conda-forge biopython or pip install biopython.\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    from gensim.models import Word2Vec\n",
    "except Exception as e:\n",
    "    print(\"ERROR: gensim is required. Install via conda install -c conda-forge gensim or pip install gensim.\")\n",
    "    raise\n",
    "\n",
    "# 4) Load or train Word2Vec\n",
    "w2v = None\n",
    "if candidate_models:\n",
    "    # try to load first candidate that works\n",
    "    for mp in candidate_models:\n",
    "        try:\n",
    "            w2v = Word2Vec.load(str(mp))\n",
    "            print(f\"[W2V LOAD] Loaded model from: {mp}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"[W2V LOAD] Failed to load {mp}: {e}\")\n",
    "            continue\n",
    "\n",
    "# Safe re-iterable k-mer corpus for training (only if needed)\n",
    "class KmerCorpus:\n",
    "    def _init_(self, fasta_paths, k=6, min_kmers=1):\n",
    "        self.fasta_paths = [Path(p) for p in fasta_paths]\n",
    "        self.k = int(k)\n",
    "        self.min_kmers = int(min_kmers)\n",
    "    def _iter_(self):\n",
    "        for fp in self.fasta_paths:\n",
    "            with fp.open(\"r\", errors=\"replace\") as fh:\n",
    "                for rec in SeqIO.parse(fh, \"fasta\"):\n",
    "                    seq = str(rec.seq).upper().replace(\"\\n\",\"\").replace(\"\\r\",\"\")\n",
    "                    if len(seq) < self.k:\n",
    "                        continue\n",
    "                    kmers = []\n",
    "                    end = len(seq) - self.k + 1\n",
    "                    for i in range(end):\n",
    "                        kmer = seq[i:i+self.k]\n",
    "                        if \"N\" in kmer or \"-\" in kmer:\n",
    "                            continue\n",
    "                        kmers.append(kmer)\n",
    "                    if len(kmers) >= self.min_kmers:\n",
    "                        yield kmers\n",
    "\n",
    "if w2v is None:\n",
    "    print(\"[W2V TRAIN] No loadable model found; training new Word2Vec (k=%d) from discovered FASTAs.\" % K)\n",
    "    corpus = KmerCorpus(fasta_paths, k=K, min_kmers=1)\n",
    "    # minimal/robust Word2Vec parameters\n",
    "    workers = max(1, (os.cpu_count() or 1) - 1)\n",
    "    w2v = Word2Vec(vector_size=VECTOR_SIZE, window=5, min_count=1, workers=workers, sg=1, seed=42)\n",
    "    print(\"[W2V TRAIN] Building vocab (this may take a minute)...\")\n",
    "    w2v.build_vocab(corpus)\n",
    "    print(\"[W2V TRAIN] vocab_size:\", len(w2v.wv.index_to_key))\n",
    "    EPOCHS = 8\n",
    "    print(f\"[W2V TRAIN] Training for {EPOCHS} epochs (workers={workers}) ...\")\n",
    "    w2v.train(corpus, total_examples=w2v.corpus_count, epochs=EPOCHS)\n",
    "    # save into EXTRACT_DIR for reproducibility\n",
    "    save_path = EXTRACT_DIR / W2V_BASENAME\n",
    "    w2v.save(str(save_path))\n",
    "    print(\"[W2V TRAIN] Saved new Word2Vec to:\", save_path)\n",
    "\n",
    "# determine vector size from model\n",
    "try:\n",
    "    VECTOR_SIZE = w2v.vector_size\n",
    "except Exception:\n",
    "    VECTOR_SIZE = w2v.wv.vector_size\n",
    "\n",
    "print(\"[W2V] ready. vector_size =\", VECTOR_SIZE, \"vocab_size =\", len(w2v.wv.index_to_key))\n",
    "\n",
    "# 5) Build embeddings: average k-mer vectors per sequence\n",
    "rows = []\n",
    "embs = []\n",
    "zero_fallback = 0\n",
    "total_seq = 0\n",
    "missing_kmer_counts = []\n",
    "\n",
    "for fp in fasta_paths:\n",
    "    with fp.open(\"r\", errors=\"replace\") as fh:\n",
    "        for rec in SeqIO.parse(fh, \"fasta\"):\n",
    "            total_seq += 1\n",
    "            seq = str(rec.seq).upper().replace(\"\\n\",\"\").replace(\"\\r\",\"\")\n",
    "            seq_id = rec.id if getattr(rec, \"id\", None) else (rec.name if getattr(rec,\"name\",None) else \"\")\n",
    "            if not seq:\n",
    "                # skip empty seqs gracefully\n",
    "                continue\n",
    "            kmers = []\n",
    "            end = len(seq) - K + 1\n",
    "            if end > 0:\n",
    "                for i in range(end):\n",
    "                    kmer = seq[i:i+K]\n",
    "                    if \"N\" in kmer or \"-\" in kmer:\n",
    "                        continue\n",
    "                    kmers.append(kmer)\n",
    "            n_kmers = len(kmers)\n",
    "            # gather vectors\n",
    "            vecs = []\n",
    "            missing = 0\n",
    "            for kmer in kmers:\n",
    "                try:\n",
    "                    # membership check then get_vector to avoid KeyError on some gensim versions\n",
    "                    if kmer in w2v.wv:\n",
    "                        vecs.append(w2v.wv.get_vector(kmer))\n",
    "                    else:\n",
    "                        missing += 1\n",
    "                except Exception:\n",
    "                    # defensive fallback\n",
    "                    missing += 1\n",
    "            if vecs:\n",
    "                emb = np.mean(np.stack(vecs, axis=0), axis=0)\n",
    "            else:\n",
    "                emb = np.zeros(VECTOR_SIZE, dtype=float)\n",
    "                zero_fallback += 1\n",
    "            embs.append(emb.astype(np.float32))\n",
    "            missing_kmer_counts.append(missing)\n",
    "            rows.append({\n",
    "                \"id\": seq_id,\n",
    "                \"source_fasta\": fp.name,\n",
    "                \"seq_len\": len(seq),\n",
    "                \"n_kmers\": n_kmers,\n",
    "                \"n_kmers_missing\": missing,\n",
    "                \"zero_vector_fallback\": int(np.allclose(emb, 0.0))\n",
    "            })\n",
    "\n",
    "# Convert and save\n",
    "if not embs:\n",
    "    raise RuntimeError(\"No embeddings produced (corpus empty after k-mer filtering). Check FASTA files and K value.\")\n",
    "\n",
    "emb_matrix = np.stack(embs, axis=0)\n",
    "print(f\"[EMB] produced embeddings: {emb_matrix.shape}  (total_seq={total_seq}, zero_fallback={zero_fallback})\")\n",
    "\n",
    "# Save numpy and metadata CSV (safe CSV options)\n",
    "OUT_EMB.parent.mkdir(parents=True, exist_ok=True)\n",
    "np.save(OUT_EMB, emb_matrix)\n",
    "df_meta = pd.DataFrame(rows)\n",
    "# use safe csv escaping to avoid pandas csv writer 'escape' issues\n",
    "df_meta.to_csv(OUT_META, index=False, encoding=\"utf-8\", quoting=csv.QUOTE_MINIMAL, escapechar='\\\\')\n",
    "\n",
    "print(\"[SAVE] saved embeddings:\", OUT_EMB)\n",
    "print(\"[SAVE] saved metadata :\", OUT_META)\n",
    "print(\"[DONE] Example metadata rows:\\n\", df_meta.head().to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f33ae06-909d-4950-99fe-655ef4385895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PATHS] EXTRACT_DIR: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\n",
      "[FILES] EMB_NPY: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\embeddings.npy\n",
      "[FILES] META_CSV: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\embeddings_meta.csv\n",
      "[LOAD] embeddings shape: (2555, 128); meta rows: 2555\n",
      "[PCA] requested 64 -> using n_components = 64\n",
      "[PCA] done. explained_variance_ratio_.sum() = 0.9637\n",
      "[SAVE] saved PCA embeddings -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\embeddings_pca.npy\n",
      "[SAVE] saved meta+PCA -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\embeddings_meta_pca.csv\n",
      "[CLUSTER] hdbscan available; will use HDBSCAN.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srijit\\anaconda3\\envs\\sih\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Srijit\\anaconda3\\envs\\sih\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLUSTER] labels produced: unique_labels=102 clusters=101 noise=423\n",
      "[SAVE] saved clustered metadata -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\embeddings_meta_clustered.csv\n",
      "[SAVE] saved cluster summary -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\cluster_summary.csv\n",
      "[RESULT] clusters: 101  noise: 423  total_samples: 2555\n",
      "[TOP CLUSTERS] (cluster_label, n, mean_novelty):\n",
      " cluster_label   n  mean_novelty\n",
      "            -1 423  3.372256e-01\n",
      "            81 270  2.209805e-03\n",
      "            90 244  3.558635e-04\n",
      "            92 212  5.829415e-04\n",
      "            94 190  1.314258e-04\n",
      "            76  72  6.895195e-03\n",
      "            68  54  5.785880e-02\n",
      "            89  52  8.022760e-08\n",
      "\n",
      "[TOP NOVEL CANDIDATES] (first 20):\n",
      "            id       source_fasta  cluster_label  cluster_size  novelty_score\n",
      "XR_013100016.1  ssu_fetched.fasta             -1           423       1.000000\n",
      "XR_013100016.1  ssu_fetched.fasta             -1           423       1.000000\n",
      "    LC876591.1  lsu_fetched.fasta             -1           423       0.987875\n",
      "    LC876591.1  lsu_fetched.fasta             -1           423       0.987875\n",
      "    LC876590.1  lsu_fetched.fasta             -1           423       0.949714\n",
      "    LC876590.1  lsu_fetched.fasta             -1           423       0.949714\n",
      "    LC876589.1  lsu_fetched.fasta             -1           423       0.936899\n",
      "    LC876589.1  lsu_fetched.fasta             -1           423       0.936899\n",
      "    PX275615.1  its_fetched.fasta             -1           423       0.911929\n",
      "    LC876588.1  lsu_fetched.fasta             -1           423       0.900430\n",
      "    LC876588.1  lsu_fetched.fasta             -1           423       0.900430\n",
      "    LC876616.1  lsu_fetched.fasta             -1           423       0.852958\n",
      "    LC876616.1  lsu_fetched.fasta             -1           423       0.852958\n",
      "    PQ523755.1  its_fetched.fasta             -1           423       0.753010\n",
      "    PV487643.1  its_fetched.fasta             -1           423       0.730459\n",
      "    PV487642.1  its_fetched.fasta             -1           423       0.720267\n",
      "               its_combined.fasta             -1           423       0.717762\n",
      "    PX278086.1  its_fetched.fasta             -1           423       0.716748\n",
      "    PX277228.1  its_fetched.fasta             -1           423       0.699558\n",
      "    PX279253.1  its_fetched.fasta             -1           423       0.686504\n",
      "[SAVE] saved cluster_summary.json\n",
      "\n",
      "[OK] PCA + clustering + novelty finished. Next: (a) map known taxonomy to clusters, (b) build training labels, (c) train multi-head classifier.\n"
     ]
    }
   ],
   "source": [
    "# Cell: PCA + clustering + novelty scoring (robust)\n",
    "import json, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# Config\n",
    "DOWNLOAD_DIR = Path(\"./ncbi_blast_db\").resolve()\n",
    "EXTRACT_DIR  = DOWNLOAD_DIR / \"extracted\"\n",
    "EMB_NPY = EXTRACT_DIR / \"embeddings.npy\"\n",
    "META_CSV = EXTRACT_DIR / \"embeddings_meta.csv\"\n",
    "\n",
    "OUT_PCA = EXTRACT_DIR / \"embeddings_pca.npy\"\n",
    "OUT_META_PCA = EXTRACT_DIR / \"embeddings_meta_pca.csv\"\n",
    "OUT_META_CLUSTERED = EXTRACT_DIR / \"embeddings_meta_clustered.csv\"\n",
    "OUT_CLUSTER_SUM = EXTRACT_DIR / \"cluster_summary.csv\"\n",
    "PCA_COMPONENTS = 64\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"[PATHS] EXTRACT_DIR:\", EXTRACT_DIR)\n",
    "print(\"[FILES] EMB_NPY:\", EMB_NPY)\n",
    "print(\"[FILES] META_CSV:\", META_CSV)\n",
    "\n",
    "# sanity: files exist\n",
    "if not EMB_NPY.exists():\n",
    "    raise FileNotFoundError(f\"Missing embeddings file: {EMB_NPY}\")\n",
    "if not META_CSV.exists():\n",
    "    raise FileNotFoundError(f\"Missing metadata CSV: {META_CSV}\")\n",
    "\n",
    "# load\n",
    "emb = np.load(EMB_NPY)\n",
    "meta = pd.read_csv(META_CSV, dtype=str, keep_default_na=False, na_filter=False)\n",
    "\n",
    "# coerce numeric columns we expect\n",
    "if 'seq_len' in meta.columns:\n",
    "    try:\n",
    "        meta['seq_len'] = pd.to_numeric(meta['seq_len'], errors='coerce').fillna(0).astype(int)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# align shapes (trim to min)\n",
    "n_meta = len(meta)\n",
    "n_emb = emb.shape[0]\n",
    "if n_meta != n_emb:\n",
    "    nmin = min(n_meta, n_emb)\n",
    "    print(f\"[ALIGN] mismatch meta({n_meta}) vs emb({n_emb}) -> trimming to {nmin}\")\n",
    "    meta = meta.iloc[:nmin].reset_index(drop=True)\n",
    "    emb = emb[:nmin]\n",
    "\n",
    "print(f\"[LOAD] embeddings shape: {emb.shape}; meta rows: {len(meta)}\")\n",
    "\n",
    "# Standardize then PCA (safe n_components)\n",
    "n_features = emb.shape[1]\n",
    "n_samples = emb.shape[0]\n",
    "n_comp = min(PCA_COMPONENTS, n_features, n_samples)\n",
    "print(f\"[PCA] requested {PCA_COMPONENTS} -> using n_components = {n_comp}\")\n",
    "\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "emb_scaled = scaler.fit_transform(emb)\n",
    "\n",
    "pca = PCA(n_components=n_comp, random_state=RANDOM_STATE)\n",
    "X_pca = pca.fit_transform(emb_scaled)\n",
    "print(f\"[PCA] done. explained_variance_ratio_.sum() = {pca.explained_variance_ratio_.sum():.4f}\")\n",
    "\n",
    "# Save PCA embeddings\n",
    "np.save(OUT_PCA, X_pca)\n",
    "print(\"[SAVE] saved PCA embeddings ->\", OUT_PCA)\n",
    "\n",
    "# add PC columns to meta\n",
    "pc_cols = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n",
    "meta_pca = meta.copy()\n",
    "for i, col in enumerate(pc_cols):\n",
    "    meta_pca[col] = X_pca[:, i]\n",
    "\n",
    "# write meta_pca CSV\n",
    "meta_pca.to_csv(OUT_META_PCA, index=False)\n",
    "print(\"[SAVE] saved meta+PCA ->\", OUT_META_PCA)\n",
    "\n",
    "# ----------------- Clustering -----------------\n",
    "# Prefer hdbscan if available\n",
    "use_hdbscan = False\n",
    "try:\n",
    "    import hdbscan\n",
    "    use_hdbscan = True\n",
    "    print(\"[CLUSTER] hdbscan available; will use HDBSCAN.\")\n",
    "except Exception:\n",
    "    print(\"[CLUSTER] hdbscan not available; falling back to DBSCAN.\")\n",
    "\n",
    "cluster_labels = None\n",
    "cluster_probs = None\n",
    "clusterer_obj = None\n",
    "\n",
    "if use_hdbscan:\n",
    "    # HDBSCAN parameters tuned for sequence clusters, but still conservative:\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=6, min_samples=1,\n",
    "                                metric='euclidean', cluster_selection_method='eom',\n",
    "                                prediction_data=False)\n",
    "    clusterer.fit(X_pca)\n",
    "    cluster_labels = clusterer.labels_.astype(int)\n",
    "    # HDBSCAN may provide membership probabilities\n",
    "    if hasattr(clusterer, \"membership_vector_\") and clusterer.membership_vector_ is not None:\n",
    "        cluster_probs = None  # membership_vector_ is per-cluster; skip generic prob\n",
    "    elif hasattr(clusterer, \"probabilities_\"):\n",
    "        cluster_probs = clusterer.probabilities_\n",
    "    elif hasattr(clusterer, \"membership_strengths_\"):\n",
    "        cluster_probs = clusterer.membership_strengths_\n",
    "    else:\n",
    "        cluster_probs = None\n",
    "    clusterer_obj = clusterer\n",
    "else:\n",
    "    # DBSCAN fallback: eps selected relative to data scale. Try to pick reasonable eps with median pairwise dist\n",
    "    try:\n",
    "        # compute a small sample of pairwise distances for heuristics (to save time)\n",
    "        sample_idx = np.random.RandomState(RANDOM_STATE).choice(X_pca.shape[0], min(800, X_pca.shape[0]), replace=False)\n",
    "        D_sample = pairwise_distances(X_pca[sample_idx], metric='euclidean')\n",
    "        median_d = np.median(D_sample)\n",
    "        eps = float(max(0.5 * median_d, 0.01))\n",
    "    except Exception:\n",
    "        eps = 0.5\n",
    "    print(f\"[DBSCAN] using eps={eps:.4g}, min_samples=5\")\n",
    "    db = DBSCAN(eps=eps, min_samples=5, metric='euclidean', n_jobs=-1)\n",
    "    cluster_labels = db.fit_predict(X_pca).astype(int)\n",
    "    cluster_probs = None\n",
    "    clusterer_obj = db\n",
    "\n",
    "meta_pca['cluster_label'] = cluster_labels\n",
    "if cluster_probs is not None:\n",
    "    meta_pca['cluster_prob'] = cluster_probs\n",
    "else:\n",
    "    meta_pca['cluster_prob'] = \"\"\n",
    "\n",
    "n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "n_noise = int((cluster_labels == -1).sum())\n",
    "print(f\"[CLUSTER] labels produced: unique_labels={len(set(cluster_labels))} clusters={n_clusters} noise={n_noise}\")\n",
    "\n",
    "# ---------------- novelty: distance-to-cluster-centroid (normalized) ----------------\n",
    "Xp = X_pca\n",
    "labels = cluster_labels\n",
    "centroids = {}\n",
    "cluster_sizes = {}\n",
    "dist_to_centroid = np.zeros(Xp.shape[0], dtype=float)\n",
    "\n",
    "for lbl in np.unique(labels):\n",
    "    idxs = np.where(labels == lbl)[0]\n",
    "    cluster_sizes[int(lbl)] = int(len(idxs))\n",
    "    if lbl == -1:\n",
    "        # noise -> we'll set centroid as None; distance computed to nearest non-noise centroid later\n",
    "        centroids[int(lbl)] = None\n",
    "        dist_to_centroid[idxs] = np.nan\n",
    "        continue\n",
    "    centroid = Xp[idxs].mean(axis=0)\n",
    "    centroids[int(lbl)] = centroid\n",
    "    # distances\n",
    "    d = np.linalg.norm(Xp[idxs] - centroid[None, :], axis=1)\n",
    "    dist_to_centroid[idxs] = d\n",
    "\n",
    "# For noise points (lbl == -1), compute distance to nearest cluster centroid (if any)\n",
    "if -1 in centroids and any(v is not None for v in centroids.values()):\n",
    "    non_noise_centroids = np.vstack([v for k,v in centroids.items() if v is not None])\n",
    "    noise_idxs = np.where(labels == -1)[0]\n",
    "    if noise_idxs.size > 0:\n",
    "        # distances to nearest centroid\n",
    "        D = pairwise_distances(Xp[noise_idxs], non_noise_centroids, metric='euclidean')\n",
    "        min_d = D.min(axis=1)\n",
    "        dist_to_centroid[noise_idxs] = min_d\n",
    "\n",
    "# handle nan/missing: replace nan by max distance (conservative novelty)\n",
    "nan_mask = np.isnan(dist_to_centroid)\n",
    "if nan_mask.any():\n",
    "    dist_to_centroid[nan_mask] = np.nanmax(dist_to_centroid[~nan_mask]) if (~nan_mask).any() else 0.0\n",
    "\n",
    "# normalize distances to 0..1 -> novelty score (higher => more novel/outlier)\n",
    "dmin = float(np.nanmin(dist_to_centroid)) if np.isfinite(dist_to_centroid).any() else 0.0\n",
    "dmax = float(np.nanmax(dist_to_centroid)) if np.isfinite(dist_to_centroid).any() else 0.0\n",
    "if dmax > dmin:\n",
    "    novelty = (dist_to_centroid - dmin) / (dmax - dmin)\n",
    "else:\n",
    "    novelty = np.zeros_like(dist_to_centroid)\n",
    "\n",
    "# For pure noise label -1 we want slightly boosted novelty (cap at 1.0)\n",
    "novelty = np.clip(novelty, 0.0, 1.0)\n",
    "novelty = novelty.tolist()\n",
    "\n",
    "meta_pca['cluster_size'] = meta_pca['cluster_label'].map(cluster_sizes).fillna(0).astype(int)\n",
    "meta_pca['cluster_centroid_dist'] = dist_to_centroid\n",
    "meta_pca['novelty_score'] = novelty\n",
    "meta_pca['is_noise'] = (meta_pca['cluster_label'] == -1).astype(int)\n",
    "\n",
    "# reorder columns: keep id, seq_len, source_fasta early if present\n",
    "cols = list(meta_pca.columns)\n",
    "preferred_front = ['id', 'source_fasta', 'seq_len', 'n_kmers', 'n_kmers_missing', 'zero_vector_fallback', 'cluster_label', 'cluster_size', 'novelty_score']\n",
    "cols_ordered = [c for c in preferred_front if c in cols] + [c for c in cols if c not in preferred_front]\n",
    "meta_pca = meta_pca[cols_ordered]\n",
    "\n",
    "# save clustered meta CSV\n",
    "meta_pca.to_csv(OUT_META_CLUSTERED, index=False)\n",
    "print(\"[SAVE] saved clustered metadata ->\", OUT_META_CLUSTERED)\n",
    "\n",
    "# cluster summary\n",
    "cluster_summary = []\n",
    "for lbl, size in cluster_sizes.items():\n",
    "    idxs = np.where(labels == lbl)[0]\n",
    "    mean_nov = float(np.nanmean(np.array(novelty)[idxs])) if idxs.size>0 else float('nan')\n",
    "    cluster_summary.append({\n",
    "        \"cluster_label\": int(lbl),\n",
    "        \"n\": int(size),\n",
    "        \"mean_novelty\": mean_nov\n",
    "    })\n",
    "df_cluster_sum = pd.DataFrame(sorted(cluster_summary, key=lambda r: (-r['n'], -r['mean_novelty'])))\n",
    "df_cluster_sum.to_csv(OUT_CLUSTER_SUM, index=False)\n",
    "print(\"[SAVE] saved cluster summary ->\", OUT_CLUSTER_SUM)\n",
    "\n",
    "# print top statistics & top novel candidates\n",
    "print(f\"[RESULT] clusters: {n_clusters}  noise: {n_noise}  total_samples: {len(meta_pca)}\")\n",
    "top_clusters = df_cluster_sum.head(8)\n",
    "print(\"[TOP CLUSTERS] (cluster_label, n, mean_novelty):\")\n",
    "print(top_clusters.to_string(index=False))\n",
    "\n",
    "# top novel candidate rows (by novelty)\n",
    "top_novel = meta_pca.sort_values(\"novelty_score\", ascending=False).head(20)\n",
    "print(\"\\n[TOP NOVEL CANDIDATES] (first 20):\")\n",
    "print(top_novel[[\"id\",\"source_fasta\",\"cluster_label\",\"cluster_size\",\"novelty_score\"]].to_string(index=False))\n",
    "\n",
    "# Save a compact JSON summary for quick programmatic consumption\n",
    "summary = {\n",
    "    \"n_samples\": int(len(meta_pca)),\n",
    "    \"n_clusters\": int(n_clusters),\n",
    "    \"n_noise\": int(n_noise),\n",
    "    \"top_clusters\": df_cluster_sum.head(10).to_dict(orient=\"records\"),\n",
    "}\n",
    "with open(EXTRACT_DIR / \"cluster_summary.json\", \"w\", encoding=\"utf-8\") as fh:\n",
    "    json.dump(summary, fh, indent=2)\n",
    "print(\"[SAVE] saved cluster_summary.json\")\n",
    "\n",
    "print(\"\\n[OK] PCA + clustering + novelty finished. Next: (a) map known taxonomy to clusters, (b) build training labels, (c) train multi-head classifier.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5eacf1e8-4d4c-4e9d-8ba9-ad7e53f5fc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[USE] metadata file: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\embeddings_meta_clustered.csv (rows=2555)\n",
      "[INFO] meta rows: 2555; id_col used: 'id'\n",
      "[INFO] metadata JSON files discovered: 3\n",
      "[INFO] id_to_tax entries from fetched jsons: 1231\n",
      "[USE] loaded label_assignment_debug.csv rows=2555\n",
      "[SAVE] saved label assignment debug CSV: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\label_assignment_debug_final.csv\n",
      "[LABEL] rank=kingdom  classes=2 saved -> y_encoded_final_kingdom.npy\n",
      "[LABEL] rank=phylum   classes=5 saved -> y_encoded_final_phylum.npy\n",
      "[LABEL] rank=class    classes=10 saved -> y_encoded_final_class.npy\n",
      "[LABEL] rank=order    classes=13 saved -> y_encoded_final_order.npy\n",
      "[LABEL] rank=family   classes=19 saved -> y_encoded_final_family.npy\n",
      "[LABEL] rank=genus    classes=27 saved -> y_encoded_final_genus.npy\n",
      "[LABEL] rank=species  classes=183 saved -> y_encoded_final_species.npy\n",
      "[SAVE] saved label encoders pickle: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\label_encoders_final.pkl\n",
      "\n",
      "=== Assignment summary ===\n",
      "rows processed: 2555\n",
      "no_match_count (pure UNASSIGNED): 1419\n",
      "  kingdom : classes=2 top=[('UNASSIGNED', 1419), ('Eukaryota', 1136)]\n",
      "  phylum  : classes=5 top=[('UNASSIGNED', 1419), ('Metazoa', 718), ('Fungi', 344), ('Viridiplantae', 69), ('Sar', 5)]\n",
      "  class   : classes=10 top=[('UNASSIGNED', 1419), ('Chordata', 688), ('Dikarya', 341), ('Chlorophyta', 56), ('Ecdysozoa', 24), ('Streptophyta', 13)]\n",
      "  order   : classes=13 top=[('UNASSIGNED', 1419), ('Craniata', 688), ('Basidiomycota', 183), ('Ascomycota', 158), ('core chlorophytes', 56), ('Nematoda', 22)]\n",
      "  family  : classes=19 top=[('UNASSIGNED', 1419), ('Vertebrata', 688), ('Agaricomycotina', 183), ('Pezizomycotina', 153), ('Trebouxiophyceae', 48), ('Enoplea', 16)]\n",
      "  genus   : classes=27 top=[('UNASSIGNED', 1419), ('Euteleostomi', 688), ('Agaricomycetes', 181), ('Pezizomycetes', 54), ('Sordariomycetes', 50), ('Prasiolales', 28)]\n",
      "  species : classes=183 top=[('UNASSIGNED', 1419), ('Maylandia zebra', 544), ('Chaetodon auriga', 112), ('Morchella sp.', 38), ('Arvicanthis niloticus', 20), ('Chloroidium saccharophilum', 20)]\n",
      "\n",
      "Outputs written (or attempted):\n",
      " - label encoders: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\label_encoders_final.pkl\n",
      " - y array: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\y_encoded_final_kingdom.npy\n",
      " - y array: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\y_encoded_final_phylum.npy\n",
      " - y array: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\y_encoded_final_class.npy\n",
      " - y array: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\y_encoded_final_order.npy\n",
      " - y array: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\y_encoded_final_family.npy\n",
      " - y array: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\y_encoded_final_genus.npy\n",
      " - y array: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\y_encoded_final_species.npy\n",
      " - debug CSV: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\label_assignment_debug_final.csv\n",
      "\n",
      "[READY] You can now run the next cell (train/test split) which will reuse these encoders and y arrays.\n"
     ]
    }
   ],
   "source": [
    "# Cell A: Build final per-sample taxonomy labels and save label encoders + y arrays (robust)\n",
    "# - Reads embeddings_meta_clustered.csv (fallbacks), fetched metadata JSONs and/or label_assignment_debug.csv\n",
    "# - Produces label_encoders_final.pkl, y_encoded_final_<rank>.npy, and label_assignment_debug_final.csv\n",
    "# - Defensive: handles encoding issues, missing files, truncated 'UNASSIGNE' artifacts, and alignment to meta rows.\n",
    "\n",
    "import json, re, csv, pickle\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ---------------- Config / paths (uses existing DOWNLOAD_DIR if present in notebook globals) ----------------\n",
    "DOWNLOAD_DIR = Path(globals().get(\"DOWNLOAD_DIR\", \"./ncbi_blast_db\"))\n",
    "EXTRACT_DIR = Path(globals().get(\"EXTRACT_DIR\", DOWNLOAD_DIR / \"extracted\"))\n",
    "OUT_ENCODERS = EXTRACT_DIR / \"label_encoders_final.pkl\"\n",
    "OUT_Y_TEMPLATE = EXTRACT_DIR / \"y_encoded_final_{}.npy\"\n",
    "DEBUG_OUT = EXTRACT_DIR / \"label_assignment_debug_final.csv\"\n",
    "\n",
    "RANKS = [\"kingdom\", \"phylum\", \"class\", \"order\", \"family\", \"genus\", \"species\"]\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def norm_label(x):\n",
    "    \"\"\"Normalize taxonomy label strings to safe canonical form; empty -> 'UNASSIGNED'\"\"\"\n",
    "    if x is None:\n",
    "        return \"UNASSIGNED\"\n",
    "    s = str(x).strip()\n",
    "    if s == \"\" or s.lower() in (\"nan\",\"none\",\"na\",\"-\",\"?\"):\n",
    "        return \"UNASSIGNED\"\n",
    "    # collapse whitespace and remove weird nulls\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = s.replace(\"\\x00\", \"\").replace(\"\\ufffd\", \"?\").strip()\n",
    "    # fix common truncated artifact like 'UNASSIGNE' or 'UNASSIG' -> 'UNASSIGNED'\n",
    "    if s.upper().startswith(\"UNASSIG\"):\n",
    "        return \"UNASSIGNED\"\n",
    "    return s\n",
    "\n",
    "def safe_json_load(p):\n",
    "    \"\"\"Load JSON robustly, return list of records or empty list\"\"\"\n",
    "    try:\n",
    "        with open(p, \"r\", encoding=\"utf-8\", errors=\"replace\") as fh:\n",
    "            data = json.load(fh)\n",
    "            # If single dict, wrap in list\n",
    "            if isinstance(data, dict):\n",
    "                return [data]\n",
    "            if isinstance(data, list):\n",
    "                return data\n",
    "            # sometimes file might be newline-delimited JSON\n",
    "            # attempt line-by-line parse\n",
    "            fh.seek(0)\n",
    "    except Exception:\n",
    "        # try newline-delimited JSON fallback\n",
    "        recs = []\n",
    "        try:\n",
    "            with open(p, \"r\", encoding=\"utf-8\", errors=\"replace\") as fh:\n",
    "                for line in fh:\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        continue\n",
    "                    try:\n",
    "                        recs.append(json.loads(line))\n",
    "                    except Exception:\n",
    "                        # skip malformed lines\n",
    "                        continue\n",
    "        except Exception:\n",
    "            return []\n",
    "        return recs\n",
    "    return []\n",
    "\n",
    "def extract_rec_id(rec):\n",
    "    \"\"\"\n",
    "    For a metadata record dict, return best id string (accession, accession.version, id, etc.)\n",
    "    \"\"\"\n",
    "    for key in (\"id\",\"accession_version\",\"accession.version\",\"accession\",\"accession.ver\",\"gi\",\"seqid\",\"name\"):\n",
    "        if isinstance(rec, dict) and key in rec and rec.get(key):\n",
    "            return str(rec.get(key))\n",
    "    # fallback: try description/organism (not ideal)\n",
    "    for key in (\"organism\",\"description\",\"title\"):\n",
    "        if isinstance(rec, dict) and key in rec and rec.get(key):\n",
    "            # attempt to extract accession-like token\n",
    "            tok = str(rec.get(key)).split()[0]\n",
    "            if tok:\n",
    "                return tok\n",
    "    return \"\"\n",
    "\n",
    "def build_tax_map_from_rec(rec):\n",
    "    \"\"\"\n",
    "    Given a metadata record, return a dict with keys for RANKS (where available).\n",
    "    Accepts rec['taxonomy'] as list OR rec may already have structured taxonomy keys.\n",
    "    \"\"\"\n",
    "    tmap = {}\n",
    "    # 1) If rec has 'taxonomy' list (ordered), map to ranks\n",
    "    taxlist = rec.get(\"taxonomy\") if isinstance(rec, dict) else None\n",
    "    if isinstance(taxlist, list) and len(taxlist) > 0:\n",
    "        ranks_order = [\"kingdom\",\"phylum\",\"class\",\"order\",\"family\",\"genus\"]\n",
    "        for i, v in enumerate(taxlist[:len(ranks_order)]):\n",
    "            if v:\n",
    "                tmap[ranks_order[i]] = norm_label(v)\n",
    "    # 2) If rec has explicit rank keys, copy them\n",
    "    for r in RANKS:\n",
    "        if r in rec and rec.get(r):\n",
    "            tmap[r] = norm_label(rec.get(r))\n",
    "    # 3) organism -> genus/species heuristic\n",
    "    organism = (rec.get(\"organism\") or rec.get(\"description\") or rec.get(\"title\") or \"\").strip()\n",
    "    if organism:\n",
    "        parts = re.split(r\"\\s+\", organism)\n",
    "        if len(parts) >= 2:\n",
    "            # genus = first, species = first + second\n",
    "            tmap.setdefault(\"genus\", norm_label(parts[0]))\n",
    "            tmap.setdefault(\"species\", norm_label(parts[0] + \" \" + parts[1]))\n",
    "        else:\n",
    "            # single token organism -> fill genus only\n",
    "            tmap.setdefault(\"genus\", norm_label(parts[0]))\n",
    "    # ensure all ranks present (or will be filled later)\n",
    "    for r in RANKS:\n",
    "        if r not in tmap:\n",
    "            tmap[r] = None\n",
    "    return tmap\n",
    "\n",
    "# ---------------- Load embeddings meta (pick the best available) ----------------\n",
    "meta_candidates = [\n",
    "    EXTRACT_DIR / \"embeddings_meta_clustered.csv\",\n",
    "    EXTRACT_DIR / \"embeddings_meta_pca.csv\",\n",
    "    EXTRACT_DIR / \"embeddings_meta.csv\",\n",
    "    EXTRACT_DIR / \"embeddings_meta.csv\"  # repeated but harmless\n",
    "]\n",
    "df_meta = None\n",
    "for p in meta_candidates:\n",
    "    if p.exists():\n",
    "        try:\n",
    "            df_meta = pd.read_csv(p, dtype=str, keep_default_na=False, na_filter=False)\n",
    "            print(f\"[USE] metadata file: {p} (rows={len(df_meta)})\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] failed to read {p}: {e}\")\n",
    "            continue\n",
    "\n",
    "if df_meta is None:\n",
    "    # final fallback: embeddings_meta.csv may not exist; try embeddings_meta (other names)\n",
    "    raise FileNotFoundError(f\"No embeddings metadata CSV found in {EXTRACT_DIR}. Expected one of: {', '.join(str(x) for x in meta_candidates)}\")\n",
    "\n",
    "# Ensure there is an 'id' column; try other column names if absent\n",
    "id_col = None\n",
    "for cand in [\"id\",\"seq_id\",\"accession\",\"accession_version\",\"header\"]:\n",
    "    if cand in df_meta.columns:\n",
    "        id_col = cand\n",
    "        break\n",
    "# fallback: take first column as id\n",
    "if id_col is None:\n",
    "    id_col = df_meta.columns[0]\n",
    "    print(f\"[WARN] No explicit id column found. Using '{id_col}' (first column) as id.\")\n",
    "\n",
    "# normalize id column strings\n",
    "df_meta[id_col] = df_meta[id_col].astype(str).fillna(\"\").apply(lambda s: s.strip())\n",
    "df_meta = df_meta.reset_index(drop=True)\n",
    "n_samples = len(df_meta)\n",
    "print(f\"[INFO] meta rows: {n_samples}; id_col used: '{id_col}'\")\n",
    "\n",
    "# ---------------- Build id -> taxonomy lookup from fetched metadata JSON files ----------------\n",
    "id_to_tax = {}\n",
    "index_alt = defaultdict(dict)  # accession, accession_base, organism -> rec_id\n",
    "\n",
    "# look for *_fetched_metadata.json or *_fetched.json in EXTRACT_DIR\n",
    "meta_json_paths = sorted(EXTRACT_DIR.glob(\"_fetched_metadata.json\")) + sorted(EXTRACT_DIR.glob(\"_fetched_meta.json\")) + sorted(EXTRACT_DIR.glob(\"_fetched.json\")) \n",
    "# also include *_fetched_metadata.json produced earlier (explicit names)\n",
    "meta_json_paths = [p for p in meta_json_paths if p.exists()]\n",
    "if not meta_json_paths:\n",
    "    # try the explicit names observed earlier\n",
    "    for name in (\"ssu_fetched_metadata.json\",\"lsu_fetched_metadata.json\",\"its_fetched_metadata.json\"):\n",
    "        p = EXTRACT_DIR / name\n",
    "        if p.exists():\n",
    "            meta_json_paths.append(p)\n",
    "\n",
    "print(f\"[INFO] metadata JSON files discovered: {len(meta_json_paths)}\")\n",
    "\n",
    "for p in meta_json_paths:\n",
    "    recs = safe_json_load(p)\n",
    "    if not recs:\n",
    "        continue\n",
    "    for rec in recs:\n",
    "        try:\n",
    "            rec_id = extract_rec_id(rec)\n",
    "            if not rec_id:\n",
    "                # try accession fields in various naming\n",
    "                rec_id = (rec.get(\"accession\") or rec.get(\"accession_version\") or rec.get(\"id\") or \"\")\n",
    "            rec_id = str(rec_id).strip()\n",
    "            if not rec_id:\n",
    "                continue\n",
    "            tmap = build_tax_map_from_rec(rec)\n",
    "            id_to_tax[rec_id] = tmap\n",
    "            # alt indices\n",
    "            acc = rec.get(\"accession\") or \"\"\n",
    "            if acc:\n",
    "                index_alt[\"accession\"][str(acc)] = rec_id\n",
    "                index_alt[\"accession_base\"][str(acc).split(\".\")[0]] = rec_id\n",
    "            index_alt[\"id_base\"][rec_id.split(\".\")[0]] = rec_id\n",
    "            if rec.get(\"organism\"):\n",
    "                index_alt[\"organism\"][str(rec.get(\"organism\")).lower()] = rec_id\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "print(f\"[INFO] id_to_tax entries from fetched jsons: {len(id_to_tax)}\")\n",
    "\n",
    "# ---------------- Also ingest label_assignment_debug.csv if present (extra mapping source) ----------------\n",
    "debug_csv = EXTRACT_DIR / \"label_assignment_debug.csv\"\n",
    "if debug_csv.exists():\n",
    "    try:\n",
    "        df_dbg = pd.read_csv(debug_csv, dtype=str, keep_default_na=False, na_filter=False)\n",
    "        print(f\"[USE] loaded label_assignment_debug.csv rows={len(df_dbg)}\")\n",
    "        # Attempt to find id-like column\n",
    "        dbg_id_col = None\n",
    "        for cand in (\"id\",\"accession\",\"accession_version\",\"acc_base\",\"acc\",\"header\"):\n",
    "            if cand in df_dbg.columns:\n",
    "                dbg_id_col = cand\n",
    "                break\n",
    "        if dbg_id_col is None:\n",
    "            dbg_id_col = df_dbg.columns[0]\n",
    "        df_dbg[dbg_id_col] = df_dbg[dbg_id_col].astype(str).fillna(\"\").apply(lambda s: s.strip())\n",
    "        # we expect columns named like assigned_genus / assigned_species etc. collect any columns containing 'assigned' or rank names\n",
    "        for _, row in df_dbg.iterrows():\n",
    "            rid = row.get(dbg_id_col, \"\")\n",
    "            if not rid:\n",
    "                continue\n",
    "            tmap = {}\n",
    "            for r in RANKS:\n",
    "                # look for columns matching the rank name or 'assigned_<rank>'\n",
    "                possible_cols = [c for c in df_dbg.columns if (c.lower() == r or c.lower().endswith(\"_\"+r) or \"assigned\" in c.lower() and r in c.lower())]\n",
    "                val = None\n",
    "                for c in possible_cols:\n",
    "                    v = row.get(c, \"\")\n",
    "                    if isinstance(v, str) and v.strip() != \"\":\n",
    "                        val = v\n",
    "                        break\n",
    "                tmap[r] = norm_label(val) if val else None\n",
    "            # if tmap has any non-empty entries, store in id_to_tax (but don't override existing fetched)\n",
    "            if any(v for v in tmap.values()):\n",
    "                if rid not in id_to_tax:\n",
    "                    id_to_tax[rid] = {k: (v if v is not None else None) for k, v in tmap.items()}\n",
    "                # also populate alt indices\n",
    "                base = rid.split(\".\")[0]\n",
    "                index_alt[\"id_base\"][base] = rid\n",
    "                acc = row.get(\"accession\",\"\")\n",
    "                if acc:\n",
    "                    index_alt[\"accession\"][acc] = rid\n",
    "                    index_alt[\"accession_base\"][acc.split(\".\")[0]] = rid\n",
    "                org = row.get(\"organism\",\"\")\n",
    "                if org:\n",
    "                    index_alt[\"organism\"][org.lower()] = rid\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] failed to read label_assignment_debug.csv: {e}\")\n",
    "\n",
    "# ---------------- Now map each df_meta row to a taxonomy (with fallbacks) ----------------\n",
    "assigned_rows = []\n",
    "no_match = 0\n",
    "for ix, row in df_meta.iterrows():\n",
    "    rid = str(row.get(id_col,\"\")).strip()\n",
    "    out = {\"id\": rid}\n",
    "    matched_source = \"\"\n",
    "    tmap = None\n",
    "    if rid and rid in id_to_tax:\n",
    "        tmap = id_to_tax[rid]\n",
    "        matched_source = \"fetched_exact\"\n",
    "    else:\n",
    "        # try base id\n",
    "        base = rid.split(\".\")[0]\n",
    "        if base and base in id_to_tax:\n",
    "            tmap = id_to_tax[base]\n",
    "            matched_source = \"fetched_base\"\n",
    "        else:\n",
    "            # try alt indices (accession, accession_base, id_base)\n",
    "            if rid in index_alt.get(\"accession\", {}):\n",
    "                cand = index_alt[\"accession\"][rid]\n",
    "                tmap = id_to_tax.get(cand)\n",
    "                matched_source = \"alt_accession\"\n",
    "            elif base in index_alt.get(\"accession_base\", {}):\n",
    "                cand = index_alt[\"accession_base\"][base]\n",
    "                tmap = id_to_tax.get(cand)\n",
    "                matched_source = \"alt_accession_base\"\n",
    "            elif base in index_alt.get(\"id_base\", {}):\n",
    "                cand = index_alt[\"id_base\"][base]\n",
    "                tmap = id_to_tax.get(cand)\n",
    "                matched_source = \"alt_id_base\"\n",
    "            else:\n",
    "                # try organism substring match heuristics\n",
    "                lowrid = rid.lower()\n",
    "                candidate = None\n",
    "                for orgname, recid in index_alt.get(\"organism\", {}).items():\n",
    "                    if not orgname:\n",
    "                        continue\n",
    "                    try:\n",
    "                        # direct substring check both ways\n",
    "                        if orgname in lowrid or lowrid in orgname:\n",
    "                            candidate = recid\n",
    "                            break\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                if candidate:\n",
    "                    tmap = id_to_tax.get(candidate)\n",
    "                    matched_source = \"alt_organism_match\"\n",
    "    # if still None, attempt to look for 'organism' field in df_meta if present\n",
    "    if tmap is None:\n",
    "        # check for organism-like columns in df_meta\n",
    "        found_org = None\n",
    "        for cand in (\"organism\",\"organism_name\",\"description\",\"desc\",\"meta_organism\"):\n",
    "            if cand in df_meta.columns and str(row.get(cand)).strip():\n",
    "                found_org = str(row.get(cand)).strip()\n",
    "                break\n",
    "        if found_org:\n",
    "            parts = re.split(r\"\\s+\", found_org)\n",
    "            tmap = {r: None for r in RANKS}\n",
    "            if len(parts) >= 2:\n",
    "                tmap[\"genus\"] = norm_label(parts[0])\n",
    "                tmap[\"species\"] = norm_label(parts[0] + \" \" + parts[1])\n",
    "            matched_source = \"meta_organism_infer\"\n",
    "    if tmap is None:\n",
    "        # nothing matched -> mark UNASSIGNED\n",
    "        tmap = {r: None for r in RANKS}\n",
    "        matched_source = \"UNASSIGNED\"\n",
    "        no_match += 1\n",
    "\n",
    "    # add final normalized labels for each rank in the out dict\n",
    "    for r in RANKS:\n",
    "        out[r] = norm_label(tmap.get(r) if isinstance(tmap, dict) else tmap) if tmap.get(r) else \"UNASSIGNED\"\n",
    "    out[\"matched_source\"] = matched_source\n",
    "    # include original meta columns to the debug row (safe strings)\n",
    "    for c in df_meta.columns:\n",
    "        # avoid huge sequence strings in debug output; only include small metadata fields if present\n",
    "        if c in (\"raw\",\"sequence\",\"seq\",\"seq_full\",\"seq_header\"): \n",
    "            continue\n",
    "        try:\n",
    "            out[f\"meta__{c}\"] = str(row.get(c,\"\"))\n",
    "        except Exception:\n",
    "            out[f\"meta__{c}\"] = \"\"\n",
    "    assigned_rows.append(out)\n",
    "\n",
    "df_assigned = pd.DataFrame(assigned_rows)\n",
    "# align length safety\n",
    "if len(df_assigned) != len(df_meta):\n",
    "    print(f\"[WARN] assigned_rows length {len(df_assigned)} != meta rows {len(df_meta)}; trimming/padding as needed.\")\n",
    "    nmin = min(len(df_assigned), len(df_meta))\n",
    "    df_assigned = df_assigned.iloc[:nmin].reset_index(drop=True)\n",
    "    df_meta = df_meta.iloc[:nmin].reset_index(drop=True)\n",
    "\n",
    "# Save debug assignment file\n",
    "try:\n",
    "    df_assigned.to_csv(DEBUG_OUT, index=False, encoding=\"utf-8\")\n",
    "    print(f\"[SAVE] saved label assignment debug CSV: {DEBUG_OUT}\")\n",
    "except Exception as e:\n",
    "    print(f\"[WARN] failed saving debug CSV: {e}\")\n",
    "\n",
    "# ---------------- Build LabelEncoders for each rank and y arrays ----------------\n",
    "label_encoders = {}\n",
    "y_encoded = {}\n",
    "\n",
    "for r in RANKS:\n",
    "    labels = df_assigned[r].astype(str).fillna(\"UNASSIGNED\").apply(norm_label).tolist()\n",
    "    # ensure explicit 'UNASSIGNED' present\n",
    "    if \"UNASSIGNED\" not in labels:\n",
    "        labels = [\"UNASSIGNED\"] + labels\n",
    "    le = LabelEncoder()\n",
    "    try:\n",
    "        le.fit(labels)\n",
    "    except Exception as e:\n",
    "        # as a safety, deduplicate and ensure strings\n",
    "        uniq = list(dict.fromkeys([norm_label(x) for x in labels]))\n",
    "        le.fit(uniq)\n",
    "    y = le.transform([norm_label(x) for x in df_assigned[r].astype(str).fillna(\"UNASSIGNED\").tolist()])\n",
    "    label_encoders[r] = le\n",
    "    y_encoded[r] = np.asarray(y, dtype=np.int32)\n",
    "    # save y array\n",
    "    outy = OUT_Y_TEMPLATE.with_name(OUT_Y_TEMPLATE.name.format(r)).resolve()\n",
    "    np.save(outy, y_encoded[r])\n",
    "    print(f\"[LABEL] rank={r:8s} classes={len(le.classes_)} saved -> {outy.name}\")\n",
    "\n",
    "# Save encoders dict\n",
    "try:\n",
    "    with open(OUT_ENCODERS, \"wb\") as fh:\n",
    "        pickle.dump(label_encoders, fh)\n",
    "    print(f\"[SAVE] saved label encoders pickle: {OUT_ENCODERS}\")\n",
    "except Exception as e:\n",
    "    print(f\"[WARN] could not save label encoders pickle: {e}\")\n",
    "\n",
    "# ---------------- Summary ----------------\n",
    "total = len(df_assigned)\n",
    "assigned_counts = {}\n",
    "for r in RANKS:\n",
    "    cnt = Counter(df_assigned[r].fillna(\"UNASSIGNED\").tolist())\n",
    "    assigned_counts[r] = dict(cnt)\n",
    "print(\"\\n=== Assignment summary ===\")\n",
    "print(f\"rows processed: {total}\")\n",
    "print(f\"no_match_count (pure UNASSIGNED): {no_match}\")\n",
    "for r in RANKS:\n",
    "    top = Counter(df_assigned[r].fillna(\"UNASSIGNED\").tolist()).most_common(8)\n",
    "    print(f\"  {r:8s}: classes={len(label_encoders[r].classes_)} top={top[:6]}\")\n",
    "\n",
    "print(\"\\nOutputs written (or attempted):\")\n",
    "print(\" - label encoders:\", OUT_ENCODERS)\n",
    "for r in RANKS:\n",
    "    print(\" - y array:\", OUT_Y_TEMPLATE.with_name(OUT_Y_TEMPLATE.name.format(r)))\n",
    "print(\" - debug CSV:\", DEBUG_OUT)\n",
    "print(\"\\n[READY] You can now run the next cell (train/test split) which will reuse these encoders and y arrays.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26dea50f-65de-441d-a842-fc4a703d9901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD] X_pca shape: (2555, 64)\n",
      "[LOAD] label_encoders ranks: ['kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species']\n",
      "[LOAD] y arrays loaded & aligned.\n",
      "[LOAD] train_idx from train_idx_final.npy (n=2175), val_idx from val_idx_final.npy (n=380)\n",
      "[OK] train_dataset size: 2175, val_dataset size: 380; batch_size=128\n",
      "[SANITY] Batch keys: ['x', 'kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species']\n",
      "[SANITY] x.shape: torch.Size([128, 64])\n",
      "  kingdom: shape=torch.Size([128]), dtype=torch.int64\n",
      "  phylum: shape=torch.Size([128]), dtype=torch.int64\n",
      "  class: shape=torch.Size([128]), dtype=torch.int64\n",
      "  order: shape=torch.Size([128]), dtype=torch.int64\n",
      "[READY] Use train_loader, val_loader, and batch_tuple_to_dict in your training loop.\n"
     ]
    }
   ],
   "source": [
    "# Safe replacement cell: build train/val TensorDatasets (no custom Dataset class)\n",
    "import numpy as np, pickle, os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# ---------- paths (adjust only if your environment differs) ----------\n",
    "DOWNLOAD_DIR = Path(\"ncbi_blast_db\")\n",
    "EXTRACT_DIR  = DOWNLOAD_DIR / \"extracted\"\n",
    "EMB_PCA = EXTRACT_DIR / \"embeddings_pca.npy\"\n",
    "LABEL_ENCODERS_PKL = EXTRACT_DIR / \"label_encoders_final.pkl\"\n",
    "\n",
    "# possible train/val index file names (tries in order)\n",
    "TRAIN_IDX_FILES = [\n",
    "    EXTRACT_DIR / \"train_idx_final.npy\",\n",
    "    EXTRACT_DIR / \"train_idx_by_acc.npy\",\n",
    "    EXTRACT_DIR / \"train_idx.npy\",\n",
    "    EXTRACT_DIR / \"train_idx_random_fallback.npy\",\n",
    "]\n",
    "VAL_IDX_FILES = [\n",
    "    EXTRACT_DIR / \"val_idx_final.npy\",\n",
    "    EXTRACT_DIR / \"val_idx_by_acc.npy\",\n",
    "    EXTRACT_DIR / \"val_idx.npy\",\n",
    "    EXTRACT_DIR / \"val_idx_random_fallback.npy\",\n",
    "]\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NUM_WORKERS = 0\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ---------- helper ----------\n",
    "def first_exist(paths):\n",
    "    for p in paths:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "# ---------- load embeddings ----------\n",
    "if not EMB_PCA.exists():\n",
    "    raise FileNotFoundError(f\"embeddings_pca.npy not found at {EMB_PCA}\")\n",
    "X_pca = np.load(EMB_PCA)\n",
    "n_samples = X_pca.shape[0]\n",
    "print(f\"[LOAD] X_pca shape: {X_pca.shape}\")\n",
    "\n",
    "# ---------- load label encoders ----------\n",
    "enc_path = first_exist([EXTRACT_DIR / \"label_encoders_final.pkl\",\n",
    "                        EXTRACT_DIR / \"label_encoders_rebuilt_v2.pkl\",\n",
    "                        EXTRACT_DIR / \"label_encoders_rebuilt.pkl\",\n",
    "                        EXTRACT_DIR / \"label_encoders_used.pkl\"])\n",
    "if enc_path is None:\n",
    "    raise FileNotFoundError(\"label_encoders pickle not found in extracted/\")\n",
    "with open(enc_path, \"rb\") as fh:\n",
    "    label_encoders = pickle.load(fh)\n",
    "RANKS = list(label_encoders.keys())\n",
    "print(f\"[LOAD] label_encoders ranks: {RANKS}\")\n",
    "\n",
    "# ---------- load y arrays ----------\n",
    "y_encoded = {}\n",
    "for r in RANKS:\n",
    "    p1 = EXTRACT_DIR / f\"y_encoded_final_{r}.npy\"\n",
    "    p2 = EXTRACT_DIR / f\"y_encoded_rebuilt_{r}.npy\"\n",
    "    p = p1 if p1.exists() else (p2 if p2.exists() else None)\n",
    "    if p is None:\n",
    "        raise FileNotFoundError(f\"Missing y array for rank '{r}' (looked for {p1} and {p2})\")\n",
    "    arr = np.load(p)\n",
    "    if arr.shape[0] != n_samples:\n",
    "        if arr.shape[0] < n_samples:\n",
    "            pad = np.zeros((n_samples - arr.shape[0],), dtype=int)\n",
    "            arr = np.concatenate([arr, pad], axis=0)\n",
    "            print(f\"[WARN] padded y array for {r} from {p.name} to length {n_samples}\")\n",
    "        else:\n",
    "            arr = arr[:n_samples]\n",
    "            print(f\"[WARN] trimmed y array for {r} from {p.name} to length {n_samples}\")\n",
    "    y_encoded[r] = arr.astype(int)\n",
    "print(\"[LOAD] y arrays loaded & aligned.\")\n",
    "\n",
    "# ---------- load train/val indices (try saved) ----------\n",
    "tfile = first_exist(TRAIN_IDX_FILES)\n",
    "vfile = first_exist(VAL_IDX_FILES)\n",
    "if tfile is None or vfile is None:\n",
    "    # fallback to deterministic random split and save\n",
    "    print(\"[WARN] train/val idx files missing; creating deterministic 85/15 split\")\n",
    "    rng = np.random.RandomState(SEED)\n",
    "    perm = rng.permutation(n_samples)\n",
    "    cutoff = int(n_samples * 0.85)\n",
    "    train_idx = np.sort(perm[:cutoff]).astype(int)\n",
    "    val_idx = np.sort(perm[cutoff:]).astype(int)\n",
    "    np.save(EXTRACT_DIR / \"train_idx_random_fallback.npy\", train_idx)\n",
    "    np.save(EXTRACT_DIR / \"val_idx_random_fallback.npy\", val_idx)\n",
    "    print(\"[SAVE] saved fallback train/val idx arrays\")\n",
    "else:\n",
    "    train_idx = np.load(tfile).astype(int)\n",
    "    val_idx   = np.load(vfile).astype(int)\n",
    "    print(f\"[LOAD] train_idx from {tfile.name} (n={len(train_idx)}), val_idx from {vfile.name} (n={len(val_idx)})\")\n",
    "\n",
    "# ---------- Build tensors for training (use TensorDataset) ----------\n",
    "# X tensors\n",
    "X_train = torch.from_numpy(X_pca[train_idx]).float()\n",
    "X_val   = torch.from_numpy(X_pca[val_idx]).float()\n",
    "\n",
    "# label tensors: keep same order as RANKS\n",
    "y_train_tensors = []\n",
    "y_val_tensors = []\n",
    "for r in RANKS:\n",
    "    ytr = torch.from_numpy(y_encoded[r][train_idx]).long()\n",
    "    yv  = torch.from_numpy(y_encoded[r][val_idx]).long()\n",
    "    y_train_tensors.append(ytr)\n",
    "    y_val_tensors.append(yv)\n",
    "\n",
    "# Compose TensorDataset: first tensor is X, then each rank tensor\n",
    "train_dataset = TensorDataset(X_train, *y_train_tensors)\n",
    "val_dataset   = TensorDataset(X_val, *y_val_tensors)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "print(f\"[OK] train_dataset size: {len(train_dataset)}, val_dataset size: {len(val_dataset)}; batch_size={BATCH_SIZE}\")\n",
    "\n",
    "# ---------- Helper to convert tuple-batch -> dict keyed by ranks ----------\n",
    "def batch_tuple_to_dict(batch_tuple):\n",
    "    \"\"\"\n",
    "    Input: a tuple from DataLoader (x, y_rank1, y_rank2, ...)\n",
    "    Returns: dict with keys: \"x\" -> tensor, and RANKS[0]..RANKS[-1] -> tensors\n",
    "    \"\"\"\n",
    "    d = {\"x\": batch_tuple[0]}\n",
    "    for i, r in enumerate(RANKS):\n",
    "        d[r] = batch_tuple[1 + i]\n",
    "    return d\n",
    "\n",
    "# quick sanity check (fetch one batch)\n",
    "batch_tuple = next(iter(train_loader))\n",
    "batch = batch_tuple_to_dict(batch_tuple)\n",
    "print(\"[SANITY] Batch keys:\", list(batch.keys()))\n",
    "print(\"[SANITY] x.shape:\", batch[\"x\"].shape)\n",
    "for r in RANKS[:4]:\n",
    "    print(f\"  {r}: shape={batch[r].shape}, dtype={batch[r].dtype}\")\n",
    "\n",
    "# expose useful objects to notebook globals for downstream training cell\n",
    "globals().update({\n",
    "    \"train_loader\": train_loader,\n",
    "    \"val_loader\": val_loader,\n",
    "    \"train_dataset\": train_dataset,\n",
    "    \"val_dataset\": val_dataset,\n",
    "    \"batch_tuple_to_dict\": batch_tuple_to_dict,\n",
    "    \"RANKS\": RANKS,\n",
    "    \"label_encoders\": label_encoders,\n",
    "    \"train_idx\": train_idx,\n",
    "    \"val_idx\": val_idx\n",
    "})\n",
    "\n",
    "print(\"[READY] Use train_loader, val_loader, and batch_tuple_to_dict in your training loop.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ceaed4e-2c87-4b80-a5a1-8424ce755790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] device=cpu; ranks=['kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species']\n",
      "[INFO] inferred input_dim = 64\n",
      "[INFO] total model parameters = 82,947; param tensors = 18\n",
      "[INFO] built loss for 'kingdom' with n_classes=2, weight_shape=torch.Size([2])\n",
      "[INFO] built loss for 'phylum' with n_classes=5, weight_shape=torch.Size([5])\n",
      "[INFO] built loss for 'class' with n_classes=10, weight_shape=torch.Size([10])\n",
      "[INFO] built loss for 'order' with n_classes=13, weight_shape=torch.Size([13])\n",
      "[INFO] built loss for 'family' with n_classes=19, weight_shape=torch.Size([19])\n",
      "[INFO] built loss for 'genus' with n_classes=27, weight_shape=torch.Size([27])\n",
      "[INFO] built loss for 'species' with n_classes=183, weight_shape=torch.Size([183])\n",
      "[INFO] optimizer params tensors=18, total_elements=82,947\n",
      "[TRAIN] starting defensive training...\n",
      "Epoch 001 | train_loss=2.6421 | val_agg_f1=0.2617 | epoch_time=0.2s\n",
      "[CHECKPOINT] saved best -> ncbi_blast_db\\extracted\\best_shared_heads_defensive.pt (val_agg_f1=0.2617)\n",
      "Epoch 002 | train_loss=2.4946 | val_agg_f1=0.3416 | epoch_time=0.1s\n",
      "[CHECKPOINT] saved best -> ncbi_blast_db\\extracted\\best_shared_heads_defensive.pt (val_agg_f1=0.3416)\n",
      "Epoch 003 | train_loss=2.1759 | val_agg_f1=0.3708 | epoch_time=0.1s\n",
      "[CHECKPOINT] saved best -> ncbi_blast_db\\extracted\\best_shared_heads_defensive.pt (val_agg_f1=0.3708)\n",
      "Epoch 004 | train_loss=1.7791 | val_agg_f1=0.5532 | epoch_time=0.2s\n",
      "[CHECKPOINT] saved best -> ncbi_blast_db\\extracted\\best_shared_heads_defensive.pt (val_agg_f1=0.5532)\n",
      "Epoch 005 | train_loss=1.4254 | val_agg_f1=0.5994 | epoch_time=0.1s\n",
      "[CHECKPOINT] saved best -> ncbi_blast_db\\extracted\\best_shared_heads_defensive.pt (val_agg_f1=0.5994)\n",
      "Epoch 006 | train_loss=1.1538 | val_agg_f1=0.6754 | epoch_time=0.2s\n",
      "[CHECKPOINT] saved best -> ncbi_blast_db\\extracted\\best_shared_heads_defensive.pt (val_agg_f1=0.6754)\n",
      "Epoch 007 | train_loss=0.9463 | val_agg_f1=0.6767 | epoch_time=0.2s\n",
      "[CHECKPOINT] saved best -> ncbi_blast_db\\extracted\\best_shared_heads_defensive.pt (val_agg_f1=0.6767)\n",
      "Epoch 008 | train_loss=0.8336 | val_agg_f1=0.6803 | epoch_time=0.2s\n",
      "[CHECKPOINT] saved best -> ncbi_blast_db\\extracted\\best_shared_heads_defensive.pt (val_agg_f1=0.6803)\n",
      "Epoch 009 | train_loss=0.6773 | val_agg_f1=0.7132 | epoch_time=0.2s\n",
      "[CHECKPOINT] saved best -> ncbi_blast_db\\extracted\\best_shared_heads_defensive.pt (val_agg_f1=0.7132)\n",
      "Epoch 010 | train_loss=0.5957 | val_agg_f1=0.6753 | epoch_time=0.2s\n",
      "[INFO] no_improve=1/8\n",
      "Epoch 011 | train_loss=0.5179 | val_agg_f1=0.7083 | epoch_time=0.2s\n",
      "[INFO] no_improve=2/8\n",
      "Epoch 012 | train_loss=0.4445 | val_agg_f1=0.7736 | epoch_time=0.3s\n",
      "[CHECKPOINT] saved best -> ncbi_blast_db\\extracted\\best_shared_heads_defensive.pt (val_agg_f1=0.7736)\n",
      "Epoch 013 | train_loss=0.4056 | val_agg_f1=0.7773 | epoch_time=0.2s\n",
      "[CHECKPOINT] saved best -> ncbi_blast_db\\extracted\\best_shared_heads_defensive.pt (val_agg_f1=0.7773)\n",
      "Epoch 014 | train_loss=0.3603 | val_agg_f1=0.7860 | epoch_time=0.3s\n",
      "[CHECKPOINT] saved best -> ncbi_blast_db\\extracted\\best_shared_heads_defensive.pt (val_agg_f1=0.7860)\n",
      "Epoch 015 | train_loss=0.3248 | val_agg_f1=0.7893 | epoch_time=0.3s\n",
      "[CHECKPOINT] saved best -> ncbi_blast_db\\extracted\\best_shared_heads_defensive.pt (val_agg_f1=0.7893)\n",
      "Epoch 016 | train_loss=0.2791 | val_agg_f1=0.7943 | epoch_time=0.3s\n",
      "[CHECKPOINT] saved best -> ncbi_blast_db\\extracted\\best_shared_heads_defensive.pt (val_agg_f1=0.7943)\n",
      "Epoch 017 | train_loss=0.2609 | val_agg_f1=0.8084 | epoch_time=0.2s\n",
      "[CHECKPOINT] saved best -> ncbi_blast_db\\extracted\\best_shared_heads_defensive.pt (val_agg_f1=0.8084)\n",
      "Epoch 018 | train_loss=0.2675 | val_agg_f1=0.8068 | epoch_time=0.3s\n",
      "[INFO] no_improve=1/8\n",
      "Epoch 019 | train_loss=0.2147 | val_agg_f1=0.8069 | epoch_time=0.3s\n",
      "[INFO] no_improve=2/8\n",
      "Epoch 020 | train_loss=0.2116 | val_agg_f1=0.8102 | epoch_time=0.3s\n",
      "[CHECKPOINT] saved best -> ncbi_blast_db\\extracted\\best_shared_heads_defensive.pt (val_agg_f1=0.8102)\n",
      "Epoch 021 | train_loss=0.2026 | val_agg_f1=0.8039 | epoch_time=0.3s\n",
      "[INFO] no_improve=1/8\n",
      "Epoch 022 | train_loss=0.1826 | val_agg_f1=0.8181 | epoch_time=0.3s\n",
      "[CHECKPOINT] saved best -> ncbi_blast_db\\extracted\\best_shared_heads_defensive.pt (val_agg_f1=0.8181)\n",
      "Epoch 023 | train_loss=0.1764 | val_agg_f1=0.8103 | epoch_time=0.3s\n",
      "[INFO] no_improve=1/8\n",
      "Epoch 024 | train_loss=0.1576 | val_agg_f1=0.8030 | epoch_time=0.3s\n",
      "[INFO] no_improve=2/8\n",
      "Epoch 025 | train_loss=0.1570 | val_agg_f1=0.8232 | epoch_time=0.3s\n",
      "[CHECKPOINT] saved best -> ncbi_blast_db\\extracted\\best_shared_heads_defensive.pt (val_agg_f1=0.8232)\n",
      "Epoch 026 | train_loss=0.1470 | val_agg_f1=0.8167 | epoch_time=0.3s\n",
      "[INFO] no_improve=1/8\n",
      "Epoch 027 | train_loss=0.1488 | val_agg_f1=0.8115 | epoch_time=0.2s\n",
      "[INFO] no_improve=2/8\n",
      "Epoch 028 | train_loss=0.1417 | val_agg_f1=0.8106 | epoch_time=0.3s\n",
      "[INFO] no_improve=3/8\n",
      "Epoch 029 | train_loss=0.1365 | val_agg_f1=0.8129 | epoch_time=0.3s\n",
      "[INFO] no_improve=4/8\n",
      "Epoch 030 | train_loss=0.1349 | val_agg_f1=0.8084 | epoch_time=0.3s\n",
      "[INFO] no_improve=5/8\n",
      "Epoch 031 | train_loss=0.1282 | val_agg_f1=0.8089 | epoch_time=0.3s\n",
      "[INFO] no_improve=6/8\n",
      "Epoch 032 | train_loss=0.1206 | val_agg_f1=0.8100 | epoch_time=0.3s\n",
      "[INFO] no_improve=7/8\n",
      "Epoch 033 | train_loss=0.1200 | val_agg_f1=0.8190 | epoch_time=0.4s\n",
      "[INFO] no_improve=8/8\n",
      "[EARLY STOP] stopping.\n",
      "[SAVE] metrics and state_dict written to extracted/\n",
      "[FINAL METRICS]\n",
      "  kingdom    acc=0.8921052631578947 f1_macro=0.890956683627405\n",
      "  phylum     acc=0.8789473684210526 f1_macro=0.9216040354264132\n",
      "  class      acc=0.8763157894736842 f1_macro=0.9265426912896713\n",
      "  order      acc=0.8552631578947368 f1_macro=0.9172273658185996\n",
      "  family     acc=0.8631578947368421 f1_macro=0.8776067405245045\n",
      "  genus      acc=0.8578947368421053 f1_macro=0.8202868283114583\n",
      "  species    acc=0.7736842105263158 f1_macro=0.3789237693820457\n",
      "[COMPLETE] Defensive training finished. total_time_s=8.1\n"
     ]
    }
   ],
   "source": [
    "# Defensive training cell (no crash; detailed diagnostics & guarded backward)\n",
    "import time, json, traceback\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "EXTRACT_DIR = Path(\"ncbi_blast_db\") / \"extracted\"\n",
    "SAVE_CKPT = EXTRACT_DIR / \"best_shared_heads_defensive.pt\"\n",
    "HISTORY_CSV = EXTRACT_DIR / \"training_history_defensive.csv\"\n",
    "METRICS_JSON = EXTRACT_DIR / \"metrics_defensive.json\"\n",
    "\n",
    "LR = 1e-3\n",
    "MAX_EPOCHS = 60\n",
    "PATIENCE = 8\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "HIDDEN_DIM = 256\n",
    "DROPOUT = 0.3\n",
    "BATCH_LOG = 200\n",
    "\n",
    "# ---- sanity globals ----\n",
    "required = [\"train_loader\", \"val_loader\", \"batch_tuple_to_dict\", \"label_encoders\"]\n",
    "miss = [n for n in required if n not in globals()]\n",
    "if miss:\n",
    "    raise RuntimeError(f\"Missing required notebook globals: {miss}. Run the dataset prep cell first.\")\n",
    "\n",
    "train_loader = globals()[\"train_loader\"]\n",
    "val_loader = globals()[\"val_loader\"]\n",
    "batch_tuple_to_dict = globals()[\"batch_tuple_to_dict\"]\n",
    "label_encoders = globals()[\"label_encoders\"]\n",
    "RANKS = list(label_encoders.keys())\n",
    "print(f\"[INFO] device={DEVICE}; ranks={RANKS}\")\n",
    "\n",
    "# ---- infer input dim ----\n",
    "sample_batch = next(iter(train_loader))\n",
    "sample = batch_tuple_to_dict(sample_batch)\n",
    "input_dim = int(sample[\"x\"].shape[1])\n",
    "print(f\"[INFO] inferred input_dim = {input_dim}\")\n",
    "\n",
    "# ---- resilient model: explicit initialize (no kw-arg ctor) ----\n",
    "class ResilientManual(nn.Module):\n",
    "    def _init_(self):\n",
    "        super()._init_()\n",
    "        self._inited = False\n",
    "    def initialize(self, input_dim, hidden_dim, ranks, encoders, dropout=0.0):\n",
    "        if getattr(self, \"_inited\", False):\n",
    "            return\n",
    "        self.ranks = list(ranks)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        h1 = int(hidden_dim)\n",
    "        h2 = max(32, hidden_dim // 2)\n",
    "        # trunk params\n",
    "        self.register_parameter(\"w1\", nn.Parameter(torch.randn(input_dim, h1) * 0.02))\n",
    "        self.register_parameter(\"b1\", nn.Parameter(torch.zeros(h1)))\n",
    "        self.register_parameter(\"w2\", nn.Parameter(torch.randn(h1, h2) * 0.02))\n",
    "        self.register_parameter(\"b2\", nn.Parameter(torch.zeros(h2)))\n",
    "        # heads\n",
    "        for r in self.ranks:\n",
    "            ncls = max(1, len(encoders[r].classes_))\n",
    "            self.register_parameter(f\"head_w__{r}\", nn.Parameter(torch.randn(h2, ncls) * 0.02))\n",
    "            self.register_parameter(f\"head_b__{r}\", nn.Parameter(torch.zeros(ncls)))\n",
    "        self._inited = True\n",
    "    def forward(self, x):\n",
    "        h = x @ self.w1 + self.b1\n",
    "        h = torch.relu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = h @ self.w2 + self.b2\n",
    "        h = torch.relu(h)\n",
    "        out = {}\n",
    "        for r in self.ranks:\n",
    "            w = getattr(self, f\"head_w__{r}\")\n",
    "            b = getattr(self, f\"head_b__{r}\")\n",
    "            out[r] = h @ w + b\n",
    "        return out\n",
    "\n",
    "model = ResilientManual()\n",
    "model.initialize(input_dim=input_dim, hidden_dim=HIDDEN_DIM, ranks=RANKS, encoders=label_encoders, dropout=DROPOUT)\n",
    "model.to(DEVICE)\n",
    "\n",
    "# ---- check params exist ----\n",
    "def param_summary(m):\n",
    "    tot = 0\n",
    "    items = []\n",
    "    for name, p in m.named_parameters():\n",
    "        items.append((name, tuple(p.shape), p.requires_grad, p.numel()))\n",
    "        tot += p.numel()\n",
    "    return tot, items\n",
    "\n",
    "tot_params, params_list = param_summary(model)\n",
    "print(f\"[INFO] total model parameters = {tot_params:,}; param tensors = {len(params_list)}\")\n",
    "if tot_params == 0:\n",
    "    for it in params_list:\n",
    "        print(\"PARAM\", it)\n",
    "    raise RuntimeError(\"Model has zero parameters; aborting.\")\n",
    "\n",
    "# ---- derive train label arrays (for weights) ----\n",
    "def derive_train_labels():\n",
    "    if \"train_dataset\" in globals():\n",
    "        td = globals()[\"train_dataset\"]\n",
    "        tensors = td.tensors\n",
    "        out = {r: tensors[1 + i].cpu().numpy() for i, r in enumerate(RANKS)}\n",
    "        return out\n",
    "    if \"y_encoded\" in globals() and \"train_idx\" in globals():\n",
    "        ti = globals()[\"train_idx\"]\n",
    "        return {r: np.asarray(globals()[\"y_encoded\"][r])[ti] for r in RANKS}\n",
    "    # fallback sample a few train batches\n",
    "    collected = {r: [] for r in RANKS}\n",
    "    n=0\n",
    "    for bt in train_loader:\n",
    "        b = batch_tuple_to_dict(bt)\n",
    "        for r in RANKS:\n",
    "            collected[r].append(b[r].cpu().numpy())\n",
    "        n+=1\n",
    "        if n>=10: break\n",
    "    return {r: np.concatenate(collected[r]) for r in RANKS}\n",
    "\n",
    "train_labels = derive_train_labels()\n",
    "\n",
    "# ---- build criterions robustly (weights on DEVICE, length = encoder classes) ----\n",
    "criterions = {}\n",
    "for r in RANKS:\n",
    "    n_classes = max(1, len(label_encoders[r].classes_))\n",
    "    if n_classes <= 1:\n",
    "        criterions[r] = None\n",
    "        print(f\"[INFO] skipping loss for '{r}' (encoder has only 1 class).\")\n",
    "        continue\n",
    "    arr = train_labels.get(r)\n",
    "    if arr is None:\n",
    "        counts_full = np.ones((n_classes,), dtype=float)\n",
    "    else:\n",
    "        counts_full = np.bincount(arr, minlength=n_classes).astype(float)\n",
    "        counts_full[counts_full == 0] = 1.0\n",
    "    inv = 1.0 / counts_full\n",
    "    inv = inv / np.mean(inv)\n",
    "    weight_tensor = torch.tensor(inv.astype(np.float32), device=DEVICE)\n",
    "    criterions[r] = nn.CrossEntropyLoss(weight=weight_tensor)\n",
    "    print(f\"[INFO] built loss for '{r}' with n_classes={n_classes}, weight_shape={weight_tensor.shape}\")\n",
    "\n",
    "# ---- optimizer ----\n",
    "params = [p for p in model.parameters() if p.requires_grad and p.numel()>0]\n",
    "optimizer = torch.optim.Adam(params, lr=LR)\n",
    "print(f\"[INFO] optimizer params tensors={len(params)}, total_elements={sum(p.numel() for p in params):,}\")\n",
    "\n",
    "# ---- helpers ----\n",
    "def evaluate(m, loader):\n",
    "    m.eval()\n",
    "    preds = {r: [] for r in RANKS}\n",
    "    trues = {r: [] for r in RANKS}\n",
    "    with torch.no_grad():\n",
    "        for bt in loader:\n",
    "            b = batch_tuple_to_dict(bt)\n",
    "            x = b[\"x\"].to(DEVICE)\n",
    "            out = m(x)\n",
    "            for r in RANKS:\n",
    "                preds[r].append(np.argmax(out[r].cpu().numpy(), axis=1))\n",
    "                trues[r].append(b[r].cpu().numpy())\n",
    "    metrics = {}\n",
    "    for r in RANKS:\n",
    "        if not preds[r]:\n",
    "            metrics[r] = {\"accuracy\": None, \"f1_macro\": None}\n",
    "            continue\n",
    "        p = np.concatenate(preds[r])\n",
    "        t = np.concatenate(trues[r])\n",
    "        metrics[r] = {\"accuracy\": float(accuracy_score(t,p)), \"f1_macro\": float(f1_score(t,p,average=\"macro\", zero_division=0))}\n",
    "    return metrics\n",
    "\n",
    "def safe_train_one_epoch(m, loader, opt):\n",
    "    m.train()\n",
    "    total_loss = 0.0\n",
    "    nbatches = 0\n",
    "    for batch_i, bt in enumerate(loader, start=1):\n",
    "        b = batch_tuple_to_dict(bt)\n",
    "        x = b[\"x\"].to(DEVICE)\n",
    "        # targets\n",
    "        targets = {r: b[r].to(DEVICE) for r in RANKS}\n",
    "        out = m(x)\n",
    "        # diagnostics per rank\n",
    "        losses = []\n",
    "        skipped_ranks = []\n",
    "        for r in RANKS:\n",
    "            if criterions[r] is None:\n",
    "                skipped_ranks.append(r); continue\n",
    "            logits = out[r]\n",
    "            targ = targets[r]\n",
    "            # quick checks\n",
    "            if not isinstance(logits, torch.Tensor):\n",
    "                skipped_ranks.append(r); continue\n",
    "            if not logits.requires_grad:\n",
    "                # report but skip\n",
    "                skipped_ranks.append(r)\n",
    "                print(f\"[WARN] batch {batch_i}: out[{r}].requires_grad=False; skipping this rank for this batch\")\n",
    "                continue\n",
    "            try:\n",
    "                l = criterions[r](logits, targ)\n",
    "            except Exception as e:\n",
    "                # shape / device / dtype mismatch: report and skip\n",
    "                print(f\"[ERROR] batch {batch_i}: loss compute failed for rank {r}: {e}; shapes logits {tuple(logits.shape)}, target {tuple(targ.shape)}\")\n",
    "                skipped_ranks.append(r)\n",
    "                continue\n",
    "            if not isinstance(l, torch.Tensor) or not l.requires_grad:\n",
    "                print(f\"[WARN] batch {batch_i}: loss for {r} does not require grad; skipping. loss type {type(l)}, requires_grad={getattr(l,'requires_grad',None)}\")\n",
    "                skipped_ranks.append(r)\n",
    "                continue\n",
    "            losses.append(l)\n",
    "        if not losses:\n",
    "            # nothing to backprop this batch\n",
    "            if batch_i % BATCH_LOG == 0:\n",
    "                print(f\"[INFO] batch {batch_i}: no rank produced a backprop-able loss; skipping optimizer step.\")\n",
    "            continue\n",
    "        loss = sum(losses) / len(losses)  # average across ranks for numeric stability\n",
    "        opt.zero_grad()\n",
    "        try:\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        except Exception as e:\n",
    "            # capture diagnostics and skip this step (do not crash)\n",
    "            print(\"[CRITICAL] backward() failed for this batch. Dumping diagnostics and skipping optimizer step for this batch.\")\n",
    "            traceback.print_exc()\n",
    "            print(\"Diagnostics:\")\n",
    "            print(\" - loss.requires_grad:\", getattr(loss, \"requires_grad\", None))\n",
    "            for name,p in m.named_parameters():\n",
    "                print(f\"   param {name}: requires_grad={p.requires_grad}, shape={tuple(p.shape)}, device={p.device}\")\n",
    "            for r in RANKS:\n",
    "                lo = out[r]\n",
    "                print(f\"   out[{r}] requires_grad={getattr(lo,'requires_grad',None)}, dtype={getattr(lo,'dtype',None)}, shape={tuple(lo.shape)}\")\n",
    "            # skip optimizer update this batch\n",
    "            continue\n",
    "        total_loss += float(loss.item())\n",
    "        nbatches += 1\n",
    "        if batch_i % BATCH_LOG == 0:\n",
    "            print(f\"[INFO] batch {batch_i}, avg loss so far = {total_loss/max(1,nbatches):.4f}, skipped_ranks={skipped_ranks[:5]}\")\n",
    "    return total_loss / max(1, nbatches)\n",
    "\n",
    "# ---- training loop with early stopping ----\n",
    "best_val = -1.0\n",
    "best_ckpt = None\n",
    "history = []\n",
    "no_improve = 0\n",
    "\n",
    "print(\"[TRAIN] starting defensive training...\")\n",
    "t0_all = time.time()\n",
    "for epoch in range(1, MAX_EPOCHS+1):\n",
    "    t_epoch = time.time()\n",
    "    train_loss = safe_train_one_epoch(model, train_loader, optimizer)\n",
    "    val_metrics = evaluate(model, val_loader)\n",
    "    f1s = [v[\"f1_macro\"] if v[\"f1_macro\"] is not None else 0.0 for v in val_metrics.values()]\n",
    "    val_agg = float(np.mean(f1s))\n",
    "    epoch_time = time.time() - t_epoch\n",
    "    history.append({\"epoch\": epoch, \"train_loss\": float(train_loss), \"val_agg_f1\": float(val_agg), \"time_s\": float(epoch_time)})\n",
    "    print(f\"Epoch {epoch:03d} | train_loss={train_loss:.4f} | val_agg_f1={val_agg:.4f} | epoch_time={epoch_time:.1f}s\")\n",
    "    # checkpoint\n",
    "    if val_agg > best_val + 1e-9:\n",
    "        best_val = val_agg\n",
    "        best_ckpt = {\"epoch\": epoch, \"val_agg_f1\": val_agg, \"model_state\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()}\n",
    "        torch.save(best_ckpt, str(SAVE_CKPT))\n",
    "        print(f\"[CHECKPOINT] saved best -> {SAVE_CKPT} (val_agg_f1={val_agg:.4f})\")\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        print(f\"[INFO] no_improve={no_improve}/{PATIENCE}\")\n",
    "        if no_improve >= PATIENCE:\n",
    "            print(\"[EARLY STOP] stopping.\")\n",
    "            break\n",
    "\n",
    "tot_time = time.time() - t0_all\n",
    "pd.DataFrame(history).to_csv(HISTORY_CSV, index=False)\n",
    "if best_ckpt is not None:\n",
    "    model.load_state_dict(best_ckpt[\"model_state\"])\n",
    "    final_metrics = evaluate(model, val_loader)\n",
    "    out = {\"best_epoch\": best_ckpt[\"epoch\"], \"best_val_agg_f1\": best_ckpt[\"val_agg_f1\"], \"per_rank\": final_metrics}\n",
    "    with open(METRICS_JSON, \"w\") as fh:\n",
    "        json.dump(out, fh, indent=2)\n",
    "    torch.save(model.state_dict(), EXTRACT_DIR / \"best_shared_heads_defensive_state_dict.pt\")\n",
    "    print(\"[SAVE] metrics and state_dict written to extracted/\")\n",
    "    print(\"[FINAL METRICS]\")\n",
    "    for r in RANKS:\n",
    "        m = final_metrics[r]\n",
    "        print(f\"  {r:10s} acc={m['accuracy']} f1_macro={m['f1_macro']}\")\n",
    "else:\n",
    "    print(\"[WARN] No checkpoint saved during training.\")\n",
    "print(\"[COMPLETE] Defensive training finished. total_time_s={:.1f}\".format(tot_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ce3df81-381a-485d-8d5c-e5323c6cf70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_grad_enabled: True\n",
      "w1 shape (64, 256) requires_grad True device cpu\n",
      "b1 shape (256,) requires_grad True device cpu\n",
      "w2 shape (256, 128) requires_grad True device cpu\n",
      "b2 shape (128,) requires_grad True device cpu\n",
      "head_w__kingdom shape (128, 2) requires_grad True device cpu\n",
      "head_b__kingdom shape (2,) requires_grad True device cpu\n",
      "head_w__phylum shape (128, 5) requires_grad True device cpu\n",
      "head_b__phylum shape (5,) requires_grad True device cpu\n",
      "head_w__class shape (128, 10) requires_grad True device cpu\n",
      "head_b__class shape (10,) requires_grad True device cpu\n",
      "head_w__order shape (128, 13) requires_grad True device cpu\n",
      "head_b__order shape (13,) requires_grad True device cpu\n",
      "head_w__family shape (128, 19) requires_grad True device cpu\n",
      "head_b__family shape (19,) requires_grad True device cpu\n",
      "head_w__genus shape (128, 27) requires_grad True device cpu\n",
      "head_b__genus shape (27,) requires_grad True device cpu\n",
      "head_w__species shape (128, 183) requires_grad True device cpu\n",
      "head_b__species shape (183,) requires_grad True device cpu\n",
      "kingdom out.requires_grad True dtype torch.float32 shape torch.Size([128, 2])\n",
      "phylum out.requires_grad True dtype torch.float32 shape torch.Size([128, 5])\n",
      "class out.requires_grad True dtype torch.float32 shape torch.Size([128, 10])\n",
      "order out.requires_grad True dtype torch.float32 shape torch.Size([128, 13])\n",
      "family out.requires_grad True dtype torch.float32 shape torch.Size([128, 19])\n",
      "genus out.requires_grad True dtype torch.float32 shape torch.Size([128, 27])\n",
      "species out.requires_grad True dtype torch.float32 shape torch.Size([128, 183])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"is_grad_enabled:\", torch.is_grad_enabled())\n",
    "for name, p in model.named_parameters():\n",
    "    print(name, \"shape\", tuple(p.shape), \"requires_grad\", p.requires_grad, \"device\", p.device)\n",
    "# run one forward and inspect:\n",
    "batch = next(iter(train_loader))\n",
    "b = batch_tuple_to_dict(batch)\n",
    "x = b[\"x\"]\n",
    "out = model(x.to(next(model.parameters()).device))\n",
    "for r in out: print(r, \"out.requires_grad\", out[r].requires_grad, \"dtype\", out[r].dtype, \"shape\", out[r].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8503671-9902-4773-be45-fa13383a769f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_grad_enabled: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.set_grad_enabled(True)\n",
    "print(\"is_grad_enabled:\", torch.is_grad_enabled())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b4832b0e-7098-4042-97f4-d564e380e057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.is_grad_enabled() True\n",
      "Loaded model_state from checkpoint (key 'model_state').\n",
      "Model placed in train() mode.\n",
      "Example param requires_grad flags:\n",
      "  w1 requires_grad= True shape= (64, 256)\n",
      "  b1 requires_grad= True shape= (256,)\n",
      "  w2 requires_grad= True shape= (256, 128)\n",
      "  b2 requires_grad= True shape= (128,)\n",
      "  head_w__kingdom requires_grad= True shape= (128, 2)\n",
      "  head_b__kingdom requires_grad= True shape= (2,)\n",
      "  head_w__phylum requires_grad= True shape= (128, 5)\n",
      "  head_b__phylum requires_grad= True shape= (5,)\n",
      "  head_w__class requires_grad= True shape= (128, 10)\n",
      "  head_b__class requires_grad= True shape= (10,)\n",
      "  head_w__order requires_grad= True shape= (128, 13)\n",
      "  head_b__order requires_grad= True shape= (13,)\n",
      "  head_w__family requires_grad= True shape= (128, 19)\n",
      "  head_b__family requires_grad= True shape= (19,)\n",
      "  head_w__genus requires_grad= True shape= (128, 27)\n",
      "  head_b__genus requires_grad= True shape= (27,)\n",
      "  head_w__species requires_grad= True shape= (128, 183)\n",
      "  head_b__species requires_grad= True shape= (183,)\n",
      "out[kingdom].requires_grad -> True shape (128, 2)\n",
      "out[phylum].requires_grad -> True shape (128, 5)\n",
      "out[class].requires_grad -> True shape (128, 10)\n",
      "out[order].requires_grad -> True shape (128, 13)\n",
      "out[family].requires_grad -> True shape (128, 19)\n",
      "out[genus].requires_grad -> True shape (128, 27)\n",
      "out[species].requires_grad -> True shape (128, 183)\n",
      "Optimizer created with 82947 params.\n",
      "Loaded optimizer state from checkpoint.\n",
      "[RESUME TRAIN] Starting...\n",
      "Epoch 001 | train_loss=1.5992 | val_agg_f1=0.7863 | time=0.15s\n",
      "Saved checkpoint -> ncbi_blast_db\\extracted\\best_shared_heads_resumed.pt\n",
      "Epoch 002 | train_loss=1.9338 | val_agg_f1=0.7913 | time=0.14s\n",
      "Saved checkpoint -> ncbi_blast_db\\extracted\\best_shared_heads_resumed.pt\n",
      "Epoch 003 | train_loss=1.5311 | val_agg_f1=0.7813 | time=0.14s\n",
      "no_improve=1/6\n",
      "Epoch 004 | train_loss=1.1386 | val_agg_f1=0.8083 | time=0.14s\n",
      "Saved checkpoint -> ncbi_blast_db\\extracted\\best_shared_heads_resumed.pt\n",
      "Epoch 005 | train_loss=0.9871 | val_agg_f1=0.8115 | time=0.22s\n",
      "Saved checkpoint -> ncbi_blast_db\\extracted\\best_shared_heads_resumed.pt\n",
      "Epoch 006 | train_loss=0.8125 | val_agg_f1=0.8206 | time=0.13s\n",
      "Saved checkpoint -> ncbi_blast_db\\extracted\\best_shared_heads_resumed.pt\n",
      "Epoch 007 | train_loss=0.8331 | val_agg_f1=0.8156 | time=0.14s\n",
      "no_improve=1/6\n",
      "Epoch 008 | train_loss=0.7237 | val_agg_f1=0.8200 | time=0.16s\n",
      "no_improve=2/6\n",
      "Epoch 009 | train_loss=0.7177 | val_agg_f1=0.8258 | time=0.13s\n",
      "Saved checkpoint -> ncbi_blast_db\\extracted\\best_shared_heads_resumed.pt\n",
      "Epoch 010 | train_loss=0.7116 | val_agg_f1=0.8201 | time=0.13s\n",
      "no_improve=1/6\n",
      "Epoch 011 | train_loss=0.7076 | val_agg_f1=0.8092 | time=0.19s\n",
      "no_improve=2/6\n",
      "Epoch 012 | train_loss=0.7861 | val_agg_f1=0.8140 | time=0.18s\n",
      "no_improve=3/6\n",
      "Epoch 013 | train_loss=0.6530 | val_agg_f1=0.8196 | time=0.20s\n",
      "no_improve=4/6\n",
      "Epoch 014 | train_loss=0.6095 | val_agg_f1=0.8231 | time=0.20s\n",
      "no_improve=5/6\n",
      "Epoch 015 | train_loss=0.5727 | val_agg_f1=0.8224 | time=0.23s\n",
      "no_improve=6/6\n",
      "Early stopping.\n",
      "Training finished. Best val_agg_f1: 0.8258221402136418\n",
      "Final per-rank metrics:\n",
      "  kingdom      acc=0.8947368421052632 f1_macro=0.8937835420393561\n",
      "  phylum       acc=0.868421052631579 f1_macro=0.9147122628312452\n",
      "  class        acc=0.8710526315789474 f1_macro=0.9280281605639296\n",
      "  order        acc=0.8526315789473684 f1_macro=0.9236803450982068\n",
      "  family       acc=0.8578947368421053 f1_macro=0.8974543935975615\n",
      "  genus        acc=0.8552631578947368 f1_macro=0.8129860766131257\n",
      "  species      acc=0.7842105263157895 f1_macro=0.3860266012437868\n"
     ]
    }
   ],
   "source": [
    "# Resume training — robust, forces grad on, loads checkpoint, trains safely\n",
    "import time, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- Config ----------\n",
    "EXTRACT_DIR = Path(\"ncbi_blast_db\") / \"extracted\"\n",
    "CKPT = EXTRACT_DIR / \"best_shared_heads_defensive.pt\"\n",
    "HISTORY_CSV = EXTRACT_DIR / \"training_history_resumed.csv\"\n",
    "SAVE_CKPT = EXTRACT_DIR / \"best_shared_heads_resumed.pt\"\n",
    "\n",
    "LR = 1e-4            # smaller LR when resuming often helps\n",
    "MAX_EPOCHS = 40\n",
    "PATIENCE = 6\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "HIDDEN_DIM = 256\n",
    "DROPOUT = 0.3\n",
    "BATCH_LOG = 200\n",
    "\n",
    "# ---------- sanity and required globals ----------\n",
    "required = [\"train_loader\", \"val_loader\", \"batch_tuple_to_dict\", \"label_encoders\"]\n",
    "missing = [n for n in required if n not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Missing required globals: {missing} — run the dataset prep cell first.\")\n",
    "\n",
    "train_loader = globals()[\"train_loader\"]\n",
    "val_loader = globals()[\"val_loader\"]\n",
    "batch_tuple_to_dict = globals()[\"batch_tuple_to_dict\"]\n",
    "label_encoders = globals()[\"label_encoders\"]\n",
    "RANKS = list(label_encoders.keys())\n",
    "\n",
    "# ---------- Force-enable global grad (the actual fix) ----------\n",
    "torch.set_grad_enabled(True)\n",
    "print(\"torch.is_grad_enabled()\", torch.is_grad_enabled())\n",
    "\n",
    "# ---------- Recreate model class (no kwargs in ctor; safe initialize) ----------\n",
    "class ResilientModel(nn.Module):\n",
    "    def _init_(self):\n",
    "        super()._init_()\n",
    "        self._inited = False\n",
    "    def initialize(self, input_dim, hidden_dim, ranks, encoders, dropout=0.0):\n",
    "        if getattr(self,\"_inited\",False): return\n",
    "        self.ranks = list(ranks)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        h1 = int(hidden_dim)\n",
    "        h2 = max(32, h1 // 2)\n",
    "        self.register_parameter(\"w1\", nn.Parameter(torch.randn(input_dim, h1) * 0.02))\n",
    "        self.register_parameter(\"b1\", nn.Parameter(torch.zeros(h1)))\n",
    "        self.register_parameter(\"w2\", nn.Parameter(torch.randn(h1, h2) * 0.02))\n",
    "        self.register_parameter(\"b2\", nn.Parameter(torch.zeros(h2)))\n",
    "        for r in self.ranks:\n",
    "            ncls = max(1, len(encoders[r].classes_))\n",
    "            self.register_parameter(f\"head_w__{r}\", nn.Parameter(torch.randn(h2, ncls) * 0.02))\n",
    "            self.register_parameter(f\"head_b__{r}\", nn.Parameter(torch.zeros(ncls)))\n",
    "        self._inited = True\n",
    "    def forward(self, x):\n",
    "        h = x @ self.w1 + self.b1\n",
    "        h = torch.relu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = h @ self.w2 + self.b2\n",
    "        h = torch.relu(h)\n",
    "        out = {}\n",
    "        for r in self.ranks:\n",
    "            w = getattr(self, f\"head_w__{r}\")\n",
    "            b = getattr(self, f\"head_b__{r}\")\n",
    "            out[r] = h @ w + b\n",
    "        return out\n",
    "\n",
    "# ---------- instantiate model and load checkpoint ----------\n",
    "# infer input dim from a loader batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "sample = batch_tuple_to_dict(sample_batch)\n",
    "input_dim = int(sample[\"x\"].shape[1])\n",
    "\n",
    "model = ResilientModel()\n",
    "model.initialize(input_dim=input_dim, hidden_dim=HIDDEN_DIM, ranks=RANKS, encoders=label_encoders, dropout=DROPOUT)\n",
    "model.to(DEVICE)\n",
    "\n",
    "# load checkpoint state if available\n",
    "if CKPT.exists():\n",
    "    ckpt = torch.load(str(CKPT), map_location=DEVICE)\n",
    "    # checkpoint may be dict with 'model_state'\n",
    "    if isinstance(ckpt, dict) and \"model_state\" in ckpt:\n",
    "        model.load_state_dict(ckpt[\"model_state\"])\n",
    "        print(\"Loaded model_state from checkpoint (key 'model_state').\")\n",
    "    else:\n",
    "        # assume ckpt is state_dict\n",
    "        model.load_state_dict(ckpt)\n",
    "        print(\"Loaded checkpoint as state_dict.\")\n",
    "else:\n",
    "    print(\"No checkpoint found; training from scratch.\")\n",
    "\n",
    "# ---------- Verify grad-able outputs and params before training ----------\n",
    "model.train()  # enable dropout + training behavior\n",
    "print(\"Model placed in train() mode.\")\n",
    "print(\"Example param requires_grad flags:\")\n",
    "for name,p in model.named_parameters():\n",
    "    print(\" \", name, \"requires_grad=\", p.requires_grad, \"shape=\", tuple(p.shape))\n",
    "\n",
    "# quick forward sanity check: run one batch and ensure outputs require grad\n",
    "bt = next(iter(train_loader))\n",
    "b = batch_tuple_to_dict(bt)\n",
    "x = b[\"x\"].to(DEVICE)\n",
    "out = model(x)\n",
    "for r in RANKS:\n",
    "    print(f\"out[{r}].requires_grad ->\", getattr(out[r], \"requires_grad\", None), \"shape\", tuple(out[r].shape))\n",
    "\n",
    "# If outputs do not require grad here, something external (very rare) is disabling grad.\n",
    "if not torch.is_grad_enabled():\n",
    "    raise RuntimeError(\"torch.is_grad_enabled() is False after set_grad_enabled(True) — cannot proceed.\")\n",
    "\n",
    "# ---------- build criterions (use encoder class counts) ----------\n",
    "criterions = {}\n",
    "for r in RANKS:\n",
    "    n_classes = max(1, len(label_encoders[r].classes_))\n",
    "    if n_classes <= 1:\n",
    "        criterions[r] = None\n",
    "    else:\n",
    "        # compute weight robustly using training labels if available\n",
    "        try:\n",
    "            if \"train_dataset\" in globals():\n",
    "                td = globals()[\"train_dataset\"]\n",
    "                arr = td.tensors[1 + RANKS.index(r)].cpu().numpy()\n",
    "            elif \"y_encoded\" in globals() and \"train_idx\" in globals():\n",
    "                arr = np.asarray(globals()[\"y_encoded\"][r])[globals()[\"train_idx\"]]\n",
    "            else:\n",
    "                # fallback: sample few batches\n",
    "                arr = np.concatenate([batch_tuple_to_dict(bt)[r].cpu().numpy() for i, bt in zip(range(10), train_loader)])\n",
    "        except Exception:\n",
    "            arr = np.zeros((1,), dtype=int)\n",
    "        counts = np.bincount(arr, minlength=n_classes).astype(float)\n",
    "        counts[counts == 0] = 1.0\n",
    "        inv = 1.0 / counts\n",
    "        inv = inv / inv.mean()\n",
    "        weight = torch.from_numpy(inv.astype(np.float32)).to(DEVICE)\n",
    "        criterions[r] = nn.CrossEntropyLoss(weight=weight)\n",
    "\n",
    "# ---------- optimizer (recreate) ----------\n",
    "optim_params = [p for p in model.parameters() if p.requires_grad and p.numel() > 0]\n",
    "optimizer = optim.Adam(optim_params, lr=LR)\n",
    "print(\"Optimizer created with\", sum(p.numel() for p in optim_params), \"params.\")\n",
    "\n",
    "# optionally resume optimizer state from checkpoint\n",
    "if CKPT.exists() and isinstance(ckpt, dict) and \"optimizer_state\" in ckpt:\n",
    "    try:\n",
    "        optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n",
    "        print(\"Loaded optimizer state from checkpoint.\")\n",
    "    except Exception as e:\n",
    "        print(\"Warning: failed to load optimizer state:\", e)\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def evaluate(m, loader):\n",
    "    m.eval()\n",
    "    preds = {r: [] for r in RANKS}\n",
    "    trues = {r: [] for r in RANKS}\n",
    "    with torch.no_grad():\n",
    "        for bt in loader:\n",
    "            b = batch_tuple_to_dict(bt)\n",
    "            x = b[\"x\"].to(DEVICE)\n",
    "            out = m(x)\n",
    "            for r in RANKS:\n",
    "                preds[r].append(out[r].argmax(dim=1).cpu().numpy())\n",
    "                trues[r].append(b[r].cpu().numpy())\n",
    "    metrics = {}\n",
    "    for r in RANKS:\n",
    "        if not preds[r]:\n",
    "            metrics[r] = {\"accuracy\": None, \"f1_macro\": None}\n",
    "            continue\n",
    "        p = np.concatenate(preds[r]); t = np.concatenate(trues[r])\n",
    "        metrics[r] = {\"accuracy\": float(accuracy_score(t,p)), \"f1_macro\": float(f1_score(t,p,average=\"macro\", zero_division=0))}\n",
    "    return metrics\n",
    "\n",
    "def train_one_epoch(m, loader, opt):\n",
    "    m.train()\n",
    "    total_loss = 0.0\n",
    "    nb = 0\n",
    "    for bt in loader:\n",
    "        b = batch_tuple_to_dict(bt)\n",
    "        x = b[\"x\"].to(DEVICE)\n",
    "        out = m(x)\n",
    "        loss = None\n",
    "        for r in RANKS:\n",
    "            if criterions[r] is None: continue\n",
    "            l = criterions[r](out[r], b[r].to(DEVICE))\n",
    "            loss = l if loss is None else loss + l\n",
    "        if loss is None:\n",
    "            continue\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total_loss += float(loss.item())\n",
    "        nb += 1\n",
    "    return total_loss / max(1, nb)\n",
    "\n",
    "# ---------- training loop (resume) ----------\n",
    "best_val = -1.0\n",
    "best_ckpt = None\n",
    "history = []\n",
    "no_improve = 0\n",
    "\n",
    "print(\"[RESUME TRAIN] Starting...\")\n",
    "tstart = time.time()\n",
    "for epoch in range(1, MAX_EPOCHS+1):\n",
    "    t0 = time.time()\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer)\n",
    "    val_metrics = evaluate(model, val_loader)\n",
    "    f1s = [v[\"f1_macro\"] if v[\"f1_macro\"] is not None else 0.0 for v in val_metrics.values()]\n",
    "    val_agg = float(np.mean(f1s))\n",
    "    epoch_time = time.time() - t0\n",
    "    history.append({\"epoch\": epoch, \"train_loss\": float(train_loss), \"val_agg_f1\": float(val_agg), \"time_s\": float(epoch_time)})\n",
    "    print(f\"Epoch {epoch:03d} | train_loss={train_loss:.4f} | val_agg_f1={val_agg:.4f} | time={epoch_time:.2f}s\")\n",
    "    if val_agg > best_val + 1e-8:\n",
    "        best_val = val_agg\n",
    "        best_ckpt = {\"epoch\": epoch, \"val_agg_f1\": val_agg, \"model_state\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()}\n",
    "        torch.save(best_ckpt, SAVE_CKPT)\n",
    "        print(\"Saved checkpoint ->\", SAVE_CKPT)\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        print(f\"no_improve={no_improve}/{PATIENCE}\")\n",
    "        if no_improve >= PATIENCE:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "\n",
    "# save history and final metrics\n",
    "pd.DataFrame(history).to_csv(HISTORY_CSV, index=False)\n",
    "if best_ckpt:\n",
    "    model.load_state_dict(best_ckpt[\"model_state\"])\n",
    "    final_metrics = evaluate(model, val_loader)\n",
    "    with open(EXTRACT_DIR / \"metrics_resumed.json\", \"w\") as fh:\n",
    "        json.dump({\"best_epoch\": best_ckpt[\"epoch\"], \"best_val_agg_f1\": best_ckpt[\"val_agg_f1\"], \"per_rank\": final_metrics}, fh, indent=2)\n",
    "    torch.save(model.state_dict(), EXTRACT_DIR / \"best_shared_heads_resumed_state_dict.pt\")\n",
    "    print(\"Training finished. Best val_agg_f1:\", best_ckpt[\"val_agg_f1\"])\n",
    "    print(\"Final per-rank metrics:\")\n",
    "    for r in RANKS:\n",
    "        m = final_metrics[r]\n",
    "        print(f\"  {r:12s} acc={m['accuracy']} f1_macro={m['f1_macro']}\")\n",
    "else:\n",
    "    print(\"Training finished but no checkpoint saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d521a71d-bc13-479b-a928-1c6944a2f197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CAL] device: cpu\n",
      "[CAL] loaded checkpoint: ncbi_blast_db\\extracted\\best_shared_heads_resumed.pt\n",
      "[CAL] model loaded and moved to device; eval() mode set.\n",
      "[CAL] collected logits from validation set: total samples ~ 380\n",
      "[CAL] fitting T for rank=kingdom (N=380, ncls=2) ... done (T=1.2370, took 0.01s)\n",
      "[CAL] fitting T for rank=phylum (N=380, ncls=5) ... done (T=1.2833, took 0.00s)\n",
      "[CAL] fitting T for rank=class (N=380, ncls=10) ... done (T=1.2860, took 0.01s)\n",
      "[CAL] fitting T for rank=order (N=380, ncls=13) ... done (T=1.4398, took 0.00s)\n",
      "[CAL] fitting T for rank=family (N=380, ncls=19) ... done (T=1.4526, took 0.01s)\n",
      "[CAL] fitting T for rank=genus (N=380, ncls=27) ... done (T=1.5589, took 0.01s)\n",
      "[CAL] fitting T for rank=species (N=380, ncls=183) ... done (T=2.0711, took 0.01s)\n",
      "[CAL] temperature fitting completed in 0.05s\n",
      "[CAL] saved temperatures to: ncbi_blast_db\\extracted\\temp_scaling_by_rank.json\n",
      "\n",
      "[CAL] per-rank NLL/accuracy before -> after (smaller NLL is better)\n",
      "  kingdom   : pre_nll=0.2337, post_nll=0.2290  | pre_acc=0.8868, post_acc=0.8868  T=1.2370\n",
      "  phylum    : pre_nll=0.2527, post_nll=0.2445  | pre_acc=0.8789, post_acc=0.8789  T=1.2833\n",
      "  class     : pre_nll=0.2629, post_nll=0.2539  | pre_acc=0.8684, post_acc=0.8684  T=1.2860\n",
      "  order     : pre_nll=0.3331, post_nll=0.3039  | pre_acc=0.8684, post_acc=0.8684  T=1.4398\n",
      "  family    : pre_nll=0.3462, post_nll=0.3133  | pre_acc=0.8684, post_acc=0.8684  T=1.4526\n",
      "  genus     : pre_nll=0.4159, post_nll=0.3549  | pre_acc=0.8658, post_acc=0.8658  T=1.5589\n",
      "  species   : pre_nll=2.3308, post_nll=1.6086  | pre_acc=0.7737, post_acc=0.7737  T=2.0711\n",
      "[CAL] wrote calibrated validation predictions to: ncbi_blast_db\\extracted\\val_predictions_calibrated.csv\n",
      "[CAL] Done. Per-rank temperatures:\n",
      "  kingdom      -> T=1.2370\n",
      "  phylum       -> T=1.2833\n",
      "  class        -> T=1.2860\n",
      "  order        -> T=1.4398\n",
      "  family       -> T=1.4526\n",
      "  genus        -> T=1.5589\n",
      "  species      -> T=2.0711\n"
     ]
    }
   ],
   "source": [
    "# Calibration cell (temperature scaling per-rank)\n",
    "# Paste & run this in the same notebook where train/val loaders and label_encoders exist.\n",
    "\n",
    "import time, json, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- Config / output paths ----------\n",
    "EXTRACT_DIR = Path(\"ncbi_blast_db\") / \"extracted\"\n",
    "CKPT_PATH = EXTRACT_DIR / \"best_shared_heads_resumed.pt\"   # trained checkpoint from your resumed training\n",
    "OUT_TEMPS = EXTRACT_DIR / \"temp_scaling_by_rank.json\"\n",
    "OUT_VAL_CALIB = EXTRACT_DIR / \"val_predictions_calibrated.csv\"\n",
    "\n",
    "# ---------- required globals sanity ----------\n",
    "required = [\"val_loader\", \"batch_tuple_to_dict\", \"label_encoders\"]\n",
    "missing = [n for n in required if n not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Missing notebook globals required for calibration: {missing}. Run dataset prep and training cells first.\")\n",
    "\n",
    "val_loader = globals()[\"val_loader\"]\n",
    "batch_tuple_to_dict = globals()[\"batch_tuple_to_dict\"]\n",
    "label_encoders = globals()[\"label_encoders\"]\n",
    "RANKS = list(label_encoders.keys())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"[CAL] device:\", device)\n",
    "torch.set_grad_enabled(True)  # ensure grads are enabled for LBFGS / param updates\n",
    "\n",
    "# ---------- model reconstruction (must match training model) ----------\n",
    "class ResilientModel(nn.Module):\n",
    "    def _init_(self):\n",
    "        super()._init_()\n",
    "        self._inited = False\n",
    "    def initialize(self, input_dim, hidden_dim, ranks, encoders, dropout=0.3):\n",
    "        if getattr(self, \"_inited\", False):\n",
    "            return\n",
    "        self.ranks = list(ranks)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        h1 = int(hidden_dim)\n",
    "        h2 = max(32, h1 // 2)\n",
    "        self.register_parameter(\"w1\", nn.Parameter(torch.randn(input_dim, h1) * 0.02))\n",
    "        self.register_parameter(\"b1\", nn.Parameter(torch.zeros(h1)))\n",
    "        self.register_parameter(\"w2\", nn.Parameter(torch.randn(h1, h2) * 0.02))\n",
    "        self.register_parameter(\"b2\", nn.Parameter(torch.zeros(h2)))\n",
    "        for r in self.ranks:\n",
    "            ncls = max(1, len(encoders[r].classes_))\n",
    "            self.register_parameter(f\"head_w__{r}\", nn.Parameter(torch.randn(h2, ncls) * 0.02))\n",
    "            self.register_parameter(f\"head_b__{r}\", nn.Parameter(torch.zeros(ncls)))\n",
    "        self._inited = True\n",
    "    def forward(self, x):\n",
    "        h = x @ self.w1 + self.b1\n",
    "        h = torch.relu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = h @ self.w2 + self.b2\n",
    "        h = torch.relu(h)\n",
    "        out = {}\n",
    "        for r in self.ranks:\n",
    "            w = getattr(self, f\"head_w__{r}\")\n",
    "            b = getattr(self, f\"head_b__{r}\")\n",
    "            out[r] = h @ w + b\n",
    "        return out\n",
    "\n",
    "# ---------- load checkpoint & model_state ----------\n",
    "if not CKPT_PATH.exists():\n",
    "    raise RuntimeError(f\"Checkpoint not found: {CKPT_PATH}. Make sure you ran training and the file exists.\")\n",
    "\n",
    "ckpt = torch.load(str(CKPT_PATH), map_location=device)\n",
    "print(\"[CAL] loaded checkpoint:\", CKPT_PATH)\n",
    "\n",
    "# infer input dim using a val batch\n",
    "sample_batch = next(iter(val_loader))\n",
    "sample = batch_tuple_to_dict(sample_batch)\n",
    "input_dim = int(sample[\"x\"].shape[1])\n",
    "\n",
    "model = ResilientModel()\n",
    "model.initialize(input_dim=input_dim, hidden_dim=256, ranks=RANKS, encoders=label_encoders, dropout=0.3)\n",
    "# load state dict contained in checkpoint (handle both dict forms)\n",
    "if isinstance(ckpt, dict) and \"model_state\" in ckpt:\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "elif isinstance(ckpt, dict) and \"state_dict\" in ckpt:\n",
    "    model.load_state_dict(ckpt[\"state_dict\"])\n",
    "elif isinstance(ckpt, dict) and all(isinstance(v, torch.Tensor) for v in ckpt.values()):\n",
    "    # maybe a direct state_dict-like mapping\n",
    "    try:\n",
    "        model.load_state_dict(ckpt)\n",
    "    except Exception:\n",
    "        # try model_state key fallback\n",
    "        raise RuntimeError(\"Unexpected checkpoint format; couldn't load state_dict.\")\n",
    "else:\n",
    "    # fallback: try load as state_dict\n",
    "    model.load_state_dict(ckpt)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"[CAL] model loaded and moved to device; eval() mode set.\")\n",
    "\n",
    "# ---------- collect logits and targets for validation set ----------\n",
    "all_logits = {r: [] for r in RANKS}\n",
    "all_targets = {r: [] for r in RANKS}\n",
    "n_samples = 0\n",
    "with torch.no_grad():\n",
    "    for bt in val_loader:\n",
    "        b = batch_tuple_to_dict(bt)\n",
    "        x = b[\"x\"].to(device)\n",
    "        out = model(x)\n",
    "        batch_n = x.shape[0]\n",
    "        n_samples += batch_n\n",
    "        for r in RANKS:\n",
    "            logits = out[r].detach()  # (batch, ncls) tensor on device\n",
    "            tgt = b[r].to(device)\n",
    "            all_logits[r].append(logits)\n",
    "            all_targets[r].append(tgt)\n",
    "print(f\"[CAL] collected logits from validation set: total samples ~ {n_samples}\")\n",
    "\n",
    "# concatenate\n",
    "for r in RANKS:\n",
    "    if all_logits[r]:\n",
    "        all_logits[r] = torch.cat(all_logits[r], dim=0)   # (N, C)\n",
    "        all_targets[r] = torch.cat(all_targets[r], dim=0).long()  # (N,)\n",
    "    else:\n",
    "        all_logits[r] = torch.zeros((0, max(1, len(label_encoders[r].classes_))), device=device)\n",
    "        all_targets[r] = torch.zeros((0,), dtype=torch.long, device=device)\n",
    "\n",
    "# ---------- temperature fitting helper ----------\n",
    "def fit_temperature_for_rank(logits: torch.Tensor, targets: torch.Tensor, init_temp=1.0, max_iter=200):\n",
    "    \"\"\"\n",
    "    Fit scalar temperature T >= small_pos to minimize CrossEntropyLoss( logits / T, targets).\n",
    "    Uses LBFGS; falls back to small Adam loop if LBFGS fails.\n",
    "    logits: torch.Tensor (N,C) on device\n",
    "    targets: torch.LongTensor (N,) on device\n",
    "    returns float T\n",
    "    \"\"\"\n",
    "    if logits.shape[0] == 0:\n",
    "        return 1.0\n",
    "    n_classes = logits.shape[1]\n",
    "    if n_classes <= 1:\n",
    "        return 1.0\n",
    "\n",
    "    # parameterize T directly but clamp in closure to avoid negative T\n",
    "    T_param = nn.Parameter(torch.tensor([float(init_temp)], device=device, dtype=torch.float32))\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # LBFGS closure\n",
    "    try:\n",
    "        optimizer = optim.LBFGS([T_param], max_iter=max_iter, line_search_fn=\"strong_wolfe\")\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            T = T_param.clamp(min=1e-6)\n",
    "            scaled = logits / T\n",
    "            loss = loss_fn(scaled, targets)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        optimizer.step(closure)\n",
    "        T_final = float(T_param.detach().clamp(min=1e-6).item())\n",
    "        # sanity: if nan or weird, fallback\n",
    "        if not math.isfinite(T_final) or T_final <= 0:\n",
    "            raise RuntimeError(\"LBFGS produced invalid T\")\n",
    "        return T_final\n",
    "    except Exception as e:\n",
    "        # fallback to small Adam loop (robust)\n",
    "        # print debug info\n",
    "        print(\"[CAL] LBFGS failed for rank (falling back to Adam):\", e)\n",
    "        T_param = nn.Parameter(torch.tensor([float(init_temp)], device=device, dtype=torch.float32))\n",
    "        optimizer2 = optim.Adam([T_param], lr=1e-2)\n",
    "        for it in range(300):\n",
    "            optimizer2.zero_grad()\n",
    "            T = T_param.clamp(min=1e-6)\n",
    "            loss = loss_fn(logits / T, targets)\n",
    "            loss.backward()\n",
    "            optimizer2.step()\n",
    "        T_final = float(T_param.detach().clamp(min=1e-6).item())\n",
    "        if not math.isfinite(T_final) or T_final <= 0:\n",
    "            return 1.0\n",
    "        return T_final\n",
    "\n",
    "# ---------- fit temperatures for each rank ----------\n",
    "temps = {}\n",
    "start_time = time.time()\n",
    "for r in RANKS:\n",
    "    ncls = len(label_encoders[r].classes_)\n",
    "    N = all_logits[r].shape[0]\n",
    "    if N == 0 or ncls <= 1:\n",
    "        temps[r] = 1.0\n",
    "        print(f\"[CAL] rank={r}: skipped (N={N}, ncls={ncls}) -> T=1.0\")\n",
    "        continue\n",
    "    print(f\"[CAL] fitting T for rank={r} (N={N}, ncls={ncls}) ...\", end=\"\", flush=True)\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        T_r = fit_temperature_for_rank(all_logits[r], all_targets[r], init_temp=1.0, max_iter=200)\n",
    "    except Exception as e:\n",
    "        print(\" failed:\", e)\n",
    "        T_r = 1.0\n",
    "    temps[r] = float(T_r)\n",
    "    print(f\" done (T={T_r:.4f}, took {time.time()-t0:.2f}s)\")\n",
    "\n",
    "print(f\"[CAL] temperature fitting completed in {time.time() - start_time:.2f}s\")\n",
    "# Save temperatures\n",
    "EXTRACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "with open(OUT_TEMPS, \"w\") as fh:\n",
    "    json.dump(temps, fh, indent=2)\n",
    "print(\"[CAL] saved temperatures to:\", OUT_TEMPS)\n",
    "\n",
    "# ---------- report NLL / acc before vs after calibration ----------\n",
    "print(\"\\n[CAL] per-rank NLL/accuracy before -> after (smaller NLL is better)\")\n",
    "for r in RANKS:\n",
    "    logits = all_logits[r]\n",
    "    targets = all_targets[r]\n",
    "    if logits.shape[0] == 0 or len(label_encoders[r].classes_) <= 1:\n",
    "        print(f\"  {r:10s}: no samples or single-class; skipped\")\n",
    "        continue\n",
    "    with torch.no_grad():\n",
    "        pre_nll = float(F.cross_entropy(logits, targets).item())\n",
    "        pre_acc = float((logits.argmax(dim=1) == targets).float().mean().item())\n",
    "        T = temps[r]\n",
    "        post_nll = float(F.cross_entropy(logits / max(1e-6, T), targets).item())\n",
    "        post_acc = float(((logits / max(1e-6, T)).argmax(dim=1) == targets).float().mean().item())\n",
    "    print(f\"  {r:10s}: pre_nll={pre_nll:.4f}, post_nll={post_nll:.4f}  | pre_acc={pre_acc:.4f}, post_acc={post_acc:.4f}  T={T:.4f}\")\n",
    "\n",
    "# ---------- produce calibrated validation CSV (one row per sample) ----------\n",
    "# We'll include index + for each rank -> true label, pred_idx, pred_label, pred_prob, T\n",
    "rows = []\n",
    "N = all_logits[RANKS[0]].shape[0] if len(RANKS) > 0 else 0\n",
    "for i in range(N):\n",
    "    row = {\"idx\": int(i)}\n",
    "    for r in RANKS:\n",
    "        logits_np = all_logits[r][i].cpu().numpy()\n",
    "        tgt_idx = int(all_targets[r][i].cpu().item()) if all_targets[r].shape[0] > 0 else None\n",
    "        T = temps.get(r, 1.0)\n",
    "        # stable softmax\n",
    "        scaled = logits_np / max(1e-12, T)\n",
    "        scaled = scaled - np.max(scaled)\n",
    "        ex = np.exp(scaled)\n",
    "        probs = ex / np.sum(ex)\n",
    "        pred_idx = int(np.argmax(probs))\n",
    "        pred_prob = float(np.max(probs))\n",
    "        # get label names if encoder has them\n",
    "        try:\n",
    "            classes = label_encoders[r].classes_\n",
    "            pred_label = str(classes[pred_idx])\n",
    "            true_label = str(classes[tgt_idx]) if tgt_idx is not None and tgt_idx >= 0 and tgt_idx < len(classes) else None\n",
    "        except Exception:\n",
    "            pred_label = str(pred_idx)\n",
    "            true_label = str(tgt_idx) if tgt_idx is not None else None\n",
    "        row[f\"{r}_true_idx\"] = true_label\n",
    "        row[f\"{r}_pred_idx\"] = pred_idx\n",
    "        row[f\"{r}_pred_label\"] = pred_label\n",
    "        row[f\"{r}_pred_prob\"] = pred_prob\n",
    "        row[f\"{r}_T\"] = float(T)\n",
    "    rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(OUT_VAL_CALIB, index=False)\n",
    "print(\"[CAL] wrote calibrated validation predictions to:\", OUT_VAL_CALIB)\n",
    "\n",
    "print(\"[CAL] Done. Per-rank temperatures:\")\n",
    "for r, t in temps.items():\n",
    "    print(f\"  {r:12s} -> T={t:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4826ad19-3325-4c66-8917-688d46486bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INF] device: cpu using loader: inference_loader\n",
      "[INF] MC_PASSES = 12 (adjust this variable if you need more/less samples)\n",
      "[INF] loaded checkpoint: ncbi_blast_db\\extracted\\best_shared_heads_resumed.pt\n",
      "[INF] model loaded to device; parameters: 82947\n",
      "[INF] loaded temperature scalars from: ncbi_blast_db\\extracted\\temp_scaling_by_rank.json\n",
      "[INF] found original indices length = 380\n",
      "[INF] processed batch 3/3  (total samples so far: 380)\n",
      "[INF] MC inference completed: 380 samples, time 0.2s\n",
      "[INF] wrote predictions with uncertainties to: ncbi_blast_db\\extracted\\predictions_with_uncertainty.csv\n",
      "[INF] wrote novel candidate ranking (top 500) to: ncbi_blast_db\\extracted\\novel_candidates_priority.csv\n",
      "\n",
      "Top 10 novel candidates (novel_score, global_index, species_pred_label, species_pred_conf, species_mutual_info):\n",
      "01) 0.4612707830965519 2127 Trichophyton japonicum 0.149295374751091 0.2001938819885254\n",
      "02) 0.44211927577853205 2402 Scedosporium aurantiacum 0.23798935115337372 0.11600136756896973\n",
      "03) 0.41102758310735227 1999 Entoloma sp. 0.08478925377130508 0.14522314071655273\n",
      "04) 0.40935407429933546 2344 UNASSIGNED 0.39044108986854553 0.3285790681838989\n",
      "05) 0.4049961492419243 2146 Aspergillus costaricensis 0.36845627427101135 0.10478878021240234\n",
      "06) 0.3967152267694473 2508 UNASSIGNED 0.20680582523345947 0.13309025764465332\n",
      "07) 0.37336395904421804 2521 Entoloma sp. 0.21163929998874664 0.1629655361175537\n",
      "08) 0.3673007354140282 2157 UNASSIGNED 0.30478039383888245 0.16463661193847656\n",
      "09) 0.36592171862721445 2504 UNASSIGNED 0.19681678712368011 0.10227251052856445\n",
      "10) 0.35684573352336885 2503 Omphalotaceae sp. 0.1777021884918213 0.08806037902832031\n",
      "\n",
      "[INF] Done. You can open the two CSVs in 'ncbi_blast_db/extracted/'.\n"
     ]
    }
   ],
   "source": [
    "# Cell: Inference with MC-dropout + uncertainty & novel-candidate ranking\n",
    "# Paste & run in the same notebook that contains your trained checkpoint, val_loader, batch_tuple_to_dict, and label_encoders.\n",
    "\n",
    "import time, json, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "EXTRACT_DIR = Path(\"ncbi_blast_db\") / \"extracted\"\n",
    "CKPT_PATH = EXTRACT_DIR / \"best_shared_heads_resumed.pt\"   # trained checkpoint (resumed)\n",
    "TEMP_PATH = EXTRACT_DIR / \"temp_scaling_by_rank.json\"\n",
    "OUT_PRED_CSV = EXTRACT_DIR / \"predictions_with_uncertainty.csv\"\n",
    "OUT_NOVEL_CSV = EXTRACT_DIR / \"novel_candidates_priority.csv\"\n",
    "\n",
    "# ---------- SETTINGS ----------\n",
    "SEED = 0\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Choose loader: prefer a global 'inference_loader' if present, otherwise use val_loader\n",
    "if \"inference_loader\" in globals():\n",
    "    inference_loader = globals()[\"inference_loader\"]\n",
    "elif \"val_loader\" in globals():\n",
    "    inference_loader = globals()[\"val_loader\"]\n",
    "else:\n",
    "    raise RuntimeError(\"No inference_loader or val_loader found in globals. Run dataset prep first.\")\n",
    "\n",
    "batch_tuple_to_dict = globals().get(\"batch_tuple_to_dict\")\n",
    "label_encoders = globals().get(\"label_encoders\")\n",
    "if batch_tuple_to_dict is None or label_encoders is None:\n",
    "    raise RuntimeError(\"Required globals missing: batch_tuple_to_dict and/or label_encoders\")\n",
    "\n",
    "RANKS = list(label_encoders.keys())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"[INF] device:\", device, \"using loader:\", \"inference_loader\" if 'inference_loader' in globals() else \"val_loader\")\n",
    "\n",
    "# sensible MC passes default: smaller on CPU\n",
    "MC_PASSES = 32 if device.type == \"cuda\" else 12\n",
    "print(f\"[INF] MC_PASSES = {MC_PASSES} (adjust this variable if you need more/less samples)\")\n",
    "\n",
    "# ---------- Resilient model definition (must match training) ----------\n",
    "class ResilientModel(nn.Module):\n",
    "    def _init_(self):\n",
    "        super()._init_()\n",
    "        self._inited = False\n",
    "    def initialize(self, input_dim, hidden_dim, ranks, encoders, dropout=0.3):\n",
    "        if getattr(self, \"_inited\", False): return\n",
    "        self.ranks = list(ranks)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        h1 = int(hidden_dim)\n",
    "        h2 = max(32, h1 // 2)\n",
    "        self.register_parameter(\"w1\", nn.Parameter(torch.randn(input_dim, h1) * 0.02))\n",
    "        self.register_parameter(\"b1\", nn.Parameter(torch.zeros(h1)))\n",
    "        self.register_parameter(\"w2\", nn.Parameter(torch.randn(h1, h2) * 0.02))\n",
    "        self.register_parameter(\"b2\", nn.Parameter(torch.zeros(h2)))\n",
    "        for r in self.ranks:\n",
    "            ncls = max(1, len(encoders[r].classes_))\n",
    "            self.register_parameter(f\"head_w__{r}\", nn.Parameter(torch.randn(h2, ncls) * 0.02))\n",
    "            self.register_parameter(f\"head_b__{r}\", nn.Parameter(torch.zeros(ncls)))\n",
    "        self._inited = True\n",
    "    def forward(self, x):\n",
    "        h = x @ self.w1 + self.b1\n",
    "        h = torch.relu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = h @ self.w2 + self.b2\n",
    "        h = torch.relu(h)\n",
    "        out = {}\n",
    "        for r in self.ranks:\n",
    "            w = getattr(self, f\"head_w__{r}\")\n",
    "            b = getattr(self, f\"head_b__{r}\")\n",
    "            out[r] = h @ w + b\n",
    "        return out\n",
    "\n",
    "# ---------- load checkpoint ----------\n",
    "if not Path(CKPT_PATH).exists():\n",
    "    raise RuntimeError(f\"Checkpoint not found: {CKPT_PATH}\")\n",
    "ckpt = torch.load(str(CKPT_PATH), map_location=device)\n",
    "print(\"[INF] loaded checkpoint:\", CKPT_PATH)\n",
    "\n",
    "# infer input dim from loader\n",
    "sample_batch = next(iter(inference_loader))\n",
    "sample = batch_tuple_to_dict(sample_batch)\n",
    "input_dim = int(sample[\"x\"].shape[1])\n",
    "\n",
    "model = ResilientModel()\n",
    "model.initialize(input_dim=input_dim, hidden_dim=256, ranks=RANKS, encoders=label_encoders, dropout=0.3)\n",
    "# load weights (handle both dict-with-model_state and direct state_dict)\n",
    "if isinstance(ckpt, dict) and \"model_state\" in ckpt:\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "elif isinstance(ckpt, dict) and \"state_dict\" in ckpt:\n",
    "    model.load_state_dict(ckpt[\"state_dict\"])\n",
    "else:\n",
    "    try:\n",
    "        model.load_state_dict(ckpt)\n",
    "    except Exception:\n",
    "        # try the 'model_state' fallback\n",
    "        if isinstance(ckpt, dict) and any(k.endswith(\"model_state\") for k in ckpt.keys()):\n",
    "            ms = ckpt.get(\"model_state\") or ckpt.get(\"state_dict\")\n",
    "            model.load_state_dict(ms)\n",
    "        else:\n",
    "            raise\n",
    "model.to(device)\n",
    "print(\"[INF] model loaded to device; parameters:\", sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# load temperatures if present\n",
    "temps = {}\n",
    "if Path(TEMP_PATH).exists():\n",
    "    with open(TEMP_PATH, \"r\") as fh:\n",
    "        temps = json.load(fh)\n",
    "    print(\"[INF] loaded temperature scalars from:\", TEMP_PATH)\n",
    "else:\n",
    "    temps = {r: 1.0 for r in RANKS}\n",
    "    print(\"[INF] no temp file found; defaulting to T=1 for all ranks\")\n",
    "\n",
    "# ---------- helper: original-index mapping for loader samples ----------\n",
    "def _loader_original_indices(loader):\n",
    "    # Try Subset.indices, global val_idx, or fallback sequential indices\n",
    "    ds = loader.dataset\n",
    "    try:\n",
    "        from torch.utils.data import Subset\n",
    "        if isinstance(ds, Subset):\n",
    "            return list(ds.indices)\n",
    "    except Exception:\n",
    "        pass\n",
    "    if hasattr(ds, \"indices\"):\n",
    "        try:\n",
    "            return list(getattr(ds, \"indices\"))\n",
    "        except Exception:\n",
    "            pass\n",
    "    # If val_idx global exists and loader is val_loader, use it\n",
    "    if \"val_idx\" in globals() and loader is globals().get(\"val_loader\"):\n",
    "        try:\n",
    "            return list(globals()[\"val_idx\"])\n",
    "        except Exception:\n",
    "            pass\n",
    "    # fallback: sequential 0..N-1\n",
    "    try:\n",
    "        N = len(ds)\n",
    "        return list(range(N))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "orig_indices = _loader_original_indices(inference_loader)\n",
    "if orig_indices is None:\n",
    "    print(\"[INF] could not determine original indices mapping; rows will use sequential batch-index.\")\n",
    "else:\n",
    "    print(f\"[INF] found original indices length = {len(orig_indices)}\")\n",
    "\n",
    "# ---------- main MC-inference loop ----------\n",
    "model.train()                # enable dropout during MC\n",
    "torch.set_grad_enabled(False)  # no gradient computation needed for MC predictions\n",
    "\n",
    "rows = []  # collected rows (dictionaries)\n",
    "sample_global_counter = 0\n",
    "total_samples_processed = 0\n",
    "t_start = time.time()\n",
    "\n",
    "n_batches = len(inference_loader)\n",
    "batch_i = 0\n",
    "for bt in inference_loader:\n",
    "    batch_i += 1\n",
    "    b = batch_tuple_to_dict(bt)\n",
    "    x = b[\"x\"].to(device)\n",
    "    batch_size = x.shape[0]\n",
    "\n",
    "    # prepare per-rank storage: shape (MC_PASSES, batch_size, n_classes)\n",
    "    per_rank_probs = {}\n",
    "    for r in RANKS:\n",
    "        ncls = max(1, len(label_encoders[r].classes_))\n",
    "        per_rank_probs[r] = np.zeros((MC_PASSES, batch_size, ncls), dtype=np.float32)\n",
    "\n",
    "    # MC forward passes (dropout active since model.train())\n",
    "    for m in range(MC_PASSES):\n",
    "        out = model(x)  # dict of logits tensors (batch, ncls)\n",
    "        for r in RANKS:\n",
    "            logits = out[r]  # torch tensor on device\n",
    "            T = float(temps.get(r, 1.0))\n",
    "            # apply temperature and softmax (torch -> numpy)\n",
    "            scaled = logits / max(1e-12, T)\n",
    "            probs = F.softmax(scaled, dim=1).cpu().numpy()\n",
    "            per_rank_probs[r][m] = probs\n",
    "\n",
    "    # per-sample aggregation\n",
    "    for i in range(batch_size):\n",
    "        entry = {}\n",
    "        # attempt to determine original dataset index\n",
    "        if orig_indices is not None:\n",
    "            try:\n",
    "                global_idx = orig_indices[sample_global_counter]\n",
    "            except Exception:\n",
    "                # if orig_indices is full-list we can map via counter modulo\n",
    "                global_idx = orig_indices[sample_global_counter] if sample_global_counter < len(orig_indices) else int(sample_global_counter)\n",
    "        else:\n",
    "            global_idx = int(sample_global_counter)\n",
    "        entry[\"global_index\"] = int(global_idx)\n",
    "\n",
    "        # optional: include metadata columns if available in a global df_meta\n",
    "        if \"df_meta\" in globals():\n",
    "            try:\n",
    "                meta_row = globals()[\"df_meta\"].iloc[global_idx]\n",
    "                # choose common columns if exist\n",
    "                for c in [\"id\", \"accession\", \"accession_base\", \"description\"]:\n",
    "                    if c in meta_row.index:\n",
    "                        entry[c] = meta_row[c]\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # compute stats per rank\n",
    "        for r in RANKS:\n",
    "            probs_all = per_rank_probs[r][:, i, :]  # shape (MC, ncls)\n",
    "            # mean predictive prob\n",
    "            mean_prob = probs_all.mean(axis=0)\n",
    "            # predictive entropy (total)\n",
    "            eps = 1e-12\n",
    "            entropy = -float(np.sum(mean_prob * np.log(np.clip(mean_prob, eps, 1.0))))\n",
    "            # expected entropy (aleatoric)\n",
    "            per_pass_ent = -np.sum(probs_all * np.log(np.clip(probs_all, eps, 1.0)), axis=1)  # shape (MC,)\n",
    "            exp_entropy = float(np.mean(per_pass_ent))\n",
    "            mutual_info = float(entropy - exp_entropy)  # epistemic uncertainty\n",
    "            pred_idx = int(np.argmax(mean_prob)) if mean_prob.size > 0 else -1\n",
    "            pred_conf = float(mean_prob[pred_idx]) if pred_idx >= 0 else 0.0\n",
    "            max_meanprob = float(np.max(mean_prob)) if mean_prob.size > 0 else 0.0\n",
    "            # label names if available\n",
    "            try:\n",
    "                classes = label_encoders[r].classes_\n",
    "                pred_label = str(classes[pred_idx]) if (pred_idx >= 0 and pred_idx < len(classes)) else str(pred_idx)\n",
    "            except Exception:\n",
    "                pred_label = str(pred_idx)\n",
    "\n",
    "            entry[f\"{r}_pred_idx\"] = pred_idx\n",
    "            entry[f\"{r}_pred_label\"] = pred_label\n",
    "            entry[f\"{r}_pred_conf\"] = pred_conf\n",
    "            entry[f\"{r}_entropy\"] = entropy\n",
    "            entry[f\"{r}_exp_entropy\"] = exp_entropy\n",
    "            entry[f\"{r}_mutual_info\"] = mutual_info\n",
    "            entry[f\"{r}_mc_mean_topprob\"] = max_meanprob\n",
    "\n",
    "        rows.append(entry)\n",
    "        sample_global_counter += 1\n",
    "        total_samples_processed += 1\n",
    "\n",
    "    # progress\n",
    "    if batch_i % 10 == 0 or batch_i == n_batches:\n",
    "        print(f\"[INF] processed batch {batch_i}/{n_batches}  (total samples so far: {total_samples_processed})\")\n",
    "\n",
    "t_total = time.time() - t_start\n",
    "print(f\"[INF] MC inference completed: {total_samples_processed} samples, time {t_total:.1f}s\")\n",
    "\n",
    "# ---------- build DataFrame and compute novelty score ----------\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Heuristic novel score:\n",
    "# combine species and genus mutual_info and low confidence into a single score (adaptable)\n",
    "score_parts = []\n",
    "if \"species_mutual_info\" in df.columns and \"species_pred_conf\" in df.columns:\n",
    "    df[\"species_novel_component\"] = df[\"species_mutual_info\"] * 1.0 + (1.0 - df[\"species_pred_conf\"]) * 0.5\n",
    "    score_parts.append(\"species_novel_component\")\n",
    "if \"genus_mutual_info\" in df.columns and \"genus_pred_conf\" in df.columns:\n",
    "    df[\"genus_novel_component\"] = df[\"genus_mutual_info\"] * 0.8 + (1.0 - df[\"genus_pred_conf\"]) * 0.3\n",
    "    score_parts.append(\"genus_novel_component\")\n",
    "# fallback: mean of mutual_info across ranks (if species/genus absent)\n",
    "if not score_parts:\n",
    "    mi_cols = [c for c in df.columns if c.endswith(\"_mutual_info\")]\n",
    "    if mi_cols:\n",
    "        df[\"novel_score\"] = df[mi_cols].mean(axis=1)\n",
    "    else:\n",
    "        df[\"novel_score\"] = 0.0\n",
    "else:\n",
    "    df[\"novel_score\"] = df[score_parts].mean(axis=1)\n",
    "\n",
    "# sort descending novel_score\n",
    "df_sorted = df.sort_values(\"novel_score\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# save outputs\n",
    "EXTRACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(OUT_PRED_CSV, index=False)\n",
    "df_sorted.head(500).to_csv(OUT_NOVEL_CSV, index=False)\n",
    "\n",
    "print(\"[INF] wrote predictions with uncertainties to:\", OUT_PRED_CSV)\n",
    "print(\"[INF] wrote novel candidate ranking (top 500) to:\", OUT_NOVEL_CSV)\n",
    "\n",
    "# quick summary printout of top 10 novel candidates\n",
    "print(\"\\nTop 10 novel candidates (novel_score, global_index, species_pred_label, species_pred_conf, species_mutual_info):\")\n",
    "cols_to_show = [\"novel_score\", \"global_index\", \"species_pred_label\", \"species_pred_conf\", \"species_mutual_info\"]\n",
    "for i, row in df_sorted.head(10).iterrows():\n",
    "    vals = [row.get(c, None) for c in cols_to_show]\n",
    "    print(f\"{i+1:02d})\", *[f\"{v}\" for v in vals])\n",
    "\n",
    "print(\"\\n[INF] Done. You can open the two CSVs in 'ncbi_blast_db/extracted/'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fbe2c946-56ed-44aa-a7d0-c67f9975abe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device -> cpu\n",
      "Single sample predictions (showing top ranks):\n",
      "  kingdom: UNASSIGNED (p=0.971)\n",
      "  phylum: UNASSIGNED (p=0.959)\n",
      "  class: UNASSIGNED (p=0.951)\n",
      "  order: UNASSIGNED (p=0.921)\n",
      "  family: UNASSIGNED (p=0.914)\n",
      "  genus: UNASSIGNED (p=0.906)\n",
      "  species: UNASSIGNED (p=0.376)\n",
      "\n",
      "Evaluation metrics per rank:\n",
      "  kingdom    acc=0.8868 f1_macro=0.8859\n",
      "  phylum     acc=0.8789 f1_macro=0.9214\n",
      "  class      acc=0.8684 f1_macro=0.9273\n",
      "  order      acc=0.8684 f1_macro=0.9256\n",
      "  family     acc=0.8684 f1_macro=0.9044\n",
      "  genus      acc=0.8658 f1_macro=0.8220\n",
      "  species    acc=0.7737 f1_macro=0.3942\n",
      "\n",
      "MC-dropout sample (showing species mutual info for sample 0):\n",
      "  kingdom: mutual_info(sample0) = 0.00904, top_pred = UNASSIGNED\n",
      "  phylum: mutual_info(sample0) = 0.00611, top_pred = UNASSIGNED\n",
      "  class: mutual_info(sample0) = 0.01922, top_pred = UNASSIGNED\n",
      "  order: mutual_info(sample0) = 0.01286, top_pred = UNASSIGNED\n",
      "  family: mutual_info(sample0) = 0.01243, top_pred = UNASSIGNED\n",
      "  genus: mutual_info(sample0) = 0.03618, top_pred = UNASSIGNED\n",
      "  species: mutual_info(sample0) = 0.07236, top_pred = UNASSIGNED\n"
     ]
    }
   ],
   "source": [
    "# Paste into your notebook cell and run (assumes the notebook already has\n",
    "# val_loader/inference_loader/batch_tuple_to_dict/label_encoders available)\n",
    "\n",
    "import torch, torch.nn.functional as F\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "EXTRACT_DIR = Path(\"ncbi_blast_db\") / \"extracted\"\n",
    "CKPT = EXTRACT_DIR / \"best_shared_heads_resumed.pt\"\n",
    "TFILE = EXTRACT_DIR / \"temp_scaling_by_rank.json\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device ->\", device)\n",
    "\n",
    "# ---------- model class (must match training) ----------\n",
    "import torch.nn as nn\n",
    "class ResilientModel(nn.Module):\n",
    "    def _init_(self):\n",
    "        super()._init_()\n",
    "        self._inited = False\n",
    "    def initialize(self, input_dim, hidden_dim, ranks, encoders, dropout=0.3):\n",
    "        if getattr(self, \"_inited\", False): return\n",
    "        self.ranks = list(ranks)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        h1 = int(hidden_dim); h2 = max(32, h1//2)\n",
    "        self.register_parameter(\"w1\", nn.Parameter(torch.randn(input_dim, h1) * 0.02))\n",
    "        self.register_parameter(\"b1\", nn.Parameter(torch.zeros(h1)))\n",
    "        self.register_parameter(\"w2\", nn.Parameter(torch.randn(h1, h2) * 0.02))\n",
    "        self.register_parameter(\"b2\", nn.Parameter(torch.zeros(h2)))\n",
    "        for r in self.ranks:\n",
    "            ncls = max(1, len(encoders[r].classes_))\n",
    "            self.register_parameter(f\"head_w__{r}\", nn.Parameter(torch.randn(h2, ncls) * 0.02))\n",
    "            self.register_parameter(f\"head_b__{r}\", nn.Parameter(torch.zeros(ncls)))\n",
    "        self._inited = True\n",
    "    def forward(self, x):\n",
    "        h = x @ self.w1 + self.b1\n",
    "        h = torch.relu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = h @ self.w2 + self.b2\n",
    "        h = torch.relu(h)\n",
    "        out = {}\n",
    "        for r in self.ranks:\n",
    "            w = getattr(self, f\"head_w__{r}\")\n",
    "            b = getattr(self, f\"head_b__{r}\")\n",
    "            out[r] = h @ w + b\n",
    "        return out\n",
    "\n",
    "# ---------- load checkpoint and temps ----------\n",
    "assert CKPT.exists(), f\"Checkpoint not found: {CKPT}\"\n",
    "ckpt = torch.load(str(CKPT), map_location=device)\n",
    "\n",
    "# required notebook globals\n",
    "label_encoders = globals().get(\"label_encoders\")\n",
    "batch_tuple_to_dict = globals().get(\"batch_tuple_to_dict\")\n",
    "if label_encoders is None or batch_tuple_to_dict is None:\n",
    "    raise RuntimeError(\"label_encoders or batch_tuple_to_dict not found in globals - run dataset prep cells first.\")\n",
    "RANKS = list(label_encoders.keys())\n",
    "\n",
    "# infer input dim from a loader (prefer inference_loader else val_loader)\n",
    "loader_for_shape = globals().get(\"inference_loader\") or globals().get(\"val_loader\")\n",
    "sample = batch_tuple_to_dict(next(iter(loader_for_shape)))\n",
    "input_dim = int(sample[\"x\"].shape[1])\n",
    "\n",
    "model = ResilientModel()\n",
    "model.initialize(input_dim=input_dim, hidden_dim=256, ranks=RANKS, encoders=label_encoders, dropout=0.3)\n",
    "# load state\n",
    "if isinstance(ckpt, dict) and \"model_state\" in ckpt:\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "else:\n",
    "    model.load_state_dict(ckpt)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# load temps if present\n",
    "import json\n",
    "temps = {r:1.0 for r in RANKS}\n",
    "if Path(TFILE).exists():\n",
    "    with open(TFILE,\"r\") as fh: temps = json.load(fh)\n",
    "    temps = {k:float(v) for k,v in temps.items()}\n",
    "\n",
    "# ---------- utility: logits -> calibrated probs ----------\n",
    "def probs_from_logits_with_temp(logits_tensor, rank):\n",
    "    # logits_tensor: torch tensor (B, C)\n",
    "    T = float(temps.get(rank, 1.0))\n",
    "    scaled = logits_tensor / max(1e-12, T)\n",
    "    probs = F.softmax(scaled, dim=1)\n",
    "    return probs  # torch tensor on same device\n",
    "\n",
    "# ---------- single-sample prediction example ----------\n",
    "def predict_single(x_numpy):\n",
    "    \"\"\"x_numpy: 1D numpy vector length=input_dim\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = torch.from_numpy(x_numpy.astype(np.float32)).unsqueeze(0).to(device)\n",
    "        out = model(x)\n",
    "        result = {}\n",
    "        for r in RANKS:\n",
    "            logits = out[r]  # (1, C)\n",
    "            probs = probs_from_logits_with_temp(logits, r).cpu().numpy()[0]\n",
    "            pred_idx = int(probs.argmax())\n",
    "            pred_label = label_encoders[r].classes_[pred_idx]\n",
    "            result[r] = {\"pred_idx\": pred_idx, \"pred_label\": pred_label, \"pred_prob\": float(probs[pred_idx]), \"probs\": probs}\n",
    "        return result\n",
    "\n",
    "# ---------- batched evaluation: accuracy & macro-F1 per rank ----------\n",
    "def evaluate_loader(loader):\n",
    "    model.eval()\n",
    "    all_preds = {r: [] for r in RANKS}\n",
    "    all_trues = {r: [] for r in RANKS}\n",
    "    with torch.no_grad():\n",
    "        for bt in loader:\n",
    "            b = batch_tuple_to_dict(bt)\n",
    "            x = b[\"x\"].to(device)\n",
    "            out = model(x)\n",
    "            for r in RANKS:\n",
    "                probs = probs_from_logits_with_temp(out[r], r).cpu().numpy()\n",
    "                preds = probs.argmax(axis=1)\n",
    "                trues = b[r].cpu().numpy()\n",
    "                all_preds[r].append(preds)\n",
    "                all_trues[r].append(trues)\n",
    "    metrics = {}\n",
    "    for r in RANKS:\n",
    "        if not all_preds[r]:\n",
    "            metrics[r] = {\"acc\": None, \"f1_macro\": None}\n",
    "            continue\n",
    "        preds = np.concatenate(all_preds[r])\n",
    "        trues = np.concatenate(all_trues[r])\n",
    "        acc = accuracy_score(trues, preds)\n",
    "        f1m = f1_score(trues, preds, average=\"macro\", zero_division=0)\n",
    "        metrics[r] = {\"acc\": float(acc), \"f1_macro\": float(f1m)}\n",
    "    return metrics\n",
    "\n",
    "# ---------- MC-dropout for one batch (epistemic/mutual info) ----------\n",
    "def mc_dropout_on_batch(batch, mc_passes=32):\n",
    "    # input: one batch from loader (tuple as batch_tuple_to_dict expects)\n",
    "    model.train()  # enable dropout\n",
    "    b = batch_tuple_to_dict(batch)\n",
    "    x = b[\"x\"].to(device)\n",
    "    batch_size = x.shape[0]\n",
    "    res = []\n",
    "    for r in RANKS:\n",
    "        ncls = len(label_encoders[r].classes_)\n",
    "        probs_all = np.zeros((mc_passes, batch_size, ncls), dtype=np.float32)\n",
    "        for m in range(mc_passes):\n",
    "            out = model(x)\n",
    "            logits = out[r].detach() / max(1e-12, temps.get(r, 1.0))\n",
    "            probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "            probs_all[m] = probs\n",
    "        # compute mean, entropy, expected entropy, mutual info\n",
    "        mean_probs = probs_all.mean(axis=0)              # (B, C)\n",
    "        ent = -np.sum(mean_probs * np.log(np.clip(mean_probs, 1e-12,1.0)), axis=1)\n",
    "        per_pass_ent = -np.sum(probs_all * np.log(np.clip(probs_all,1e-12,1.0)), axis=2)  # (MC, B)\n",
    "        exp_ent = per_pass_ent.mean(axis=0)              # (B,)\n",
    "        mutual_info = ent - exp_ent\n",
    "        res.append((r, mean_probs, ent, exp_ent, mutual_info))\n",
    "    model.eval()\n",
    "    return res\n",
    "\n",
    "# ---------- USAGE examples ----------\n",
    "# 1) single-sample (take first sample from val_loader)\n",
    "loader = globals().get(\"inference_loader\") or globals().get(\"val_loader\")\n",
    "bt = next(iter(loader))\n",
    "b = batch_tuple_to_dict(bt)\n",
    "x0 = b[\"x\"][0].cpu().numpy()\n",
    "single = predict_single(x0)\n",
    "print(\"Single sample predictions (showing top ranks):\")\n",
    "for r in RANKS:\n",
    "    print(f\"  {r}: {single[r]['pred_label']} (p={single[r]['pred_prob']:.3f})\")\n",
    "\n",
    "# 2) full evaluation on val_loader\n",
    "metrics = evaluate_loader(loader)\n",
    "print(\"\\nEvaluation metrics per rank:\")\n",
    "for r,m in metrics.items():\n",
    "    print(f\"  {r:10s} acc={m['acc']:.4f} f1_macro={m['f1_macro']:.4f}\")\n",
    "\n",
    "# 3) MC-dropout example on that same batch (fast: mc_passes=12)\n",
    "mc_results = mc_dropout_on_batch(bt, mc_passes=12)\n",
    "print(\"\\nMC-dropout sample (showing species mutual info for sample 0):\")\n",
    "for r, mean_probs, ent, exp_ent, mi in mc_results:\n",
    "    print(f\"  {r}: mutual_info(sample0) = {float(mi[0]):.5f}, top_pred = {label_encoders[r].classes_[int(mean_probs[0].argmax())]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cca3c16e-646b-400f-9072-fa5309c0e26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 species by support (val):\n",
      "species_true_idx\n",
      "UNASSIGNED                         201\n",
      "Maylandia zebra                     80\n",
      "Chaetodon auriga                    22\n",
      "Arvicanthis niloticus                6\n",
      "Morchella sp.                        5\n",
      "Aonchotheca annulosa                 4\n",
      "Pseudopestalotiopsis sp.             3\n",
      "Deuterostichococcus epilithicus      3\n",
      "Aspergillus costaricensis            3\n",
      "Chloroidium saccharophilum           3\n",
      "Morchella nipponensis                2\n",
      "Cardimyxobolus iriomotensis          2\n",
      "Amanita fuscozonata                  2\n",
      "Inocybe sp.                          2\n",
      "Inocybe miranda                      2\n",
      "Amanita sp.                          2\n",
      "Cortinarius sp.                      2\n",
      "Baruscapillaria inflexa              2\n",
      "Diplosphaera chodatii                1\n",
      "Pseudoboletus parasiticus            1\n",
      "Name: count, dtype: int64\n",
      "Top-5 species accuracy (val): 0.9052631578947369\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, top_k_accuracy_score\n",
    "\n",
    "# load calibrated val predictions (we created this) and uncertainty file\n",
    "val_df = pd.read_csv(\"ncbi_blast_db/extracted/val_predictions_calibrated.csv\")\n",
    "unc_df = pd.read_csv(\"ncbi_blast_db/extracted/predictions_with_uncertainty.csv\")\n",
    "\n",
    "# support per species (on validation)\n",
    "if 'species_true_idx' in val_df.columns:\n",
    "    support = val_df['species_true_idx'].value_counts().sort_values(ascending=False)\n",
    "    print(\"Top 20 species by support (val):\")\n",
    "    print(support.head(20))\n",
    "\n",
    "# compute top-5 accuracy for species using model outputs saved previously (if you have logits, use them).\n",
    "# If you only have predicted probs in files (mean probs in MC cell) you can compute top-k from those:\n",
    "# We'll check in-case the MC inference CSV saved mean probs per class (if not, compute from model).\n",
    "# Example assuming val_df contains species_pred_prob only (top-1); we'll compute top-5 using model on val_loader:\n",
    "\n",
    "# --- compute top-k using model (recommended) ---\n",
    "from sklearn.metrics import accuracy_score\n",
    "loader = globals().get(\"val_loader\")\n",
    "model = globals().get(\"model\")  # if model in scope\n",
    "label_encoders = globals()[\"label_encoders\"]\n",
    "RANKS = list(label_encoders.keys())\n",
    "\n",
    "def topk_species_accuracy(loader, model, k=5):\n",
    "    model.eval()\n",
    "    ys, preds_topk = [], []\n",
    "    for bt in loader:\n",
    "        batch = batch_tuple_to_dict(bt)\n",
    "        x = batch['x'].to(next(model.parameters()).device)\n",
    "        out = model(x)\n",
    "        logits = out['species'].detach().cpu().numpy()  # (B, C)\n",
    "        # apply temp if you use temps:\n",
    "        temps = {}\n",
    "        import json\n",
    "        try:\n",
    "            temps = json.load(open(\"ncbi_blast_db/extracted/temp_scaling_by_rank.json\"))\n",
    "        except:\n",
    "            temps = {r:1.0 for r in RANKS}\n",
    "        logits = logits / temps.get('species', 1.0)\n",
    "        topk = np.argsort(-logits, axis=1)[:, :k]\n",
    "        preds_topk.append(topk)\n",
    "        ys.append(batch['species'].cpu().numpy())\n",
    "    ys = np.concatenate(ys)\n",
    "    preds_topk = np.vstack(preds_topk)\n",
    "    # top-k accuracy: check if true label in predicted topk row\n",
    "    hit = np.array([ys[i] in preds_topk[i] for i in range(len(ys))])\n",
    "    return float(hit.mean())\n",
    "\n",
    "print(\"Top-5 species accuracy (val):\", topk_species_accuracy(loader, model, k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "55111751-a0a9-4c33-8aac-63b13ed38036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inocybe favoris -> Aspergillus costaricensis : 24 times\n",
      "Inocybe favoris -> Armillaria sp. : 6 times\n",
      "Inocybe favoris -> Diplosphaera chodatii : 5 times\n",
      "Inocybe favoris -> Clavulina amethystina : 5 times\n",
      "Amanita fuscozonata -> Cladosporium pseudocladosporioides : 4 times\n",
      "Inocybe favoris -> Cladosporium allicinum : 3 times\n",
      "Inocybe favoris -> Ancylostoma ceylanicum : 3 times\n",
      "Inocybe favoris -> Amphibiocapillaria tritonispunctati : 2 times\n",
      "Inocybe favoris -> Inocybe beatifica : 2 times\n",
      "Antarctomyces sp. -> Amanita fulva : 2 times\n",
      "Inocybe favoris -> Chloroidium saccharophilum : 2 times\n",
      "Inocybe favoris -> Agaricus sp. : 2 times\n",
      "Clitocybula sp. -> Chloroidium saccharophilum : 1 times\n",
      "Diplosphaera chodatii -> Cortinarius vagabundus : 1 times\n",
      "Inocybe favoris -> Inocybe derbschii : 1 times\n",
      "Gymnopilus luteus -> Agaricus argyropotamicus : 1 times\n",
      "Gliophorus sp. -> Geotrichum citri-aurantii : 1 times\n",
      "Inocybe favoris -> Hysterothylacium fabri : 1 times\n",
      "Inocybe favoris -> Geotrichum citri-aurantii : 1 times\n",
      "Inocybe favoris -> Entoloma sp. : 1 times\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# build true/pred arrays for species on val\n",
    "trues, preds = [], []\n",
    "for bt in loader:\n",
    "    b = batch_tuple_to_dict(bt)\n",
    "    x = b['x'].to(next(model.parameters()).device)\n",
    "    out = model(x)\n",
    "    logits = out['species'].detach().cpu().numpy()\n",
    "    # temperature scaling\n",
    "    import json\n",
    "    temps = json.load(open(\"ncbi_blast_db/extracted/temp_scaling_by_rank.json\"))\n",
    "    logits = logits / temps.get('species', 1.0)\n",
    "    p = np.argmax(logits, axis=1)\n",
    "    preds.append(p); trues.append(b['species'].cpu().numpy())\n",
    "trues = np.concatenate(trues); preds = np.concatenate(preds)\n",
    "\n",
    "cm = confusion_matrix(trues, preds)\n",
    "# find most common off-diagonal confusions\n",
    "cm_off = cm.copy()\n",
    "np.fill_diagonal(cm_off, 0)\n",
    "# get top confused pairs\n",
    "pairs = []\n",
    "for i,j in zip(*np.unravel_index(np.argsort(cm_off.ravel())[::-1], cm_off.shape)):\n",
    "    if cm_off[i,j] <= 0: break\n",
    "    pairs.append((i,j,cm_off[i,j]))\n",
    "# map to labels\n",
    "species_labels = label_encoders['species'].classes_\n",
    "for a,b,count in pairs[:20]:\n",
    "    print(f\"{species_labels[a]} -> {species_labels[b]} : {count} times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "821c1b80-0980-4344-9308-d57ba0c9c456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Species ECE: 0.16603384813980052\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def expected_calibration_error(probs, labels, n_bins=15):\n",
    "    probs = np.asarray(probs)\n",
    "    labels = np.asarray(labels)\n",
    "    confidences = probs.max(axis=1)\n",
    "    predictions = probs.argmax(axis=1)\n",
    "    bins = np.linspace(0,1,n_bins+1)\n",
    "    ece = 0.0\n",
    "    for i in range(n_bins):\n",
    "        lower, upper = bins[i], bins[i+1]\n",
    "        mask = (confidences > lower) & (confidences <= upper)\n",
    "        if mask.sum()==0: continue\n",
    "        acc = (predictions[mask] == labels[mask]).mean()\n",
    "        conf = confidences[mask].mean()\n",
    "        ece += (mask.sum() / len(labels)) * abs(acc - conf)\n",
    "    return ece\n",
    "\n",
    "# example: compute species ECE by running model on val and collecting probs\n",
    "probs_list = []; labs = []\n",
    "for bt in loader:\n",
    "    b = batch_tuple_to_dict(bt); x=b['x'].to(next(model.parameters()).device)\n",
    "    out = model(x)\n",
    "    logits = out['species'].detach().cpu()\n",
    "    logits = logits / temps.get('species',1.0)\n",
    "    probs = torch.softmax(logits, dim=1).numpy()\n",
    "    probs_list.append(probs); labs.append(b['species'].cpu().numpy())\n",
    "probs_arr = np.vstack(probs_list); labs = np.concatenate(labs)\n",
    "print(\"Species ECE:\", expected_calibration_error(probs_arr, labs, n_bins=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c7704ff9-b63b-4575-b038-7cfb97af56a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kingdom: UNASSIGNED  p=0.971\n",
      "phylum: UNASSIGNED  p=0.959\n",
      "class: UNASSIGNED  p=0.951\n",
      "order: UNASSIGNED  p=0.921\n",
      "family: UNASSIGNED  p=0.914\n",
      "genus: UNASSIGNED  p=0.906\n",
      "species: UNASSIGNED  p=0.376\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch, torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "CKPT = Path(\"ncbi_blast_db/extracted/best_shared_heads_resumed.pt\")\n",
    "TFILE = Path(\"ncbi_blast_db/extracted/temp_scaling_by_rank.json\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# recreate the same model class (use the ResilientModel class you used earlier),\n",
    "# initialize exactly as before, then:\n",
    "ckpt = torch.load(str(CKPT), map_location=device)\n",
    "model = ResilientModel()              # use the same class definition you already used\n",
    "model.initialize(input_dim=64, hidden_dim=256, ranks=list(label_encoders.keys()), encoders=label_encoders, dropout=0.3)\n",
    "model.load_state_dict(ckpt.get(\"model_state\", ckpt))\n",
    "model.to(device); model.eval()\n",
    "\n",
    "# load temps\n",
    "import json\n",
    "temps = json.load(open(TFILE)) if TFILE.exists() else {r:1.0 for r in label_encoders.keys()}\n",
    "\n",
    "# pick one sample from a loader:\n",
    "loader = globals().get(\"inference_loader\") or globals().get(\"val_loader\")\n",
    "batch = next(iter(loader))\n",
    "b = batch_tuple_to_dict(batch)\n",
    "x0 = b[\"x\"][0:1].to(device)   # single sample\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(x0)\n",
    "    for r in label_encoders.keys():\n",
    "        logits = out[r] / max(1e-12, temps.get(r, 1.0))\n",
    "        probs = F.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "        pred_idx = int(probs.argmax())\n",
    "        print(f\"{r}: {label_encoders[r].classes_[pred_idx]}  p={probs[pred_idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ba91a4c6-ce2e-431b-b064-9c4ba69b99d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kingdom': {'acc': 0.8868421052631579, 'f1_macro': 0.8858739305046273}, 'phylum': {'acc': 0.8789473684210526, 'f1_macro': 0.9214409010002644}, 'class': {'acc': 0.868421052631579, 'f1_macro': 0.9273010143352481}, 'order': {'acc': 0.868421052631579, 'f1_macro': 0.9255512297263465}, 'family': {'acc': 0.868421052631579, 'f1_macro': 0.9043662190901388}, 'genus': {'acc': 0.8657894736842106, 'f1_macro': 0.8220249642221711}, 'species': {'acc': 0.7736842105263158, 'f1_macro': 0.39419672261669525}}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "def evaluate_loader(loader):\n",
    "    model.eval()\n",
    "    all_preds, all_trues = {r:[] for r in label_encoders}, {r:[] for r in label_encoders}\n",
    "    with torch.no_grad():\n",
    "        for bt in loader:\n",
    "            b = batch_tuple_to_dict(bt)\n",
    "            x = b['x'].to(device)\n",
    "            out = model(x)\n",
    "            for r in label_encoders:\n",
    "                logits = out[r] / max(1e-12, temps.get(r,1.0))\n",
    "                probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "                all_preds[r].append(probs.argmax(axis=1))\n",
    "                all_trues[r].append(b[r].cpu().numpy())\n",
    "    metrics = {}\n",
    "    for r in label_encoders:\n",
    "        if not all_preds[r]: \n",
    "            metrics[r] = {\"acc\":None, \"f1_macro\":None}; continue\n",
    "        preds = np.concatenate(all_preds[r]); trues = np.concatenate(all_trues[r])\n",
    "        metrics[r] = {\"acc\": float(accuracy_score(trues,preds)), \"f1_macro\": float(f1_score(trues,preds,average='macro', zero_division=0))}\n",
    "    return metrics\n",
    "\n",
    "metrics = evaluate_loader(loader)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "87ffa4b9-6cc2-40a9-a6ed-8087440b9242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example keys in checkpoint/state_dict (first 40):\n",
      "  0 w1\n",
      "  1 b1\n",
      "  2 w2\n",
      "  3 b2\n",
      "  4 head_w__kingdom\n",
      "  5 head_b__kingdom\n",
      "  6 head_w__phylum\n",
      "  7 head_b__phylum\n",
      "  8 head_w__class\n",
      "  9 head_b__class\n",
      "  10 head_w__order\n",
      "  11 head_b__order\n",
      "  12 head_w__family\n",
      "  13 head_b__family\n",
      "  14 head_w__genus\n",
      "  15 head_b__genus\n",
      "  16 head_w__species\n",
      "  17 head_b__species\n",
      "... total keys: 18\n",
      "Model loaded. number of parameters: 82947\n",
      "Head parameter presence (sample):\n",
      "  kingdom: found w shape=(128, 2), b shape=(2,)\n",
      "  phylum: found w shape=(128, 5), b shape=(5,)\n",
      "  class: found w shape=(128, 10), b shape=(10,)\n",
      "  order: found w shape=(128, 13), b shape=(13,)\n",
      "  family: found w shape=(128, 19), b shape=(19,)\n",
      "\n",
      "Single-sample predictions:\n",
      " {'kingdom': {'pred_idx': 1, 'pred_label': 'UNASSIGNED', 'prob': 0.9707451462745667}, 'phylum': {'pred_idx': 3, 'pred_label': 'UNASSIGNED', 'prob': 0.9586387872695923}, 'class': {'pred_idx': 9, 'pred_label': 'UNASSIGNED', 'prob': 0.9505306482315063}, 'order': {'pred_idx': 11, 'pred_label': 'UNASSIGNED', 'prob': 0.9205213785171509}, 'family': {'pred_idx': 17, 'pred_label': 'UNASSIGNED', 'prob': 0.9137222170829773}, 'genus': {'pred_idx': 25, 'pred_label': 'UNASSIGNED', 'prob': 0.9064501523971558}, 'species': {'pred_idx': 180, 'pred_label': 'UNASSIGNED', 'prob': 0.3763955235481262}}\n"
     ]
    }
   ],
   "source": [
    "# Fix for AttributeError: missing head_w_...  (use the correct parameter names)\n",
    "import torch, torch.nn as nn, torch.nn.functional as F, json\n",
    "from pathlib import Path\n",
    "EXTRACT_DIR = Path(\"ncbi_blast_db\") / \"extracted\"\n",
    "CKPT = EXTRACT_DIR / \"best_shared_heads_resumed.pt\"\n",
    "TFILE = EXTRACT_DIR / \"temp_scaling_by_rank.json\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1) show state_dict keys so we confirm the exact names present in checkpoint / model\n",
    "ckpt = torch.load(str(CKPT), map_location=device)\n",
    "ms = ckpt.get(\"model_state\", ckpt) if isinstance(ckpt, dict) else ckpt\n",
    "print(\"Example keys in checkpoint/state_dict (first 40):\")\n",
    "for i,k in enumerate(list(ms.keys())[:40]):\n",
    "    print(\" \", i, k)\n",
    "print(\"... total keys:\", len(ms))\n",
    "\n",
    "# 2) defensive model: forward tries both name patterns (double underscore then single)\n",
    "class ResilientModel(nn.Module):\n",
    "    def _init_(self):\n",
    "        super()._init_()\n",
    "        self._inited = False\n",
    "    def initialize(self, input_dim, hidden_dim, ranks, encoders, dropout=0.3):\n",
    "        # this registers parameters with the double-underscore names used during training\n",
    "        if getattr(self, \"_inited\", False): return\n",
    "        self.ranks = list(ranks)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        h1 = int(hidden_dim); h2 = max(32, h1 // 2)\n",
    "        self.register_parameter(\"w1\", nn.Parameter(torch.randn(input_dim, h1) * 0.02))\n",
    "        self.register_parameter(\"b1\", nn.Parameter(torch.zeros(h1)))\n",
    "        self.register_parameter(\"w2\", nn.Parameter(torch.randn(h1, h2) * 0.02))\n",
    "        self.register_parameter(\"b2\", nn.Parameter(torch.zeros(h2)))\n",
    "        for r in self.ranks:\n",
    "            ncls = max(1, len(encoders[r].classes_))\n",
    "            # register using the double-underscore convention (matches your checkpoint)\n",
    "            self.register_parameter(f\"head_w__{r}\", nn.Parameter(torch.randn(h2, ncls) * 0.02))\n",
    "            self.register_parameter(f\"head_b__{r}\", nn.Parameter(torch.zeros(ncls)))\n",
    "        self._inited = True\n",
    "\n",
    "    def _get_head_params(self, r):\n",
    "        # try double-underscore name first (the name used in training), then single-underscore fallback\n",
    "        name_w_ds = f\"head_w__{r}\"\n",
    "        name_b_ds = f\"head_b__{r}\"\n",
    "        name_w_ss = f\"head_w_{r}\"\n",
    "        name_b_ss = f\"head_b_{r}\"\n",
    "        if hasattr(self, name_w_ds) and hasattr(self, name_b_ds):\n",
    "            return getattr(self, name_w_ds), getattr(self, name_b_ds)\n",
    "        if hasattr(self, name_w_ss) and hasattr(self, name_b_ss):\n",
    "            return getattr(self, name_w_ss), getattr(self, name_b_ss)\n",
    "        # final fallback: try to find any parameter that contains the rank string\n",
    "        for n,p in self.named_parameters():\n",
    "            if f\"head_w\" in n and (r in n):\n",
    "                w_name = n\n",
    "                b_name = n.replace(\"head_w\",\"head_b\")\n",
    "                if hasattr(self, b_name):\n",
    "                    return getattr(self, w_name), getattr(self, b_name)\n",
    "        raise AttributeError(f\"No head parameters found for rank='{r}' (tried {name_w_ds},{name_w_ss})\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x @ self.w1 + self.b1\n",
    "        h = torch.relu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = h @ self.w2 + self.b2\n",
    "        h = torch.relu(h)\n",
    "        out = {}\n",
    "        for r in self.ranks:\n",
    "            w, b = self._get_head_params(r)   # robustly fetch params\n",
    "            out[r] = h @ w + b\n",
    "        return out\n",
    "\n",
    "# 3) Load label_encoders and a loader from your notebookglobals (must exist)\n",
    "assert \"label_encoders\" in globals() and (\"inference_loader\" in globals() or \"val_loader\" in globals()) and \"batch_tuple_to_dict\" in globals()\n",
    "label_encoders = globals()[\"label_encoders\"]\n",
    "loader = globals().get(\"inference_loader\") or globals().get(\"val_loader\")\n",
    "batch_tuple_to_dict = globals()[\"batch_tuple_to_dict\"]\n",
    "RANKS = list(label_encoders.keys())\n",
    "\n",
    "# 4) instantiate and load state\n",
    "sample_batch = next(iter(loader))\n",
    "sample = batch_tuple_to_dict(sample_batch)\n",
    "input_dim = int(sample[\"x\"].shape[1])\n",
    "\n",
    "model = ResilientModel()\n",
    "model.initialize(input_dim=input_dim, hidden_dim=256, ranks=RANKS, encoders=label_encoders, dropout=0.3)\n",
    "# load checkpoint state (handle both forms)\n",
    "state = ckpt.get(\"model_state\", ckpt) if isinstance(ckpt, dict) else ckpt\n",
    "# load_state_dict will match param names from the checkpoint to the module's param names.\n",
    "# If checkpoint uses double-underscore names and we've registered the same, this will succeed.\n",
    "model.load_state_dict(state)\n",
    "model.to(device)\n",
    "print(\"Model loaded. number of parameters:\", sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# 5) quick check: show that head params exist\n",
    "print(\"Head parameter presence (sample):\")\n",
    "for r in RANKS[:5]:\n",
    "    try:\n",
    "        w,b = model._get_head_params(r)\n",
    "        print(f\"  {r}: found w shape={tuple(w.shape)}, b shape={tuple(b.shape)}\")\n",
    "    except Exception as e:\n",
    "        print(\"  \", r, \"->\", e)\n",
    "\n",
    "# 6) run a single-sample deterministic prediction (with calibration if temp file exists)\n",
    "temps = {r:1.0 for r in RANKS}\n",
    "if Path(TFILE).exists():\n",
    "    temps = {k:float(v) for k,v in json.load(open(TFILE)).items()}\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    b0 = batch_tuple_to_dict(next(iter(loader)))\n",
    "    x0 = b0[\"x\"][0:1].to(device)\n",
    "    out = model(x0)\n",
    "    results = {}\n",
    "    for r in RANKS:\n",
    "        logits = out[r] / max(1e-12, temps.get(r,1.0))\n",
    "        probs = F.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "        idx = int(probs.argmax())\n",
    "        label = label_encoders[r].classes_[idx]\n",
    "        results[r] = {\"pred_idx\": idx, \"pred_label\": label, \"prob\": float(probs[idx])}\n",
    "print(\"\\nSingle-sample predictions:\\n\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0f492b42-694c-4ab9-8301-d60d822fdef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deterministic: {'kingdom': {'pred_idx': 1, 'pred_label': 'UNASSIGNED', 'prob': 0.9707451462745667, 'probs': array([0.02925488, 0.97074515], dtype=float32)}, 'phylum': {'pred_idx': 3, 'pred_label': 'UNASSIGNED', 'prob': 0.9586387872695923, 'probs': array([3.9736195e-03, 3.7238460e-02, 9.8026078e-07, 9.5863879e-01,\n",
      "       1.4807584e-04], dtype=float32)}, 'class': {'pred_idx': 9, 'pred_label': 'UNASSIGNED', 'prob': 0.9505306482315063, 'probs': array([7.1348179e-05, 4.3802533e-02, 3.4696552e-06, 4.8463512e-03,\n",
      "       6.3554020e-05, 7.6261688e-07, 5.7074840e-07, 1.9403724e-05,\n",
      "       6.6132913e-04, 9.5053065e-01], dtype=float32)}, 'order': {'pred_idx': 11, 'pred_label': 'UNASSIGNED', 'prob': 0.9205213785171509, 'probs': array([1.3949699e-05, 7.9814587e-03, 9.0692297e-04, 1.5622020e-06,\n",
      "       6.4422026e-02, 5.5940636e-03, 9.1526119e-05, 2.0697414e-06,\n",
      "       4.8125563e-05, 1.7701190e-04, 7.1040442e-05, 9.2052138e-01,\n",
      "       1.6889031e-04], dtype=float32)}, 'family': {'pred_idx': 17, 'pred_label': 'UNASSIGNED', 'prob': 0.9137222170829773, 'probs': array([8.9746714e-04, 8.4337244e-06, 6.2546729e-07, 1.3416303e-04,\n",
      "       1.8481524e-06, 1.2060649e-04, 4.8181719e-06, 8.9369274e-07,\n",
      "       1.1219098e-04, 1.7465478e-06, 2.3449936e-06, 2.1383863e-05,\n",
      "       4.3956832e-05, 6.3420441e-03, 1.4569536e-04, 7.9236785e-03,\n",
      "       7.2696118e-04, 9.1372222e-01, 6.9788881e-02], dtype=float32)}, 'genus': {'pred_idx': 25, 'pred_label': 'UNASSIGNED', 'prob': 0.9064501523971558, 'probs': array([4.9363810e-04, 5.5609039e-06, 4.0980372e-05, 7.6396719e-07,\n",
      "       6.9322432e-06, 2.0968228e-05, 2.5266388e-06, 8.4470128e-05,\n",
      "       2.0307071e-04, 2.9052719e-03, 7.4160300e-02, 5.7147012e-05,\n",
      "       2.6447121e-06, 7.7839133e-05, 1.9530378e-03, 1.6076107e-06,\n",
      "       1.6658657e-06, 9.8846226e-07, 5.3265703e-05, 2.0541155e-03,\n",
      "       7.7896053e-04, 1.5876903e-03, 1.8768026e-04, 6.4905551e-03,\n",
      "       2.3566403e-03, 9.0645015e-01, 2.1707176e-05], dtype=float32)}, 'species': {'pred_idx': 180, 'pred_label': 'UNASSIGNED', 'prob': 0.3763955235481262, 'probs': array([4.43497593e-05, 1.46517041e-05, 1.90360966e-04, 6.98943739e-04,\n",
      "       8.76497128e-04, 2.91690609e-04, 6.66960317e-04, 2.26743147e-03,\n",
      "       1.24282262e-03, 1.36845699e-03, 1.40386066e-04, 1.75055684e-04,\n",
      "       1.97137579e-01, 3.00514791e-03, 2.88368342e-03, 1.38125457e-02,\n",
      "       1.52670269e-04, 1.88113586e-03, 4.06003437e-06, 6.60879984e-02,\n",
      "       3.63353211e-05, 2.26293807e-04, 1.76061377e-01, 9.19770755e-05,\n",
      "       1.53266548e-04, 4.76012938e-04, 4.38028161e-04, 7.65144614e-06,\n",
      "       2.52470295e-06, 8.55637263e-05, 1.99828555e-05, 1.43661164e-04,\n",
      "       6.40298385e-05, 5.40844303e-06, 2.64026632e-04, 1.79113194e-05,\n",
      "       6.19601997e-05, 7.20361732e-06, 7.86646979e-06, 2.07748981e-05,\n",
      "       1.37620009e-04, 1.77354974e-04, 7.16760678e-06, 6.15720201e-05,\n",
      "       8.98811501e-04, 3.38718179e-03, 2.21818500e-05, 8.60080414e-04,\n",
      "       3.10617825e-03, 1.97296598e-04, 3.32399993e-03, 1.10684923e-04,\n",
      "       1.36052622e-04, 1.15087081e-04, 8.94244877e-05, 1.52749053e-05,\n",
      "       1.63832683e-05, 2.92815494e-05, 1.76696316e-03, 1.17763283e-03,\n",
      "       8.91937641e-04, 4.92121399e-06, 2.31798094e-05, 2.78690481e-04,\n",
      "       2.02623405e-03, 2.30815822e-05, 2.85659789e-05, 4.48119026e-05,\n",
      "       4.75992740e-04, 3.42817584e-05, 1.27240928e-04, 1.42645786e-05,\n",
      "       3.49437687e-05, 7.69777580e-06, 1.40304674e-05, 5.21325965e-05,\n",
      "       1.50905844e-05, 5.55971201e-06, 3.80593410e-05, 6.47905326e-05,\n",
      "       2.87129888e-06, 6.35934412e-05, 2.15089422e-05, 4.51289197e-05,\n",
      "       1.98509788e-05, 1.24412458e-04, 7.76268353e-05, 1.82298027e-05,\n",
      "       3.82415436e-07, 3.09980387e-04, 1.11926440e-03, 1.14387385e-05,\n",
      "       2.26680026e-03, 3.30303283e-06, 7.40459131e-04, 1.65919027e-05,\n",
      "       3.31364026e-06, 2.36856533e-04, 1.04832370e-03, 6.90301647e-04,\n",
      "       1.32335936e-05, 1.05783374e-05, 1.25548584e-04, 2.85103481e-04,\n",
      "       6.41285034e-04, 1.89520462e-04, 2.72149628e-04, 6.35271545e-06,\n",
      "       1.18803996e-06, 2.54501501e-05, 1.24104484e-03, 1.62059930e-03,\n",
      "       2.20954767e-03, 5.09688444e-03, 4.14370419e-03, 1.43509172e-03,\n",
      "       2.40720995e-03, 5.27984754e-04, 7.84774125e-03, 8.72850615e-06,\n",
      "       2.05365850e-05, 1.51741202e-04, 1.34582966e-04, 4.19504242e-04,\n",
      "       7.44975659e-06, 4.69834813e-05, 8.31195721e-05, 3.72359409e-06,\n",
      "       8.86441776e-05, 1.17836862e-04, 2.06571026e-03, 7.24002079e-04,\n",
      "       3.56995201e-06, 8.43277667e-04, 1.14636328e-04, 1.28598287e-04,\n",
      "       1.63712185e-02, 7.77185021e-04, 7.42373813e-04, 8.57633058e-05,\n",
      "       6.43782914e-05, 1.76053843e-04, 1.75850982e-05, 1.50220512e-04,\n",
      "       7.46436708e-05, 1.49283512e-02, 8.07431235e-04, 1.47734024e-03,\n",
      "       3.45443748e-03, 5.91693679e-05, 5.31631995e-05, 1.86425077e-05,\n",
      "       3.67090135e-04, 1.85532510e-04, 7.65367586e-05, 1.57850838e-04,\n",
      "       1.75741501e-04, 1.01798614e-05, 9.08642614e-05, 1.49409621e-04,\n",
      "       3.93367198e-04, 4.45261685e-04, 7.84278847e-04, 1.47818937e-04,\n",
      "       1.15019400e-02, 1.90667026e-02, 1.76678528e-04, 4.16663039e-04,\n",
      "       2.28364865e-04, 7.80890137e-03, 4.26897313e-05, 2.59242224e-04,\n",
      "       6.73354225e-06, 8.19682318e-05, 6.59867714e-04, 5.58820728e-04,\n",
      "       1.02441630e-03, 2.56446481e-04, 7.84353353e-03, 5.74815658e-06,\n",
      "       3.76395524e-01, 1.26400002e-04, 4.38430638e-04], dtype=float32)}}\n",
      "mc (12 passes): {'kingdom': {'pred_idx': 1, 'pred_label': 'UNASSIGNED', 'mean_prob': array([0.04630553, 0.9536944 ], dtype=float32), 'entropy': 0.18749001622200012, 'exp_entropy': 0.16729529201984406, 'mutual_info': 0.020194724202156067}, 'phylum': {'pred_idx': 3, 'pred_label': 'UNASSIGNED', 'mean_prob': array([5.0989785e-03, 5.8323722e-02, 3.1048176e-06, 9.3622285e-01,\n",
      "       3.5131595e-04], dtype=float32), 'entropy': 0.25718966126441956, 'exp_entropy': 0.22926217317581177, 'mutual_info': 0.027927488088607788}, 'class': {'pred_idx': 9, 'pred_label': 'UNASSIGNED', 'mean_prob': array([1.7106604e-04, 6.6529892e-02, 7.9129286e-06, 5.8346540e-03,\n",
      "       9.5290023e-05, 2.1492899e-06, 1.9746624e-06, 3.4722394e-05,\n",
      "       1.0244468e-03, 9.2629796e-01], dtype=float32), 'entropy': 0.291154146194458, 'exp_entropy': 0.26365384459495544, 'mutual_info': 0.027500301599502563}, 'order': {'pred_idx': 11, 'pred_label': 'UNASSIGNED', 'mean_prob': array([2.4791969e-05, 9.4546210e-03, 1.0432507e-03, 4.1203289e-06,\n",
      "       8.6532526e-02, 5.9545729e-03, 1.7888028e-04, 4.8414886e-06,\n",
      "       7.4422489e-05, 2.3154943e-04, 1.0279557e-04, 8.9605141e-01,\n",
      "       3.4231957e-04], dtype=float32), 'entropy': 0.40009331703186035, 'exp_entropy': 0.36903712153434753, 'mutual_info': 0.031056195497512817}, 'family': {'pred_idx': 17, 'pred_label': 'UNASSIGNED', 'mean_prob': array([1.0275125e-03, 1.4596410e-05, 1.9017779e-06, 1.5177502e-04,\n",
      "       4.5283900e-06, 1.8914884e-04, 1.1997050e-05, 2.3268776e-06,\n",
      "       2.1089493e-04, 3.9819010e-06, 5.3345998e-06, 3.5764828e-05,\n",
      "       6.7018271e-05, 7.5190230e-03, 1.8044759e-04, 7.8592794e-03,\n",
      "       1.3193398e-03, 8.8870996e-01, 9.2685223e-02], dtype=float32), 'entropy': 0.4238154888153076, 'exp_entropy': 0.39116808772087097, 'mutual_info': 0.032647401094436646}, 'genus': {'pred_idx': 25, 'pred_label': 'UNASSIGNED', 'mean_prob': array([5.9405767e-04, 1.0526285e-05, 6.6944223e-05, 2.2262316e-06,\n",
      "       1.2192623e-05, 3.0733157e-05, 5.8676510e-06, 1.3601236e-04,\n",
      "       3.2546112e-04, 3.3966911e-03, 9.7427644e-02, 8.8112596e-05,\n",
      "       7.3440478e-06, 1.4911301e-04, 2.1318891e-03, 3.9498841e-06,\n",
      "       3.6913780e-06, 2.4304748e-06, 8.0874634e-05, 2.2323274e-03,\n",
      "       8.3036110e-04, 2.5307487e-03, 2.1674484e-04, 7.9361526e-03,\n",
      "       2.6606915e-03, 8.7906688e-01, 5.0288483e-05], dtype=float32), 'entropy': 0.4764082133769989, 'exp_entropy': 0.44098320603370667, 'mutual_info': 0.035425007343292236}, 'species': {'pred_idx': 180, 'pred_label': 'UNASSIGNED', 'mean_prob': array([5.69941731e-05, 1.87375881e-05, 2.28580611e-04, 8.10468104e-04,\n",
      "       8.70755175e-04, 2.85695831e-04, 6.68757129e-04, 2.31434754e-03,\n",
      "       1.17294665e-03, 1.37322221e-03, 1.58356605e-04, 1.95164044e-04,\n",
      "       2.14968994e-01, 3.08264419e-03, 2.96732248e-03, 1.34631507e-02,\n",
      "       1.71115811e-04, 1.95822166e-03, 7.63143453e-06, 6.20617270e-02,\n",
      "       4.64188488e-05, 2.48849654e-04, 1.65528283e-01, 1.34606875e-04,\n",
      "       1.97623929e-04, 5.71330951e-04, 5.44756942e-04, 1.29868686e-05,\n",
      "       4.47739831e-06, 1.03302613e-04, 2.78659791e-05, 1.60647483e-04,\n",
      "       7.60334442e-05, 9.70881501e-06, 2.72316276e-04, 2.04655153e-05,\n",
      "       6.40012804e-05, 9.48532033e-06, 9.10287508e-06, 2.54220358e-05,\n",
      "       1.43364174e-04, 1.90461986e-04, 8.06676053e-06, 7.31171385e-05,\n",
      "       1.18594745e-03, 4.00948850e-03, 3.17613194e-05, 8.12910264e-04,\n",
      "       2.80883280e-03, 2.37717832e-04, 3.55436583e-03, 1.26812796e-04,\n",
      "       1.47938306e-04, 1.27256979e-04, 1.03082537e-04, 2.58347136e-05,\n",
      "       1.87425430e-05, 3.05619214e-05, 1.73609518e-03, 1.11665681e-03,\n",
      "       8.69798940e-04, 7.30307192e-06, 3.41780469e-05, 3.01616295e-04,\n",
      "       1.83617265e-03, 3.06326110e-05, 3.35350160e-05, 6.16546677e-05,\n",
      "       4.87105659e-04, 3.94658273e-05, 1.44053702e-04, 1.71157171e-05,\n",
      "       4.06054933e-05, 1.10689225e-05, 1.69814975e-05, 5.99085724e-05,\n",
      "       2.00329432e-05, 9.03354612e-06, 4.08242195e-05, 7.40389951e-05,\n",
      "       4.84783595e-06, 7.53502536e-05, 2.78341049e-05, 5.65311384e-05,\n",
      "       2.88629799e-05, 1.20852637e-04, 9.44069761e-05, 2.10939270e-05,\n",
      "       7.09560027e-07, 3.09398776e-04, 1.19763205e-03, 1.54446952e-05,\n",
      "       2.30340404e-03, 5.99431178e-06, 8.13160965e-04, 2.47661319e-05,\n",
      "       5.75590093e-06, 2.68687319e-04, 1.09894702e-03, 8.53722915e-04,\n",
      "       1.41563460e-05, 1.28961356e-05, 1.53438028e-04, 3.14805453e-04,\n",
      "       6.73422823e-04, 2.09643375e-04, 2.84776877e-04, 8.32691694e-06,\n",
      "       2.05735819e-06, 3.47017376e-05, 2.20520515e-03, 1.51953381e-03,\n",
      "       2.34459713e-03, 5.21086203e-03, 4.11687279e-03, 1.42400584e-03,\n",
      "       2.48700823e-03, 5.52711077e-04, 7.49685382e-03, 1.30779163e-05,\n",
      "       2.81056364e-05, 1.67081642e-04, 1.54448469e-04, 4.61345335e-04,\n",
      "       1.09733583e-05, 5.15866377e-05, 1.04060826e-04, 6.03784565e-06,\n",
      "       9.44313288e-05, 1.32309491e-04, 2.19285674e-03, 6.93128677e-04,\n",
      "       5.23926610e-06, 8.27903103e-04, 1.32677495e-04, 1.47054670e-04,\n",
      "       1.74217820e-02, 7.95354601e-04, 7.72631436e-04, 1.05399464e-04,\n",
      "       8.46823168e-05, 1.96256966e-04, 2.66838560e-05, 1.68278973e-04,\n",
      "       1.01935228e-04, 1.48507571e-02, 9.25331551e-04, 1.47818646e-03,\n",
      "       3.34463641e-03, 7.27248480e-05, 5.64407055e-05, 2.80599070e-05,\n",
      "       4.05665254e-04, 1.93501823e-04, 8.07819233e-05, 1.74885514e-04,\n",
      "       2.30696271e-04, 1.35229029e-05, 9.47778462e-05, 1.58023307e-04,\n",
      "       4.34102025e-04, 4.91048617e-04, 8.31868441e-04, 1.66362777e-04,\n",
      "       1.03514390e-02, 1.79339219e-02, 1.93520842e-04, 4.71336505e-04,\n",
      "       2.17282228e-04, 8.82859249e-03, 5.55245315e-05, 2.22437258e-04,\n",
      "       1.31261404e-05, 9.86530213e-05, 6.95758674e-04, 6.48249814e-04,\n",
      "       1.08568440e-03, 2.85869290e-04, 7.80781172e-03, 9.09852770e-06,\n",
      "       3.69596988e-01, 1.43208177e-04, 4.94917913e-04], dtype=float32), 'entropy': 2.18670654296875, 'exp_entropy': 2.149648904800415, 'mutual_info': 0.03705763816833496}}\n"
     ]
    }
   ],
   "source": [
    "# Predictor helper: load model, deterministic predict and MC-dropout predict\n",
    "import torch, torch.nn as nn, torch.nn.functional as F, json, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def _make_model_class():\n",
    "    class ResilientModel(nn.Module):\n",
    "        def _init(self): super().init_(); self._inited=False\n",
    "        def initialize(self, input_dim, hidden_dim, ranks, encoders, dropout=0.3):\n",
    "            if getattr(self,\"_inited\",False): return\n",
    "            self.ranks = list(ranks); self.dropout = nn.Dropout(dropout)\n",
    "            h1=int(hidden_dim); h2=max(32,h1//2)\n",
    "            self.register_parameter(\"w1\", nn.Parameter(torch.randn(input_dim,h1)*0.02))\n",
    "            self.register_parameter(\"b1\", nn.Parameter(torch.zeros(h1)))\n",
    "            self.register_parameter(\"w2\", nn.Parameter(torch.randn(h1,h2)*0.02))\n",
    "            self.register_parameter(\"b2\", nn.Parameter(torch.zeros(h2)))\n",
    "            for r in self.ranks:\n",
    "                ncls = max(1, len(encoders[r].classes_))\n",
    "                # register head parameters with double-underscore convention (matches your checkpoint)\n",
    "                self.register_parameter(f\"head_w__{r}\", nn.Parameter(torch.randn(h2,ncls)*0.02))\n",
    "                self.register_parameter(f\"head_b__{r}\", nn.Parameter(torch.zeros(ncls)))\n",
    "            self._inited=True\n",
    "\n",
    "        def _get_head(self, r):\n",
    "            # try double-underscore then single-underscore then fuzzy\n",
    "            n_w = f\"head_w_{r}\"; n_b = f\"head_b_{r}\"\n",
    "            if hasattr(self, n_w) and hasattr(self, n_b):\n",
    "                return getattr(self, n_w), getattr(self, n_b)\n",
    "            n_w2 = f\"head_w_{r}\"; n_b2 = f\"head_b_{r}\"\n",
    "            if hasattr(self, n_w2) and hasattr(self, n_b2):\n",
    "                return getattr(self, n_w2), getattr(self, n_b2)\n",
    "            # fallback: find param containing rank\n",
    "            for n,p in self.named_parameters():\n",
    "                if \"head_w\" in n and (r in n):\n",
    "                    w = getattr(self, n); bname = n.replace(\"head_w\",\"head_b\")\n",
    "                    if hasattr(self, bname): return getattr(self, n), getattr(self, bname)\n",
    "            raise AttributeError(f\"No head found for rank '{r}'\")\n",
    "        def forward(self, x):\n",
    "            h = x @ self.w1 + self.b1; h = torch.relu(h)\n",
    "            h = self.dropout(h)\n",
    "            h = h @ self.w2 + self.b2; h = torch.relu(h)\n",
    "            out = {}\n",
    "            for r in self.ranks:\n",
    "                w,b = self._get_head(r)\n",
    "                out[r] = h @ w + b\n",
    "            return out\n",
    "    return ResilientModel\n",
    "\n",
    "def load_predictor(checkpoint_path, label_encoders, temp_path=None, device=None):\n",
    "    device = torch.device(device or (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    CK = Path(checkpoint_path)\n",
    "    assert CK.exists(), f\"Checkpoint missing: {CK}\"\n",
    "    ck = torch.load(str(CK), map_location=device)\n",
    "    ResilientModel = _make_model_class()\n",
    "    model = ResilientModel()\n",
    "    # infer input dim from a sample if user provides one — but we'll require user to pass input_dim or loader.\n",
    "    # Here we expect user to pass input_dim via label_encoders container (we'll not assume).\n",
    "    # We'll attempt to infer input_dim from a global loader if present\n",
    "    input_dim = None\n",
    "    if \"inference_loader\" in globals():\n",
    "        sample = globals()[\"batch_tuple_to_dict\"](next(iter(globals()[\"inference_loader\"])))\n",
    "        input_dim = int(sample[\"x\"].shape[1])\n",
    "    elif \"val_loader\" in globals():\n",
    "        sample = globals()[\"batch_tuple_to_dict\"](next(iter(globals()[\"val_loader\"])))\n",
    "        input_dim = int(sample[\"x\"].shape[1])\n",
    "    if input_dim is None:\n",
    "        raise RuntimeError(\"Cannot infer input_dim — provide a loader in globals or modify load_predictor to accept input_dim.\")\n",
    "    model.initialize(input_dim=input_dim, hidden_dim=256, ranks=list(label_encoders.keys()), encoders=label_encoders, dropout=0.3)\n",
    "    st = ck.get(\"model_state\", ck) if isinstance(ck, dict) else ck\n",
    "    model.load_state_dict(st)\n",
    "    model.to(device).eval()\n",
    "    temps = {r:1.0 for r in label_encoders}\n",
    "    if temp_path:\n",
    "        p = Path(temp_path)\n",
    "        if p.exists():\n",
    "            temps = {k:float(v) for k,v in json.load(open(p)).items()}\n",
    "    return {\"model\": model, \"device\": device, \"temps\": temps, \"ranks\": list(label_encoders.keys()), \"encoders\": label_encoders}\n",
    "\n",
    "def predict_single(predictor, embedding_np):\n",
    "    \"\"\"embedding_np: 1D numpy array (input_dim,)\"\"\"\n",
    "    model = predictor[\"model\"]; device = predictor[\"device\"]; temps = predictor[\"temps\"]; ranks = predictor[\"ranks\"]; encs = predictor[\"encoders\"]\n",
    "    model.eval()\n",
    "    x = torch.from_numpy(np.asarray(embedding_np, dtype=np.float32)).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(x)\n",
    "    out_res = {}\n",
    "    for r in ranks:\n",
    "        logits = out[r] / max(1e-12, float(temps.get(r,1.0)))\n",
    "        probs = F.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "        idx = int(np.argmax(probs))\n",
    "        out_res[r] = {\"pred_idx\": idx, \"pred_label\": encs[r].classes_[idx], \"prob\": float(probs[idx]), \"probs\": probs}\n",
    "    return out_res\n",
    "\n",
    "def predict_with_mc(predictor, embedding_np, mc_passes=32):\n",
    "    \"\"\"Returns mean probs, predictive entropy, expected entropy, mutual_info per rank.\"\"\"\n",
    "    model = predictor[\"model\"]; device = predictor[\"device\"]; temps = predictor[\"temps\"]; ranks = predictor[\"ranks\"]; encs = predictor[\"encoders\"]\n",
    "    x = torch.from_numpy(np.asarray(embedding_np, dtype=np.float32)).unsqueeze(0).to(device)\n",
    "    model.train()   # enable dropout\n",
    "    probs_all = {r: [] for r in ranks}\n",
    "    with torch.no_grad():\n",
    "        for m in range(mc_passes):\n",
    "            out = model(x)\n",
    "            for r in ranks:\n",
    "                logits = out[r] / max(1e-12, float(temps.get(r,1.0)))\n",
    "                probs = F.softmax(logits, dim=1).cpu().numpy()[0]  # (C,)\n",
    "                probs_all[r].append(probs)\n",
    "    # aggregate\n",
    "    res = {}\n",
    "    for r in ranks:\n",
    "        arr = np.stack(probs_all[r], axis=0)   # (MC, C)\n",
    "        mean_prob = arr.mean(axis=0)\n",
    "        entropy = -np.sum(mean_prob * np.log(np.clip(mean_prob,1e-12,1.0)))\n",
    "        per_pass_ent = -np.sum(arr * np.log(np.clip(arr,1e-12,1.0)), axis=1)\n",
    "        exp_ent = per_pass_ent.mean()\n",
    "        mutual_info = float(entropy - exp_ent)\n",
    "        pred_idx = int(np.argmax(mean_prob))\n",
    "        res[r] = {\"pred_idx\": pred_idx, \"pred_label\": encs[r].classes_[pred_idx], \"mean_prob\": mean_prob, \"entropy\": float(entropy), \"exp_entropy\": float(exp_ent), \"mutual_info\": mutual_info}\n",
    "    model.eval()\n",
    "    return res\n",
    "\n",
    "# ---------------- Example usage (run after previous definitions) ----------------\n",
    "# 1) load predictor:\n",
    "predictor = load_predictor(\"ncbi_blast_db/extracted/best_shared_heads_resumed.pt\", label_encoders, temp_path=\"ncbi_blast_db/extracted/temp_scaling_by_rank.json\")\n",
    "# 2) deterministic prediction for first val sample\n",
    "batch = next(iter(globals().get(\"inference_loader\") or globals().get(\"val_loader\")))\n",
    "b = globals()[\"batch_tuple_to_dict\"](batch)\n",
    "emb = b[\"x\"][0].cpu().numpy()\n",
    "print(\"deterministic:\", predict_single(predictor, emb))\n",
    "print(\"mc (12 passes):\", predict_with_mc(predictor, emb, mc_passes=12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8d83ee3e-b00e-486f-8031-1cd3ba8cc01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[FOUND] using extracted folder: C:\\Users\\Srijit\\OneDrive\\Desktop\\sihtaxa\\sihabundance\\ncbi_blast_db\\extracted\n",
      "[LOAD] loading embeddings (numpy) ... C:\\Users\\Srijit\\OneDrive\\Desktop\\sihtaxa\\sihabundance\\ncbi_blast_db\\extracted\\embeddings_pca.npy\n",
      "[OK] embeddings shape: (2555, 64)\n",
      "[LOAD] loading metadata CSV ... C:\\Users\\Srijit\\OneDrive\\Desktop\\sihtaxa\\sihabundance\\ncbi_blast_db\\extracted\\embeddings_meta_clustered.csv\n",
      "[OK] metadata rows: 2555\n",
      "[INFO] No abundance-like column in metadata; searching for label CSVs in extracted/ ...\n",
      "[INFO] No candidate label CSVs found in extracted/\n",
      "\n",
      "[NO TARGET FOUND]\n",
      "I couldn't find a numeric abundance column in metadata or a label CSV.\n",
      "To proceed, either:\n",
      "  - place a CSV with columns 'id' and 'abundance' into:\n",
      "      C:\\Users\\Srijit\\OneDrive\\Desktop\\sihtaxa\\sihabundance\\ncbi_blast_db\\extracted  (recommended) or\n",
      "      C:\\Users\\Srijit\\sih or another folder listed above and re-run this cell.\n",
      "  - OR edit this cell and set DOWNLOAD_DIR to your exact folder path, e.g.:\n",
      "      DOWNLOAD_DIR = Path(r\"C:\\Users\\Srijit\\OneDrive\\Desktop\\sihtaxa\\sihabundance\\ncbi_blast_db\")\n"
     ]
    }
   ],
   "source": [
    "# Robust Cell — locate embeddings + meta (search common locations) and assemble abundance_dataset.csv if possible.\n",
    "# Paste/run this cell in your notebook. It will NOT raise uncaught exceptions; it prints helpful diagnostics.\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "\n",
    "def main():\n",
    "    # ---------- EDITABLE fallback path(s) ----------\n",
    "    # If you know the exact folder, set it here as a raw Windows path, e.g.:\n",
    "    # DOWNLOAD_DIR = Path(r\"C:\\Users\\Srijit\\OneDrive\\Desktop\\sihtaxa\\sihabundance\\ncbi_blast_db\")\n",
    "    DOWNLOAD_DIR = None   # <-- leave None to let the search try common candidates\n",
    "    # -----------------------------------------------\n",
    "\n",
    "    # Candidate roots to check (in priority order). We include the exact path you mentioned.\n",
    "    candidates = []\n",
    "    if DOWNLOAD_DIR:\n",
    "        candidates.append(Path(str(DOWNLOAD_DIR)))\n",
    "    # Path the user reported having files in:\n",
    "    candidates.append(Path(r\"C:\\Users\\Srijit\\OneDrive\\Desktop\\sihtaxa\\sihabundance\\ncbi_blast_db\"))\n",
    "    # some convenient relatives (project/workdir)\n",
    "    candidates.append(Path.cwd() / \"sih\" / \"ncbi_blast_db\")\n",
    "    candidates.append(Path.cwd() / \"ncbi_blast_db\")\n",
    "    candidates.append(Path.cwd())\n",
    "    # home OneDrive common location (Windows)\n",
    "    home = Path.home()\n",
    "    candidates.append(home / \"OneDrive\" / \"Desktop\" / \"sihtaxa\" / \"sihabundance\" / \"ncbi_blast_db\")\n",
    "    candidates.append(home / \"OneDrive\" / \"Desktop\" / \"ncbi_blast_db\")\n",
    "    candidates = [p.resolve() for p in candidates if p is not None]\n",
    "\n",
    "    # Files we need inside extracted/\n",
    "    need_files = (\"embeddings_pca.npy\", \"embeddings_meta_clustered.csv\")\n",
    "\n",
    "    found = None\n",
    "    scanned = []\n",
    "    for root in candidates:\n",
    "        extracted = root / \"extracted\"\n",
    "        scanned.append(str(extracted))\n",
    "        if extracted.exists() and extracted.is_dir():\n",
    "            all_present = all((extracted / f).exists() for f in need_files)\n",
    "            if all_present:\n",
    "                found = extracted\n",
    "                break\n",
    "\n",
    "    # Also try a broader search: scan subfolders of current working directory for an extracted/ with required files\n",
    "    if found is None:\n",
    "        for p in Path.cwd().rglob(\"extracted\"):\n",
    "            if (p / need_files[0]).exists() and (p / need_files[1]).exists():\n",
    "                found = p.resolve()\n",
    "                scanned.append(str(p.resolve()))\n",
    "                break\n",
    "\n",
    "    if found is None:\n",
    "        print(\"\\n[ERROR] Could not locate required files in any scanned 'extracted/' locations.\")\n",
    "        print(\"Scanned candidate 'extracted' directories (in order):\")\n",
    "        for s in scanned:\n",
    "            print(\"  -\", s)\n",
    "        print(\"\\nWhat to do next (pick one):\")\n",
    "        print(\"  1) Edit the top of this cell and set DOWNLOAD_DIR = Path(r\\\"C:\\\\full\\\\path\\\\to\\\\ncbi_blast_db\\\")\")\n",
    "        print(\"     Example for your machine:\")\n",
    "        print(\"       DOWNLOAD_DIR = Path(r\\\"C:\\\\Users\\\\Srijit\\\\OneDrive\\\\Desktop\\\\sihtaxa\\\\sihabundance\\\\ncbi_blast_db\\\")\")\n",
    "        print(\"  2) Move the 'extracted' folder that contains the two files into one of the scanned locations above.\")\n",
    "        print(\"\\nFiles expected inside the 'extracted' folder:\")\n",
    "        for f in need_files:\n",
    "            print(\"  -\", f)\n",
    "        print(\"\\nThis cell will exit gracefully; edit DOWNLOAD_DIR and re-run.\")\n",
    "        return  # graceful return\n",
    "\n",
    "    # Found the extracted folder\n",
    "    EXTRACT_DIR = found\n",
    "    print(f\"\\n[FOUND] using extracted folder: {EXTRACT_DIR}\")\n",
    "    emb_pca_path = EXTRACT_DIR / \"embeddings_pca.npy\"\n",
    "    meta_path    = EXTRACT_DIR / \"embeddings_meta_clustered.csv\"\n",
    "\n",
    "    # Load files defensively\n",
    "    try:\n",
    "        print(\"[LOAD] loading embeddings (numpy) ...\", emb_pca_path)\n",
    "        X = np.load(emb_pca_path)\n",
    "        print(\"[OK] embeddings shape:\", X.shape)\n",
    "    except Exception as e:\n",
    "        print(\"[ERROR] Failed to load embeddings_pca.npy:\", e)\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        print(\"[LOAD] loading metadata CSV ...\", meta_path)\n",
    "        meta = pd.read_csv(meta_path)\n",
    "        print(\"[OK] metadata rows:\", len(meta))\n",
    "    except Exception as e:\n",
    "        print(\"[ERROR] Failed to load embeddings_meta_clustered.csv:\", e)\n",
    "        return\n",
    "\n",
    "    # Ensure 'id' column exists (try common alternates)\n",
    "    if \"id\" not in meta.columns:\n",
    "        for alt in (\"accession\",\"seqid\",\"accession_id\",\"accession_id_base\",\"accession.version\"):\n",
    "            if alt in meta.columns:\n",
    "                meta = meta.rename(columns={alt: \"id\"})\n",
    "                print(f\"[INFO] Renamed metadata column '{alt}' -> 'id'\")\n",
    "                break\n",
    "    if \"id\" not in meta.columns:\n",
    "        print(\"[ERROR] metadata has no 'id' column. Columns present:\", list(meta.columns)[:40])\n",
    "        print(\"If your metadata uses a different name for accession, rename that column to 'id' or edit the code.\")\n",
    "        return\n",
    "\n",
    "    # Search for abundance-like column(s) in meta\n",
    "    cand_keywords = [\"abund\", \"abundance\", \"count\", \"reads\", \"depth\", \"coverage\", \"relative\", \"rpm\"]\n",
    "    abundance_candidates = []\n",
    "    for c in meta.columns:\n",
    "        lc = c.lower()\n",
    "        if any(kw in lc for kw in cand_keywords):\n",
    "            abundance_candidates.append(c)\n",
    "    y = None\n",
    "    y_df = None\n",
    "    if abundance_candidates:\n",
    "        abundance_col = abundance_candidates[0]\n",
    "        print(f\"[FOUND] abundance-like column in metadata: '{abundance_col}' (using it as target)\")\n",
    "        # coerce to numeric\n",
    "        try:\n",
    "            y = pd.to_numeric(meta[abundance_col], errors=\"coerce\")\n",
    "        except Exception:\n",
    "            y = meta[abundance_col]\n",
    "        n_nan = int(y.isna().sum()) if hasattr(y, \"isna\") else 0\n",
    "        if n_nan > 0:\n",
    "            print(f\"[WARN] {n_nan} rows in the abundance column are not numeric / NaN (they will be handled later).\")\n",
    "    else:\n",
    "        # look for external label files inside EXTRACT_DIR\n",
    "        print(\"[INFO] No abundance-like column in metadata; searching for label CSVs in extracted/ ...\")\n",
    "        candidates_files = list(EXTRACT_DIR.glob(\"abundance*.csv\")) + list(EXTRACT_DIR.glob(\"*abund*.csv\")) + list(EXTRACT_DIR.glob(\"labels*.csv\")) + list(EXTRACT_DIR.glob(\"*labels*.csv\"))\n",
    "        if candidates_files:\n",
    "            opened = False\n",
    "            for p in sorted(set(candidates_files)):\n",
    "                try:\n",
    "                    df = pd.read_csv(p)\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] could not read {p.name}: {e}\")\n",
    "                    continue\n",
    "                cols_lower = [c.lower() for c in df.columns]\n",
    "                if \"id\" in cols_lower or \"accession\" in cols_lower or \"seqid\" in cols_lower:\n",
    "                    # pick numeric column for abundance\n",
    "                    id_col = df.columns[cols_lower.index(\"id\")] if \"id\" in cols_lower else df.columns[cols_lower.index(\"accession\")] if \"accession\" in cols_lower else df.columns[cols_lower.index(\"seqid\")]\n",
    "                    numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
    "                    if not numeric_cols:\n",
    "                        # try keywords\n",
    "                        numeric_cols = [c for c in df.columns if any(kw in c.lower() for kw in cand_keywords)]\n",
    "                    if numeric_cols:\n",
    "                        abundance_col = numeric_cols[0]\n",
    "                        y_df = df[[id_col, abundance_col]].rename(columns={id_col: \"id\", abundance_col: \"abundance\"})\n",
    "                        print(f\"[USING] external label file: {p.name} -> id='{id_col}', abundance='{abundance_col}'\")\n",
    "                        opened = True\n",
    "                        break\n",
    "            if not opened:\n",
    "                print(\"[WARN] found candidate CSVs but none had clear numeric abundance+id columns.\")\n",
    "        else:\n",
    "            print(\"[INFO] No candidate label CSVs found in extracted/\")\n",
    "\n",
    "    # Assemble final dataset and save\n",
    "    out_csv = EXTRACT_DIR / \"abundance_dataset.csv\"\n",
    "    if y is not None:\n",
    "        # assume meta order corresponds to X order; validate lengths\n",
    "        if len(meta) == X.shape[0]:\n",
    "            df_out = meta.reset_index(drop=True).copy()\n",
    "            df_out[\"abundance_target\"] = y.values\n",
    "            df_out[\"pca_index\"] = np.arange(len(df_out))\n",
    "            try:\n",
    "                df_out.to_csv(out_csv, index=False)\n",
    "                print(f\"[SAVED] merged abundance dataset -> {out_csv} (rows={len(df_out)})\")\n",
    "            except Exception as e:\n",
    "                print(\"[ERROR] failed to save abundance_dataset.csv:\", e)\n",
    "            print(\"\\n[SAMPLE]\")\n",
    "            print(df_out.head().to_string(index=False))\n",
    "        else:\n",
    "            # lengths mismatch: save partial with aligned index where possible\n",
    "            print(\"[WARN] metadata rows != embeddings rows. Attempting to save partial merged dataset with pca_index where possible.\")\n",
    "            df_out = meta.reset_index(drop=True).copy()\n",
    "            df_out[\"abundance_target\"] = y.values if len(y)==len(df_out) else pd.Series(y).reindex(df_out.index)\n",
    "            df_out[\"pca_index\"] = pd.Series(range(len(df_out)))\n",
    "            try:\n",
    "                df_out.to_csv(EXTRACT_DIR / \"abundance_dataset_partial.csv\", index=False)\n",
    "                print(f\"[SAVED] partial dataset -> {EXTRACT_DIR / 'abundance_dataset_partial.csv'} (rows={len(df_out)})\")\n",
    "            except Exception as e:\n",
    "                print(\"[ERROR] failed to save partial dataset:\", e)\n",
    "            print(\"Please check lengths: embeddings rows =\", X.shape[0], \"meta rows =\", len(meta))\n",
    "    elif y_df is not None:\n",
    "        # merge external labels with meta on 'id'\n",
    "        merged = pd.merge(meta, y_df, on=\"id\", how=\"inner\")\n",
    "        if merged.empty:\n",
    "            print(\"[ERROR] merging with external label file produced 0 rows. Check the 'id' values for exact matches (case sensitive).\")\n",
    "            print(\"Example meta ids:\", list(meta['id'].astype(str).head(10)))\n",
    "            print(\"Example label ids:\", list(y_df['id'].astype(str).head(10)))\n",
    "            return\n",
    "        merged = merged.reset_index(drop=True).rename(columns={merged.columns[-1]:\"abundance_target\"}) if merged.columns[-1]!=\"abundance_target\" else merged\n",
    "        try:\n",
    "            merged.to_csv(out_csv, index=False)\n",
    "            print(f\"[SAVED] merged abundance dataset -> {out_csv} (rows={len(merged)})\")\n",
    "            print(\"\\n[SAMPLE]\")\n",
    "            print(merged.head().to_string(index=False))\n",
    "        except Exception as e:\n",
    "            print(\"[ERROR] failed to save merged abundance dataset:\", e)\n",
    "            return\n",
    "    else:\n",
    "        print(\"\\n[NO TARGET FOUND]\")\n",
    "        print(\"I couldn't find a numeric abundance column in metadata or a label CSV.\")\n",
    "        print(\"To proceed, either:\")\n",
    "        print(\"  - place a CSV with columns 'id' and 'abundance' into:\")\n",
    "        print(f\"      {EXTRACT_DIR}  (recommended) or\")\n",
    "        print(f\"      {Path.cwd()} or another folder listed above and re-run this cell.\")\n",
    "        print(\"  - OR edit this cell and set DOWNLOAD_DIR to your exact folder path, e.g.:\")\n",
    "        print(r'      DOWNLOAD_DIR = Path(r\"C:\\Users\\Srijit\\OneDrive\\Desktop\\sihtaxa\\sihabundance\\ncbi_blast_db\")')\n",
    "        return\n",
    "\n",
    "    print(\"\\n[READY] If abundance_dataset.csv exists in the extracted folder you can now run the training cell (Cell 2).\")\n",
    "    return\n",
    "\n",
    "# run main (safe)\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9a3e89a1-94aa-4e08-bb95-71bc79aab68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PATHS] EXTRACT_DIR: C:\\Users\\Srijit\\OneDrive\\Desktop\\sihtaxa\\sihabundance\\ncbi_blast_db\\extracted\n",
      "[FILES] looking for: abundance_dataset.csv  (fallback: abundance_dataset_partial.csv)\n",
      "[FILES] embeddings PCA: embeddings_pca.npy\n",
      "[ERROR] No abundance dataset found. Place a CSV named 'abundance_dataset.csv' (columns: id, abundance) into:\n",
      "  C:\\Users\\Srijit\\OneDrive\\Desktop\\sihtaxa\\sihabundance\\ncbi_blast_db\\extracted\n",
      "Then re-run this cell.\n",
      "Cell finished without training. Fix the issue above and re-run.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 (modified & defensive) -- train / evaluate abundance regressor and save it\n",
    "import time, traceback\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# ---------- EDITED default paths for your environment ----------\n",
    "DOWNLOAD_DIR = Path(r\"C:\\Users\\Srijit\\OneDrive\\Desktop\\sihtaxa\\sihabundance\\ncbi_blast_db\")\n",
    "EXTRACT_DIR  = DOWNLOAD_DIR / \"extracted\"\n",
    "ABUND_CSV    = EXTRACT_DIR / \"abundance_dataset.csv\"\n",
    "ABUND_PARTIAL= EXTRACT_DIR / \"abundance_dataset_partial.csv\"\n",
    "EMB_PCA      = EXTRACT_DIR / \"embeddings_pca.npy\"\n",
    "META_CSV     = EXTRACT_DIR / \"embeddings_meta_clustered.csv\"\n",
    "MODEL_OUT    = EXTRACT_DIR / \"abundance_model.joblib\"\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "print(f\"[PATHS] EXTRACT_DIR: {EXTRACT_DIR}\")\n",
    "print(f\"[FILES] looking for: {ABUND_CSV.name}  (fallback: {ABUND_PARTIAL.name})\")\n",
    "print(f\"[FILES] embeddings PCA: {EMB_PCA.name}\")\n",
    "\n",
    "# defensive helper to exit this cell gracefully\n",
    "def stop(msg):\n",
    "    print(msg)\n",
    "    print(\"Cell finished without training. Fix the issue above and re-run.\")\n",
    "    return\n",
    "\n",
    "# 1) file existence checks\n",
    "if not EMB_PCA.exists():\n",
    "    stop(f\"[ERROR] Embeddings PCA file not found at: {EMB_PCA}\")\n",
    "    # do not raise an exception; return\n",
    "else:\n",
    "    try:\n",
    "        X_pca = np.load(EMB_PCA)\n",
    "    except Exception as e:\n",
    "        stop(f\"[ERROR] Failed to load embeddings from {EMB_PCA}: {e}\")\n",
    "        X_pca = None\n",
    "\n",
    "# choose abundance CSV (prefer full, fallback to partial)\n",
    "if ABUND_CSV.exists():\n",
    "    abund_path = ABUND_CSV\n",
    "elif ABUND_PARTIAL.exists():\n",
    "    abund_path = ABUND_PARTIAL\n",
    "    print(f\"[WARN] full abundance_dataset.csv not found; using partial: {ABUND_PARTIAL.name}\")\n",
    "else:\n",
    "    stop(f\"[ERROR] No abundance dataset found. Place a CSV named '{ABUND_CSV.name}' (columns: id, abundance) into:\\n  {EXTRACT_DIR}\\nThen re-run this cell.\")\n",
    "    abund_path = None\n",
    "\n",
    "if X_pca is None or abund_path is None:\n",
    "    # we've already printed an error; stop\n",
    "    pass\n",
    "else:\n",
    "    try:\n",
    "        df = pd.read_csv(abund_path)\n",
    "    except Exception as e:\n",
    "        stop(f\"[ERROR] Failed to read abundance CSV {abund_path}: {e}\")\n",
    "        df = None\n",
    "\n",
    "    if df is None:\n",
    "        pass\n",
    "    else:\n",
    "        print(f\"[LOAD] abundance dataset rows={len(df)}; embeddings shape={X_pca.shape}\")\n",
    "\n",
    "        # pick abundance target column\n",
    "        possible_target_cols = [c for c in df.columns if c.lower() in (\"abundance_target\",\"abundance\",\"count\",\"abund\",\"reads\",\"value\")]\n",
    "        if not possible_target_cols:\n",
    "            numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c]) and c.lower()!=\"pca_index\"]\n",
    "            if numeric_cols:\n",
    "                possible_target_cols = [numeric_cols[-1]]\n",
    "        if not possible_target_cols:\n",
    "            stop(\"[ERROR] Could not detect numeric abundance column in dataset. Columns found:\\n  \" + \", \".join(list(df.columns)))\n",
    "        else:\n",
    "            target_cols = possible_target_cols if isinstance(possible_target_cols, (list,tuple)) else [possible_target_cols]\n",
    "            print(f\"[TARGETS] Using target columns: {target_cols}\")\n",
    "\n",
    "            # Align embeddings <-> df rows\n",
    "            X_use = None\n",
    "            df_use = None\n",
    "\n",
    "            # if pca_index present, use it (preferred)\n",
    "            if \"pca_index\" in df.columns:\n",
    "                try:\n",
    "                    idx = df[\"pca_index\"].astype(int).values\n",
    "                    if idx.max() >= X_pca.shape[0] or idx.min() < 0:\n",
    "                        print(\"[WARN] pca_index contains out-of-range values; falling back to row-order alignment.\")\n",
    "                    else:\n",
    "                        X_use = X_pca[idx]\n",
    "                        df_use = df.reset_index(drop=True)\n",
    "                        print(f\"[ALIGN] aligned via pca_index, rows={len(df_use)}\")\n",
    "                except Exception as e:\n",
    "                    print(\"[WARN] pca_index exists but failed to use it:\", e)\n",
    "\n",
    "            # id-based alignment if pca_index not used\n",
    "            if X_use is None:\n",
    "                if META_CSV.exists():\n",
    "                    try:\n",
    "                        meta = pd.read_csv(META_CSV)\n",
    "                        if \"id\" in meta.columns:\n",
    "                            id_to_index = {rid: i for i, rid in enumerate(meta[\"id\"].astype(str).values)}\n",
    "                            if \"id\" in df.columns:\n",
    "                                matched_indices = []\n",
    "                                keep_rows = []\n",
    "                                for i, rid in enumerate(df[\"id\"].astype(str).values):\n",
    "                                    if rid in id_to_index:\n",
    "                                        matched_indices.append(id_to_index[rid])\n",
    "                                        keep_rows.append(i)\n",
    "                                if len(matched_indices) > 0:\n",
    "                                    X_use = X_pca[matched_indices]\n",
    "                                    df_use = df.iloc[keep_rows].reset_index(drop=True)\n",
    "                                    print(f\"[ALIGN] matched {len(matched_indices)} rows by 'id' to embeddings\")\n",
    "                                else:\n",
    "                                    print(\"[WARN] Could not match ids between abundance CSV and metadata; falling back to top-row alignment.\")\n",
    "                            else:\n",
    "                                print(\"[WARN] abundance CSV has no 'id' column; falling back to top-row alignment.\")\n",
    "                        else:\n",
    "                            print(\"[WARN] meta CSV exists but has no 'id' column; falling back to top-row alignment.\")\n",
    "                    except Exception as e:\n",
    "                        print(\"[WARN] Failed to read/parse meta CSV for id-alignment:\", e)\n",
    "\n",
    "            # last resort: align by top-n rows\n",
    "            if X_use is None:\n",
    "                min_n = min(X_pca.shape[0], len(df))\n",
    "                X_use = X_pca[:min_n]\n",
    "                df_use = df.iloc[:min_n].reset_index(drop=True)\n",
    "                print(f\"[ALIGN] fallback top-row alignment used. Using first {min_n} rows.\")\n",
    "\n",
    "            # Prepare Y array\n",
    "            try:\n",
    "                Y = df_use[target_cols].copy()\n",
    "                if Y.shape[1] == 1:\n",
    "                    y = Y.iloc[:,0].astype(float).values\n",
    "                else:\n",
    "                    y = Y.astype(float).values\n",
    "            except Exception as e:\n",
    "                stop(f\"[ERROR] Failed to prepare target y: {e}\")\n",
    "                y = None\n",
    "\n",
    "            if y is None:\n",
    "                pass\n",
    "            else:\n",
    "                # drop rows with NaN target\n",
    "                if np.isnan(y).any():\n",
    "                    mask = ~np.isnan(y).any(axis=1) if y.ndim>1 else ~np.isnan(y)\n",
    "                    before = X_use.shape[0]\n",
    "                    X_use = X_use[mask]\n",
    "                    y = y[mask]\n",
    "                    after = X_use.shape[0]\n",
    "                    print(f\"[CLEAN] Dropped {before-after} rows with NaN target. New shapes X={X_use.shape}, y={y.shape}\")\n",
    "\n",
    "                if X_use.shape[0] < 5:\n",
    "                    stop(f\"[ERROR] Not enough rows after alignment/cleaning to train (found {X_use.shape[0]} rows). Need more labeled rows.\")\n",
    "                else:\n",
    "                    # Train/test split\n",
    "                    RANDOM_SEED = 42\n",
    "                    test_size = 0.15 if X_use.shape[0] >= 20 else 0.2\n",
    "                    X_train, X_val, y_train, y_val = train_test_split(X_use, y, test_size=test_size, random_state=RANDOM_SEED)\n",
    "                    print(f\"[SPLIT] train={len(X_train)} val={len(X_val)}\")\n",
    "\n",
    "                    # choose/model\n",
    "                    base = RandomForestRegressor(n_estimators=200, n_jobs=-1, random_state=RANDOM_SEED)\n",
    "                    if y_train.ndim == 1 or (hasattr(y_train, \"shape\") and y_train.shape[1] == 1):\n",
    "                        model = base\n",
    "                    else:\n",
    "                        model = MultiOutputRegressor(base, n_jobs=-1)\n",
    "\n",
    "                    # fit with try/except\n",
    "                    t0 = time.time()\n",
    "                    try:\n",
    "                        print(\"[TRAIN] fitting RandomForest regressor (this may take some time)...\")\n",
    "                        model.fit(X_train, y_train)\n",
    "                        t1 = time.time()\n",
    "                        print(f\"[TRAINED] fit time: {t1-t0:.2f}s\")\n",
    "                    except Exception as e:\n",
    "                        print(\"[ERROR] training failed:\", e)\n",
    "                        traceback.print_exc()\n",
    "                        stop(\"Training aborted due to error.\")\n",
    "                        model = None\n",
    "\n",
    "                    if model is not None:\n",
    "                        try:\n",
    "                            y_pred = model.predict(X_val)\n",
    "                            if y_pred.ndim == 1:\n",
    "                                r2 = r2_score(y_val, y_pred)\n",
    "                                mae = mean_absolute_error(y_val, y_pred)\n",
    "                                mse = mean_squared_error(y_val, y_pred)\n",
    "                                print(f\"[EVAL] R2={r2:.4f}  MAE={mae:.4f}  MSE={mse:.4f}\")\n",
    "                            else:\n",
    "                                r2 = r2_score(y_val, y_pred, multioutput='uniform_average')\n",
    "                                mae = mean_absolute_error(y_val, y_pred, multioutput='uniform_average')\n",
    "                                mse = mean_squared_error(y_val, y_pred, multioutput='uniform_average')\n",
    "                                print(f\"[EVAL multi] R2(avg)={r2:.4f}  MAE(avg)={mae:.4f}  MSE(avg)={mse:.4f}\")\n",
    "                        except Exception as e:\n",
    "                            print(\"[WARN] evaluation failed:\", e)\n",
    "\n",
    "                        # save model artifact\n",
    "                        try:\n",
    "                            joblib.dump({\n",
    "                                \"model\": model,\n",
    "                                \"target_cols\": target_cols,\n",
    "                                \"feature_dim\": X_use.shape[1],\n",
    "                                \"train_date\": time.ctime(),\n",
    "                            }, MODEL_OUT)\n",
    "                            print(f\"[SAVED] model + metadata -> {MODEL_OUT}\")\n",
    "                        except Exception as e:\n",
    "                            print(\"[ERROR] Failed to save model:\", e)\n",
    "\n",
    "                        # helper function exposed in this cell\n",
    "                        def predict_abundance_from_embedding(embeddings):\n",
    "                            info = joblib.load(MODEL_OUT)\n",
    "                            mdl = info[\"model\"]\n",
    "                            expected_dim = info[\"feature_dim\"]\n",
    "                            arr = np.asarray(embeddings)\n",
    "                            if arr.ndim == 1:\n",
    "                                arr = arr.reshape(1, -1)\n",
    "                            if arr.shape[1] != expected_dim:\n",
    "                                raise ValueError(f\"Embedding dimension mismatch. Model expects {expected_dim} features; got {arr.shape[1]}.\")\n",
    "                            preds = mdl.predict(arr)\n",
    "                            return preds\n",
    "\n",
    "                        # sample predictions (safely)\n",
    "                        try:\n",
    "                            sample_pred = predict_abundance_from_embedding(X_val[:3])\n",
    "                            print(\"[SAMPLE PREDICTIONS] for first 3 validation rows ->\")\n",
    "                            print(sample_pred)\n",
    "                        except Exception as e:\n",
    "                            print(\"[WARN] sample prediction failed:\", e)\n",
    "\n",
    "                        print(\"\\n[DONE] Training complete. Use `predict_abundance_from_embedding(embedding_array)` to predict.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8eadfe3b-2959-4c8a-b335-055720c9369f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FOUND] predictions CSV -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\predictions_with_uncertainty.csv\n",
      "[USING] species column = 'species_pred_label'\n",
      "[USING] species confidence column = 'species_pred_conf'\n",
      "[SAVED] count-based abundance -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\abundance_from_predictions.csv  (rows=52)\n",
      "[SAVED] confidence-weighted abundance -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\abundance_from_predictions_weighted.csv  (rows=52)\n",
      "[SAVED] kingdom-level abundance -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\abundance_kingdom.csv  (rows=2)\n",
      "[SAVED] phylum-level abundance -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\abundance_phylum.csv  (rows=5)\n",
      "[SAVED] class-level abundance -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\abundance_class.csv  (rows=8)\n",
      "[SAVED] order-level abundance -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\abundance_order.csv  (rows=11)\n",
      "[SAVED] family-level abundance -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\abundance_family.csv  (rows=13)\n",
      "[SAVED] genus-level abundance -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\abundance_genus.csv  (rows=18)\n",
      "\n",
      "Top 10 species by count-based abundance:\n",
      "                    species  count  relative_abundance\n",
      "                 UNASSIGNED    144            0.378947\n",
      "            Maylandia zebra     80            0.210526\n",
      "           Chaetodon auriga     46            0.121053\n",
      "              Morchella sp.     10            0.026316\n",
      "      Arvicanthis niloticus      9            0.023684\n",
      "  Aspergillus costaricensis      7            0.018421\n",
      "Callospermophilus lateralis      6            0.015789\n",
      "     Hysterothylacium fabri      5            0.013158\n",
      "               Entoloma sp.      5            0.013158\n",
      "        Amanita fuscozonata      4            0.010526\n",
      "\n",
      "Top 10 species by confidence-weighted abundance:\n",
      "                    species  conf_sum  relative_abundance_weighted\n",
      "                 UNASSIGNED 89.403568                     0.396608\n",
      "            Maylandia zebra 59.536462                     0.264113\n",
      "           Chaetodon auriga 19.165359                     0.085020\n",
      "      Arvicanthis niloticus  7.736281                     0.034319\n",
      "  Aspergillus costaricensis  4.226915                     0.018751\n",
      "              Morchella sp.  3.351529                     0.014868\n",
      "     Hysterothylacium fabri  3.203144                     0.014210\n",
      " Chloroidium saccharophilum  2.551860                     0.011320\n",
      "               Eucoleus sp.  2.421103                     0.010740\n",
      "Callospermophilus lateralis  2.125962                     0.009431\n",
      "\n",
      "[DONE] You can open the CSV(s) in the extracted folder. If you want a different aggregation (e.g. per-sample abundances, per-contig, or hierarchical aggregation by taxonomic path) tell me and I'll give the exact cell for that.\n"
     ]
    }
   ],
   "source": [
    "# Abundance-from-predictions cell (run this in your notebook)\n",
    "# - searches for predictions CSVs that the inference step writes\n",
    "# - computes count-based and confidence-weighted relative abundances per species (and per higher ranks optionally)\n",
    "# - saves results to extracted/abundance_from_predictions.csv and abundance_from_predictions_weighted.csv\n",
    "# - does NOT change the classifier; only aggregates its outputs\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import textwrap\n",
    "\n",
    "# Candidate parent folders to search (no single hardcoded path only)\n",
    "candidates = [\n",
    "    Path.cwd() / \"sih\" / \"ncbi_blast_db\" / \"extracted\",\n",
    "    Path.cwd() / \"ncbi_blast_db\" / \"extracted\",\n",
    "    Path.cwd() / \"extracted\",\n",
    "    Path.cwd(),\n",
    "    Path(r\"C:\\Users\\Srijit\\OneDrive\\Desktop\\sihtaxa\\sihabundance\\ncbi_blast_db\\extracted\"),\n",
    "    Path(r\"C:\\Users\\HP\\sihabundance\\ncbi_blast_db\\extracted\"),\n",
    "    Path.home() / \"OneDrive\" / \"Desktop\" / \"sihtaxa\" / \"sihabundance\" / \"ncbi_blast_db\" / \"extracted\",\n",
    "]\n",
    "\n",
    "# also look for any \"extracted\" subfolder under cwd (scan shallow)\n",
    "for p in Path.cwd().glob(\"**/extracted\"):\n",
    "    candidates.append(p.resolve())\n",
    "\n",
    "# make unique and keep only existing directories for informational order\n",
    "seen = []\n",
    "candidates_clean = []\n",
    "for p in candidates:\n",
    "    if p not in seen:\n",
    "        seen.append(p)\n",
    "        candidates_clean.append(p)\n",
    "candidates = candidates_clean\n",
    "\n",
    "# filenames we expect from previous inference step (in order of preference)\n",
    "pred_filenames = [\n",
    "    \"predictions_with_uncertainty.csv\",\n",
    "    \"predictions.csv\",\n",
    "    \"val_predictions_calibrated.csv\",\n",
    "    \"val_predictions.csv\",\n",
    "    \"predictions_with_uncertainty_latest.csv\",\n",
    "]\n",
    "\n",
    "found = None\n",
    "found_path = None\n",
    "for extracted in candidates:\n",
    "    if not extracted.exists():\n",
    "        continue\n",
    "    for name in pred_filenames:\n",
    "        p = extracted / name\n",
    "        if p.exists():\n",
    "            found = name\n",
    "            found_path = p\n",
    "            break\n",
    "    if found_path:\n",
    "        break\n",
    "\n",
    "# also try scanning any extracted folder for matching filenames\n",
    "if found_path is None:\n",
    "    for p in Path.cwd().rglob(\"predictions_with_uncertainty.csv\"):\n",
    "        found_path = p.resolve()\n",
    "        break\n",
    "\n",
    "# final fallback: any CSV in extracted folder containing 'species' / 'species_pred' columns\n",
    "if found_path is None:\n",
    "    for extracted in candidates:\n",
    "        if not extracted.exists():\n",
    "            continue\n",
    "        for p in extracted.glob(\"*.csv\"):\n",
    "            try:\n",
    "                tmp = pd.read_csv(p, nrows=5)\n",
    "            except Exception:\n",
    "                continue\n",
    "            cols = [c.lower() for c in tmp.columns]\n",
    "            if any(c in cols for c in (\"species_pred_label\", \"species_label\", \"species_pred\", \"species\")):\n",
    "                found_path = p\n",
    "                break\n",
    "        if found_path:\n",
    "            break\n",
    "\n",
    "if found_path is None:\n",
    "    print(textwrap.dedent(f\"\"\"\n",
    "    [ERROR] Could not find inference predictions CSV in the usual places.\n",
    "    I searched the following candidate extracted folders (in this order):\n",
    "      {', '.join(str(x) for x in candidates)}\n",
    "    Expected one of these files (examples): {pred_filenames}\n",
    "    If you have not yet run the inference step that writes predictions (predictions_with_uncertainty.csv),\n",
    "    please run it first. Otherwise, place the CSV into an 'extracted/' folder and re-run this cell.\n",
    "\n",
    "    Alternatively, if you *do* have a predictions CSV but with a different name, move or rename it to:\n",
    "      predictions_with_uncertainty.csv\n",
    "    or place it into one of the candidate 'extracted' folders above.\n",
    "    \"\"\"))\n",
    "else:\n",
    "    print(f\"[FOUND] predictions CSV -> {found_path}\")\n",
    "    try:\n",
    "        df = pd.read_csv(found_path)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to read CSV {found_path}: {e}\")\n",
    "        df = None\n",
    "\n",
    "    if df is None:\n",
    "        pass\n",
    "    else:\n",
    "        # find species label column (common variants)\n",
    "        col_candidates = [\n",
    "            \"species_pred_label\", \"species_label\", \"species_pred\", \"species_predicted\", \"species\",\n",
    "            \"species_prediction_label\", \"species_pred_lbl\"\n",
    "        ]\n",
    "        cols_lower = {c.lower(): c for c in df.columns}\n",
    "        species_col = None\n",
    "        for cand in col_candidates:\n",
    "            if cand in cols_lower:\n",
    "                species_col = cols_lower[cand]\n",
    "                break\n",
    "\n",
    "        # try other heuristics if not found\n",
    "        if species_col is None:\n",
    "            # look for any column containing the word 'species' (case-insensitive)\n",
    "            for c in df.columns:\n",
    "                if \"species\" in c.lower():\n",
    "                    species_col = c\n",
    "                    break\n",
    "\n",
    "        if species_col is None:\n",
    "            print(\"[ERROR] Could not find a 'species' prediction column in the predictions CSV.\")\n",
    "            print(\"Columns present:\", list(df.columns)[:50])\n",
    "            print(\"If your predictions file uses a different column name for species predictions,\")\n",
    "            print(\"please rename or let me know the column name. I will try to detect it automatically next.\")\n",
    "        else:\n",
    "            print(f\"[USING] species column = '{species_col}'\")\n",
    "\n",
    "            # optional: column containing species probability/confidence\n",
    "            conf_candidates = [\"species_pred_conf\", \"species_conf\", \"species_prob\", \"species_probability\", \"species_pred_proba\", \"species_proba\"]\n",
    "            conf_col = None\n",
    "            for cand in conf_candidates:\n",
    "                if cand in cols_lower:\n",
    "                    conf_col = cols_lower[cand]\n",
    "                    break\n",
    "            # try again heuristically\n",
    "            if conf_col is None:\n",
    "                for c in df.columns:\n",
    "                    if \"conf\" in c.lower() or \"prob\" in c.lower() or \"score\" in c.lower():\n",
    "                        if \"species\" in c.lower() or \"pred\" in c.lower():\n",
    "                            conf_col = c\n",
    "                            break\n",
    "\n",
    "            if conf_col is None:\n",
    "                print(\"[INFO] No explicit species confidence column detected. Weighted abundance will be skipped.\")\n",
    "            else:\n",
    "                print(f\"[USING] species confidence column = '{conf_col}'\")\n",
    "\n",
    "            # compute count-based abundance\n",
    "            df_species = df[[species_col]].copy()\n",
    "            df_species = df_species.dropna(subset=[species_col])\n",
    "            counts = df_species[species_col].value_counts(dropna=True).rename_axis(\"species\").reset_index(name=\"count\")\n",
    "            total = counts[\"count\"].sum()\n",
    "            counts[\"relative_abundance\"] = counts[\"count\"] / total\n",
    "            counts = counts.sort_values(\"relative_abundance\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "            # compute confidence-weighted abundance if possible\n",
    "            if conf_col is not None and conf_col in df.columns:\n",
    "                df_conf = df[[species_col, conf_col]].copy()\n",
    "                # coerce conf to numeric; invalids -> NaN -> dropped\n",
    "                df_conf[conf_col] = pd.to_numeric(df_conf[conf_col], errors=\"coerce\")\n",
    "                df_conf = df_conf.dropna(subset=[species_col, conf_col])\n",
    "                # groupby species and sum confidences\n",
    "                weighted = df_conf.groupby(species_col)[conf_col].sum().rename(\"conf_sum\").reset_index()\n",
    "                total_conf = weighted[\"conf_sum\"].sum()\n",
    "                if total_conf <= 0:\n",
    "                    print(\"[WARN] Sum of confidences <= 0; skipping weighted abundance.\")\n",
    "                    weighted = None\n",
    "                else:\n",
    "                    weighted[\"relative_abundance_weighted\"] = weighted[\"conf_sum\"] / total_conf\n",
    "                    weighted = weighted.rename(columns={species_col: \"species\"})\n",
    "                    weighted = weighted.sort_values(\"relative_abundance_weighted\", ascending=False).reset_index(drop=True)\n",
    "            else:\n",
    "                weighted = None\n",
    "\n",
    "            # optional: compute per-higher-rank abundances if higher-rank columns exist\n",
    "            # search for phylum/class/order/family/genus columns\n",
    "            rank_cols = {}\n",
    "            for rank in (\"kingdom\",\"phylum\",\"class\",\"order\",\"family\",\"genus\"):\n",
    "                for c in df.columns:\n",
    "                    if rank in c.lower():\n",
    "                        rank_cols[rank] = c\n",
    "                        break\n",
    "\n",
    "            rank_abundances = {}\n",
    "            for rank, col in rank_cols.items():\n",
    "                tmp = df[[col]].dropna()\n",
    "                counts_rank = tmp[col].value_counts().rename_axis(rank).reset_index(name=\"count\")\n",
    "                counts_rank[\"relative_abundance\"] = counts_rank[\"count\"] / counts_rank[\"count\"].sum()\n",
    "                rank_abundances[rank] = counts_rank\n",
    "\n",
    "            # Save outputs to same extracted folder\n",
    "            out_dir = found_path.parent\n",
    "            out_counts = out_dir / \"abundance_from_predictions.csv\"\n",
    "            out_weighted = out_dir / \"abundance_from_predictions_weighted.csv\"\n",
    "            try:\n",
    "                counts.to_csv(out_counts, index=False)\n",
    "                print(f\"[SAVED] count-based abundance -> {out_counts}  (rows={len(counts)})\")\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] failed to save count-based CSV: {e}\")\n",
    "\n",
    "            if weighted is not None:\n",
    "                try:\n",
    "                    weighted.to_csv(out_weighted, index=False)\n",
    "                    print(f\"[SAVED] confidence-weighted abundance -> {out_weighted}  (rows={len(weighted)})\")\n",
    "                except Exception as e:\n",
    "                    print(f\"[ERROR] failed to save weighted CSV: {e}\")\n",
    "\n",
    "            # Save per-rank abundances if present\n",
    "            for rank, df_rank in rank_abundances.items():\n",
    "                out_rank = out_dir / f\"abundance_{rank}.csv\"\n",
    "                try:\n",
    "                    df_rank.to_csv(out_rank, index=False)\n",
    "                    print(f\"[SAVED] {rank}-level abundance -> {out_rank}  (rows={len(df_rank)})\")\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] failed to save {rank} abundance: {e}\")\n",
    "\n",
    "            # print top-10 species by count and weighted (if available)\n",
    "            print(\"\\nTop 10 species by count-based abundance:\")\n",
    "            print(counts.head(10).to_string(index=False))\n",
    "\n",
    "            if weighted is not None:\n",
    "                print(\"\\nTop 10 species by confidence-weighted abundance:\")\n",
    "                print(weighted.head(10).to_string(index=False))\n",
    "\n",
    "            print(\"\\n[DONE] You can open the CSV(s) in the extracted folder. If you want a different aggregation (e.g. per-sample abundances, per-contig, or hierarchical aggregation by taxonomic path) tell me and I'll give the exact cell for that.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "47b04f18-65e9-49d4-81a3-3823b2b63887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FOUND] predictions -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\predictions_with_uncertainty.csv\n",
      "[FOUND] validation preds -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\val_predictions_calibrated.csv\n",
      "[USING] species column: 'species_pred_label' , confidence column: 'species_pred_conf'\n",
      "[CONFUSION] built confusion matrix from validation with 72 classes.\n",
      "[SAVED] raw count-based abundance -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\abundance_from_predictions.csv\n",
      "[SAVED] raw confidence-weighted abundance -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\abundance_from_predictions_weighted.csv\n",
      "[DECONV] Running NNLS deconvolution (projected gradient)...\n",
      "[SAVED] deconvolved counts -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\abundance_from_predictions_deconvolved.csv\n",
      "[SAVED] deconvolved weighted -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\abundance_from_predictions_deconvolved_weighted.csv\n",
      "\n",
      "Top 15 predicted (raw counts):\n",
      "                        species  count  rel_abund_count\n",
      "                     UNASSIGNED    144         0.378947\n",
      "                Maylandia zebra     80         0.210526\n",
      "               Chaetodon auriga     46         0.121053\n",
      "                  Morchella sp.     10         0.026316\n",
      "          Arvicanthis niloticus      9         0.023684\n",
      "      Aspergillus costaricensis      7         0.018421\n",
      "    Callospermophilus lateralis      6         0.015789\n",
      "         Hysterothylacium fabri      5         0.013158\n",
      "                   Entoloma sp.      5         0.013158\n",
      "                   Eucoleus sp.      4         0.010526\n",
      "            Amanita fuscozonata      4         0.010526\n",
      "             Trichoderma viride      3         0.007895\n",
      "     Chloroidium saccharophilum      3         0.007895\n",
      "       Pseudopestalotiopsis sp.      3         0.007895\n",
      "Deuterostichococcus epilithicus      3         0.007895\n",
      "\n",
      "Top 15 estimated true (after deconvolution):\n",
      "                        species  pred_count  pred_count_rel  est_true_count  est_true_rel\n",
      "                     UNASSIGNED       144.0        0.378947      201.000143      0.528947\n",
      "                Maylandia zebra        80.0        0.210526       80.000056      0.210526\n",
      "               Chaetodon auriga        46.0        0.121053       22.000014      0.057895\n",
      "          Arvicanthis niloticus         9.0        0.023684        7.000003      0.018421\n",
      "                  Morchella sp.        10.0        0.026316        5.000002      0.013158\n",
      "           Aonchotheca annulosa         0.0        0.000000        4.000001      0.010526\n",
      "            Amanita fuscozonata         4.0        0.010526        4.000001      0.010526\n",
      "     Chloroidium saccharophilum         3.0        0.007895        3.000000      0.007895\n",
      "       Pseudopestalotiopsis sp.         3.0        0.007895        3.000000      0.007895\n",
      "Deuterostichococcus epilithicus         3.0        0.007895        3.000000      0.007895\n",
      "          Nannizziopsis guarroi         0.0        0.000000        2.000000      0.005263\n",
      "    Cardimyxobolus iriomotensis         2.0        0.005263        2.000000      0.005263\n",
      "                    Inocybe sp.         0.0        0.000000        2.000000      0.005263\n",
      "        Baruscapillaria inflexa         0.0        0.000000        2.000000      0.005263\n",
      "                Cortinarius sp.         2.0        0.005263        2.000000      0.005263\n",
      "\n",
      "[DONE] Outputs saved to: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\n"
     ]
    }
   ],
   "source": [
    "# Fixed cell: robust confusion-based deconvolution (handles the KeyError & column/variable issues)\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import traceback\n",
    "import sys\n",
    "\n",
    "def find_file_in_candidates(names):\n",
    "    candidates = [\n",
    "        Path.cwd() / \"sih\" / \"ncbi_blast_db\" / \"extracted\",\n",
    "        Path.cwd() / \"ncbi_blast_db\" / \"extracted\",\n",
    "        Path.cwd() / \"extracted\",\n",
    "        Path.cwd(),\n",
    "        Path(r\"C:\\Users\\Srijit\\OneDrive\\Desktop\\sihtaxa\\sihabundance\\ncbi_blast_db\\extracted\"),\n",
    "        Path(r\"C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\"),\n",
    "        Path(r\"C:\\Users\\HP\\sihabundance\\ncbi_blast_db\\extracted\"),\n",
    "        Path.home() / \"OneDrive\" / \"Desktop\" / \"sihtaxa\" / \"sihabundance\" / \"ncbi_blast_db\" / \"extracted\",\n",
    "    ]\n",
    "    for p in Path.cwd().glob(\"**/extracted\"):\n",
    "        candidates.append(p.resolve())\n",
    "    visited = set()\n",
    "    for c in candidates:\n",
    "        if c in visited: continue\n",
    "        visited.add(c)\n",
    "        for n in names:\n",
    "            p = c / n\n",
    "            if p.exists():\n",
    "                return p\n",
    "    # fallback recursive search\n",
    "    for n in names:\n",
    "        for p in Path.cwd().rglob(n):\n",
    "            return p.resolve()\n",
    "    return None\n",
    "\n",
    "def normalize_label(s):\n",
    "    # convert to str, strip whitespace, collapse internal multiple spaces, keep case (you can optionally lower())\n",
    "    if pd.isna(s):\n",
    "        return \"UNASSIGNED\"\n",
    "    s = str(s).strip()\n",
    "    # collapse multiple spaces\n",
    "    s = \" \".join(s.split())\n",
    "    if s == \"\":\n",
    "        return \"UNASSIGNED\"\n",
    "    return s\n",
    "\n",
    "def nnls_pgd_solve(A, Pred, max_iter=3000, tol=1e-7, verbose=False):\n",
    "    M = A.T\n",
    "    m, k = M.shape\n",
    "    try:\n",
    "        x0, *_ = np.linalg.lstsq(M, Pred, rcond=None)\n",
    "        x = np.maximum(0.0, x0)\n",
    "    except Exception:\n",
    "        x = np.maximum(0.0, np.ones(k) * (Pred.sum() / max(1,k)))\n",
    "    try:\n",
    "        L = np.linalg.norm(M, ord=2)**2\n",
    "        if L <= 0: L = 1.0\n",
    "        lr = 1.0 / (L + 1e-10)\n",
    "    except Exception:\n",
    "        lr = 1e-3\n",
    "    prev_loss = None\n",
    "    for it in range(max_iter):\n",
    "        r = M.dot(x) - Pred\n",
    "        loss = 0.5 * (r @ r)\n",
    "        if prev_loss is not None and abs(prev_loss - loss) < tol:\n",
    "            break\n",
    "        prev_loss = loss\n",
    "        grad = M.T.dot(r)\n",
    "        x -= lr * grad\n",
    "        x = np.maximum(0.0, x)\n",
    "    return x\n",
    "\n",
    "try:\n",
    "    pred_path = find_file_in_candidates([\"predictions_with_uncertainty.csv\", \"predictions.csv\", \"predictions_with_uncertainty_latest.csv\"])\n",
    "    val_path  = find_file_in_candidates([\"val_predictions_calibrated.csv\", \"val_predictions.csv\", \"val_predictions_with_uncertainty.csv\", \"val_predictions_with_uncertainty.csv\"])\n",
    "    if pred_path is None:\n",
    "        print(\"[ERROR] predictions CSV not found. Run inference or place predictions_with_uncertainty.csv in an 'extracted/' folder.\")\n",
    "        raise SystemExit(0)\n",
    "    print(\"[FOUND] predictions ->\", pred_path)\n",
    "    if val_path is None:\n",
    "        print(\"[WARN] validation predictions not found; confusion correction will be skipped.\")\n",
    "    else:\n",
    "        print(\"[FOUND] validation preds ->\", val_path)\n",
    "\n",
    "    df_pred = pd.read_csv(pred_path)\n",
    "    cols_lower = {c.lower(): c for c in df_pred.columns}\n",
    "\n",
    "    # detect species prediction column robustly\n",
    "    species_candidates = [\"species_pred_label\", \"species_label\", \"species_pred\", \"species\"]\n",
    "    species_col = None\n",
    "    for c in species_candidates:\n",
    "        if c in cols_lower:\n",
    "            species_col = cols_lower[c]; break\n",
    "    if species_col is None:\n",
    "        for c in df_pred.columns:\n",
    "            if \"species\" in c.lower():\n",
    "                species_col = c; break\n",
    "    if species_col is None:\n",
    "        print(\"[ERROR] Could not find a species prediction column. Columns available:\", list(df_pred.columns))\n",
    "        raise SystemExit(0)\n",
    "    # detect confidence column (optional)\n",
    "    conf_candidates = [\"species_pred_conf\", \"species_conf\", \"species_prob\", \"species_probability\", \"species_pred_proba\"]\n",
    "    conf_col = None\n",
    "    for c in conf_candidates:\n",
    "        if c in cols_lower:\n",
    "            conf_col = cols_lower[c]; break\n",
    "\n",
    "    print(f\"[USING] species column: '{species_col}'\", f\", confidence column: '{conf_col}'\" if conf_col else \"\")\n",
    "\n",
    "    # normalize predicted species\n",
    "    df_pred[\"_species_norm\"] = df_pred[species_col].apply(normalize_label)\n",
    "    preds_series = df_pred[\"_species_norm\"]\n",
    "\n",
    "    counts_df = preds_series.value_counts(dropna=True).rename_axis(\"species\").reset_index(name=\"count\")\n",
    "    counts_df[\"rel_abund_count\"] = counts_df[\"count\"] / counts_df[\"count\"].sum()\n",
    "\n",
    "    weighted_df = None\n",
    "    if conf_col and conf_col in df_pred.columns:\n",
    "        df_pred[\"_conf_num\"] = pd.to_numeric(df_pred[conf_col], errors=\"coerce\").fillna(0.0)\n",
    "        weighted_df = df_pred.groupby(\"_species_norm\")[\"_conf_num\"].sum().reset_index().rename(columns={\"_species_norm\":\"species\", \"_conf_num\":\"conf_sum\"})\n",
    "        total_conf = weighted_df[\"conf_sum\"].sum()\n",
    "        if total_conf>0:\n",
    "            weighted_df[\"rel_abund_weighted\"] = weighted_df[\"conf_sum\"] / total_conf\n",
    "    else:\n",
    "        print(\"[INFO] no confidence column found -> weighted abundance will be skipped.\")\n",
    "\n",
    "    # Build confusion matrix if validation file available\n",
    "    confusion_possible = False\n",
    "    A = None\n",
    "    classes = None\n",
    "    if val_path is not None:\n",
    "        try:\n",
    "            df_val = pd.read_csv(val_path)\n",
    "            cols_val_lower = {c.lower(): c for c in df_val.columns}\n",
    "            # detect true label and predicted columns in validation\n",
    "            true_col = None\n",
    "            predcol_val = None\n",
    "            # try likely names\n",
    "            for cand in (\"species_true\",\"species_label_true\",\"true_species\",\"species_gold\",\"species_label\"):\n",
    "                if cand in cols_val_lower:\n",
    "                    true_col = cols_val_lower[cand]; break\n",
    "            for cand in (\"species_pred_label\",\"species_pred\",\"species_prediction\",\"species_label_pred\",\"pred_species\"):\n",
    "                if cand in cols_val_lower:\n",
    "                    predcol_val = cols_val_lower[cand]; break\n",
    "            # heuristics: look for any 'species' column that likely means true label vs pred\n",
    "            if true_col is None or predcol_val is None:\n",
    "                # collect columns containing 'species'\n",
    "                species_like = [c for c in df_val.columns if \"species\" in c.lower()]\n",
    "                # if exactly two species-like columns, guess one is true and other is pred (best-effort)\n",
    "                if len(species_like) >= 2 and (true_col is None or predcol_val is None):\n",
    "                    # pick first as true, second as pred unless names indicate otherwise\n",
    "                    if true_col is None:\n",
    "                        true_col = species_like[0]\n",
    "                    if predcol_val is None and len(species_like) > 1:\n",
    "                        predcol_val = species_like[1]\n",
    "            if true_col is None or predcol_val is None:\n",
    "                print(\"[WARN] Could not locate both true and predicted species columns in validation CSV. Columns:\", list(df_val.columns))\n",
    "                confusion_possible = False\n",
    "            else:\n",
    "                # normalize\n",
    "                true_norm = df_val[true_col].apply(normalize_label)\n",
    "                pred_norm_val = df_val[predcol_val].apply(normalize_label)\n",
    "                # classes = union of all possible species across preds and validation sets\n",
    "                classes_set = set(preds_series.unique()) | set(true_norm.unique()) | set(pred_norm_val.unique())\n",
    "                classes = sorted(classes_set)\n",
    "                idx = {c:i for i,c in enumerate(classes)}\n",
    "                n = len(classes)\n",
    "                A = np.zeros((n,n), dtype=float)  # rows=true, cols=pred\n",
    "                for t,p in zip(true_norm.values, pred_norm_val.values):\n",
    "                    # guard: if a label is not in classes (shouldn't happen), add dynamically\n",
    "                    if t not in idx:\n",
    "                        # extend structures (rare) - do dynamic expansion\n",
    "                        classes.append(t)\n",
    "                        idx[t] = len(idx)\n",
    "                        A = np.pad(A, ((0,1),(0,0)), mode='constant', constant_values=0.0)\n",
    "                    if p not in idx:\n",
    "                        classes.append(p)\n",
    "                        idx[p] = len(idx)\n",
    "                        A = np.pad(A, ((0,1),(0,0)), mode='constant', constant_values=0.0)\n",
    "                    A[idx[t], idx[p]] += 1.0\n",
    "                # row-normalize with tiny smoothing for stability\n",
    "                row_sums = A.sum(axis=1, keepdims=True)\n",
    "                row_sums[row_sums==0] = 1.0\n",
    "                A = A / row_sums\n",
    "                # tiny smoothing to avoid singular rows\n",
    "                eps = 1e-8\n",
    "                A = (A + eps)\n",
    "                A = A / A.sum(axis=1, keepdims=True)\n",
    "                confusion_possible = True\n",
    "                print(f\"[CONFUSION] built confusion matrix from validation with {len(classes)} classes.\")\n",
    "        except Exception:\n",
    "            print(\"[WARN] failed to parse validation file for confusion matrix, skipping confusion correction. Error:\")\n",
    "            traceback.print_exc()\n",
    "            confusion_possible = False\n",
    "\n",
    "    # Prepare Pred vectors (counts / weighted) using the same classes ordering\n",
    "    if confusion_possible:\n",
    "        idx_map = {c:i for i,c in enumerate(classes)}\n",
    "    else:\n",
    "        classes = sorted(set(preds_series.unique()))\n",
    "        idx_map = {c:i for i,c in enumerate(classes)}\n",
    "\n",
    "    Pred_counts = np.zeros(len(classes), dtype=float)\n",
    "    for row in counts_df.itertuples(index=False):\n",
    "        specie = normalize_label(row.species)\n",
    "        if specie in idx_map:\n",
    "            Pred_counts[idx_map[specie]] += float(row.count)\n",
    "        else:\n",
    "            # should not usually happen; try to match by fuzzy heuristics? here we skip silently\n",
    "            pass\n",
    "\n",
    "    Pred_conf = None\n",
    "    if weighted_df is not None:\n",
    "        Pred_conf = np.zeros(len(classes), dtype=float)\n",
    "        for row in weighted_df.itertuples(index=False):\n",
    "            specie = normalize_label(row.species)\n",
    "            if specie in idx_map:\n",
    "                Pred_conf[idx_map[specie]] += float(row.conf_sum)\n",
    "\n",
    "    # Save raw aggregates (always)\n",
    "    out_dir = pred_path.parent\n",
    "    out_counts = out_dir / \"abundance_from_predictions.csv\"\n",
    "    out_weighted = out_dir / \"abundance_from_predictions_weighted.csv\"\n",
    "    counts_df.to_csv(out_counts, index=False)\n",
    "    print(\"[SAVED] raw count-based abundance ->\", out_counts)\n",
    "    if weighted_df is not None:\n",
    "        weighted_df.to_csv(out_weighted, index=False)\n",
    "        print(\"[SAVED] raw confidence-weighted abundance ->\", out_weighted)\n",
    "\n",
    "    # If confusion matrix available, run deconvolution\n",
    "    if confusion_possible and A is not None:\n",
    "        print(\"[DECONV] Running NNLS deconvolution (projected gradient)...\")\n",
    "        est_true_counts = nnls_pgd_solve(A, Pred_counts, max_iter=3000)\n",
    "        est_true_counts = np.maximum(0.0, est_true_counts)\n",
    "        if est_true_counts.sum() > 0:\n",
    "            est_true_rel = est_true_counts / est_true_counts.sum()\n",
    "        else:\n",
    "            est_true_rel = est_true_counts.copy()\n",
    "\n",
    "        if Pred_conf is not None and Pred_conf.sum() > 0:\n",
    "            est_true_conf = nnls_pgd_solve(A, Pred_conf, max_iter=3000)\n",
    "            est_true_conf = np.maximum(0.0, est_true_conf)\n",
    "            if est_true_conf.sum() > 0:\n",
    "                est_true_conf_rel = est_true_conf / est_true_conf.sum()\n",
    "            else:\n",
    "                est_true_conf_rel = est_true_conf.copy()\n",
    "        else:\n",
    "            est_true_conf = None\n",
    "            est_true_conf_rel = None\n",
    "\n",
    "        df_deconv = pd.DataFrame({\n",
    "            \"species\": classes,\n",
    "            \"pred_count\": Pred_counts,\n",
    "            \"pred_count_rel\": Pred_counts / max(1.0, Pred_counts.sum()),\n",
    "            \"est_true_count\": est_true_counts,\n",
    "            \"est_true_rel\": est_true_rel\n",
    "        })\n",
    "        out_deconv = out_dir / \"abundance_from_predictions_deconvolved.csv\"\n",
    "        df_deconv.to_csv(out_deconv, index=False)\n",
    "        print(\"[SAVED] deconvolved counts ->\", out_deconv)\n",
    "\n",
    "        if est_true_conf is not None:\n",
    "            df_deconv_conf = pd.DataFrame({\n",
    "                \"species\": classes,\n",
    "                \"pred_conf_sum\": Pred_conf,\n",
    "                \"pred_conf_rel\": Pred_conf / max(1.0, Pred_conf.sum()),\n",
    "                \"est_true_conf\": est_true_conf,\n",
    "                \"est_true_conf_rel\": est_true_conf_rel\n",
    "            })\n",
    "            out_deconv_conf = out_dir / \"abundance_from_predictions_deconvolved_weighted.csv\"\n",
    "            df_deconv_conf.to_csv(out_deconv_conf, index=False)\n",
    "            print(\"[SAVED] deconvolved weighted ->\", out_deconv_conf)\n",
    "\n",
    "        # print small summary\n",
    "        print(\"\\nTop 15 predicted (raw counts):\")\n",
    "        print(counts_df.head(15).to_string(index=False))\n",
    "        print(\"\\nTop 15 estimated true (after deconvolution):\")\n",
    "        print(df_deconv.sort_values(\"est_true_rel\", ascending=False).head(15).to_string(index=False))\n",
    "    else:\n",
    "        print(\"[INFO] Confusion correction not performed (no usable validation file). Raw aggregates saved above.\")\n",
    "\n",
    "    print(\"\\n[DONE] Outputs saved to:\", out_dir)\n",
    "\n",
    "except SystemExit:\n",
    "    pass\n",
    "except Exception:\n",
    "    print(\"[ERROR] unexpected failure:\")\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c53f7785-ddf4-491b-9142-23df9748456e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FOUND] predictions: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\predictions_with_uncertainty.csv\n",
      "[FOUND] validation: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\val_predictions_calibrated.csv\n",
      "[SAVED] deconvolved at rank 'kingdom' -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\abundance_kingdom_deconvolved.csv\n",
      "[SAVED] deconvolved at rank 'phylum' -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\abundance_phylum_deconvolved.csv\n",
      "[SAVED] deconvolved at rank 'class' -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\abundance_class_deconvolved.csv\n",
      "[SAVED] deconvolved at rank 'order' -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\abundance_order_deconvolved.csv\n",
      "[SAVED] deconvolved at rank 'family' -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\abundance_family_deconvolved.csv\n",
      "[SAVED] deconvolved at rank 'genus' -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\abundance_genus_deconvolved.csv\n",
      "[SAVED] deconvolved at rank 'species' -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\abundance_species_deconvolved.csv\n",
      "[SAVED] raw count-based abundance -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\abundance_from_predictions.csv\n",
      "[SAVED] raw confidence-weighted abundance -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\abundance_from_predictions_weighted.csv\n",
      "[INFO] using species-level deconvolved estimates as base.\n",
      "[RECONCILE] applied rank 'genus' -> sum before=1.000000 after=1.000000\n",
      "[RECONCILE] applied rank 'family' -> sum before=1.000000 after=1.000000\n",
      "[RECONCILE] applied rank 'order' -> sum before=1.000000 after=1.000000\n",
      "[RECONCILE] applied rank 'class' -> sum before=1.000000 after=1.000000\n",
      "[RECONCILE] applied rank 'phylum' -> sum before=1.000000 after=1.000000\n",
      "[RECONCILE] applied rank 'kingdom' -> sum before=1.000000 after=1.000000\n",
      "[SAVED] reconciled species abundance -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\abundance_reconciled_species.csv\n",
      "\n",
      "Top 20 final reconciled species:\n",
      "                        species  est_rel   est_pct  pred_count  pred_conf_sum\n",
      "                     UNASSIGNED 0.564607 56.460682         144      89.403568\n",
      "                Maylandia zebra 0.224719 22.471913          80      59.536462\n",
      "               Chaetodon auriga 0.061798  6.179776          46      19.165359\n",
      "          Arvicanthis niloticus 0.019663  1.966292           9       7.736281\n",
      "                  Morchella sp. 0.014045  1.404494          10       3.351529\n",
      "            Amanita fuscozonata 0.011236  1.123595           4       1.549599\n",
      "     Chloroidium saccharophilum 0.008427  0.842696           3       2.551860\n",
      "       Pseudopestalotiopsis sp. 0.008427  0.842696           3       1.623526\n",
      "Deuterostichococcus epilithicus 0.008427  0.842696           3       1.915702\n",
      "    Cardimyxobolus iriomotensis 0.005618  0.561797           2       1.954282\n",
      "                Cortinarius sp. 0.005618  0.561797           2       1.042702\n",
      "                Inocybe miranda 0.005618  0.561797           2       1.173907\n",
      "      Omphalotus flagelliformis 0.005618  0.561797           2       0.645134\n",
      "          Morchella nipponensis 0.005618  0.561797           2       1.094246\n",
      "      Aspergillus costaricensis 0.005618  0.561797           7       4.226915\n",
      "                   Entoloma sp. 0.002809  0.280898           5       1.378032\n",
      "        Stichococcus bacillaris 0.002809  0.280898           1       0.635950\n",
      "        Metschnikowia reukaufii 0.002809  0.280898           1       0.907472\n",
      "           Pseudosperma curreyi 0.002809  0.280898           1       0.671144\n",
      "            Plasmopara viticola 0.002809  0.280898           1       0.952234\n",
      "[SAVED] per-sample species counts -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\abundance_per_sample_species_counts.csv\n",
      "\n",
      "DONE. All outputs are in: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\n"
     ]
    }
   ],
   "source": [
    "# Fixed hierarchical deconvolution + reconciliation cell (bugfix applied)\n",
    "# Paste & run in your notebook (searches for 'extracted' like previous cells).\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "\n",
    "def find_file(names):\n",
    "    cand_dirs = [\n",
    "        Path.cwd() / \"sih\" / \"ncbi_blast_db\" / \"extracted\",\n",
    "        Path.cwd() / \"ncbi_blast_db\" / \"extracted\",\n",
    "        Path.cwd() / \"extracted\",\n",
    "        Path.cwd(),\n",
    "        Path(r\"C:\\Users\\Srijit\\OneDrive\\Desktop\\sihtaxa\\sihabundance\\ncbi_blast_db\\extracted\"),\n",
    "        Path(r\"C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\"),\n",
    "        Path.home() / \"OneDrive\" / \"Desktop\" / \"sihtaxa\" / \"sihabundance\" / \"ncbi_blast_db\" / \"extracted\",\n",
    "    ]\n",
    "    # include any extracted under cwd\n",
    "    for p in Path.cwd().glob(\"**/extracted\"):\n",
    "        cand_dirs.append(p.resolve())\n",
    "    visited = set()\n",
    "    for d in cand_dirs:\n",
    "        if d in visited: continue\n",
    "        visited.add(d)\n",
    "        for n in names:\n",
    "            f = d / n\n",
    "            if f.exists():\n",
    "                return f\n",
    "    # fallback recursive search\n",
    "    for n in names:\n",
    "        for f in Path.cwd().rglob(n):\n",
    "            return f.resolve()\n",
    "    return None\n",
    "\n",
    "def normalize_label(x):\n",
    "    if pd.isna(x):\n",
    "        return \"UNASSIGNED\"\n",
    "    s = str(x).strip()\n",
    "    s = \" \".join(s.split())\n",
    "    return s if s != \"\" else \"UNASSIGNED\"\n",
    "\n",
    "def nnls_pgd(A, Pred, max_iter=3000, tol=1e-7):\n",
    "    M = A.T\n",
    "    try:\n",
    "        x0, *_ = np.linalg.lstsq(M, Pred, rcond=None)\n",
    "        x = np.maximum(0.0, x0)\n",
    "    except Exception:\n",
    "        x = np.maximum(0.0, np.ones(M.shape[1]) * (Pred.sum()/max(1,M.shape[1])))\n",
    "    try:\n",
    "        L = np.linalg.norm(M, ord=2)**2\n",
    "        if L <= 0: L = 1.0\n",
    "        lr = 1.0/(L + 1e-8)\n",
    "    except Exception:\n",
    "        lr = 1e-3\n",
    "    prev = None\n",
    "    for i in range(max_iter):\n",
    "        r = M.dot(x) - Pred\n",
    "        loss = 0.5 * (r @ r)\n",
    "        if prev is not None and abs(prev - loss) < tol:\n",
    "            break\n",
    "        prev = loss\n",
    "        grad = M.T.dot(r)\n",
    "        x -= lr * grad\n",
    "        x = np.maximum(0.0, x)\n",
    "    return x\n",
    "\n",
    "def build_confusion_from_val(df_val, true_col_candidates, pred_col_candidates, rank_name):\n",
    "    cols = {c.lower(): c for c in df_val.columns}\n",
    "    true_col = None; pred_col = None\n",
    "    for cand in true_col_candidates:\n",
    "        if cand in cols:\n",
    "            true_col = cols[cand]; break\n",
    "    for cand in pred_col_candidates:\n",
    "        if cand in cols:\n",
    "            pred_col = cols[cand]; break\n",
    "    # heuristics fallback: any columns that mention rank_name\n",
    "    if true_col is None or pred_col is None:\n",
    "        species_like = [c for c in df_val.columns if rank_name in c.lower()]\n",
    "        if len(species_like) >= 2:\n",
    "            if true_col is None: true_col = species_like[0]\n",
    "            if pred_col is None and len(species_like) > 1: pred_col = species_like[1]\n",
    "    if true_col is None or pred_col is None:\n",
    "        return None, None\n",
    "    true_norm = df_val[true_col].apply(normalize_label)\n",
    "    pred_norm = df_val[pred_col].apply(normalize_label)\n",
    "    classes = sorted(set(true_norm.unique()) | set(pred_norm.unique()))\n",
    "    idx = {c:i for i,c in enumerate(classes)}\n",
    "    n = len(classes)\n",
    "    A = np.zeros((n,n), dtype=float)\n",
    "    for t,p in zip(true_norm.values, pred_norm.values):\n",
    "        # ensure mapping present\n",
    "        if t not in idx or p not in idx:\n",
    "            continue\n",
    "        A[idx[t], idx[p]] += 1.0\n",
    "    row_sums = A.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums==0] = 1.0\n",
    "    A = A / row_sums\n",
    "    eps = 1e-8\n",
    "    A = (A + eps)\n",
    "    A = A / A.sum(axis=1, keepdims=True)\n",
    "    return A, classes\n",
    "\n",
    "# ---------- locate files ----------\n",
    "pred_file = find_file([\"predictions_with_uncertainty.csv\", \"predictions.csv\"])\n",
    "val_file  = find_file([\"val_predictions_calibrated.csv\", \"val_predictions.csv\", \"val_predictions_with_uncertainty.csv\"])\n",
    "if pred_file is None:\n",
    "    raise SystemExit(\"predictions CSV not found. Run inference or place predictions_with_uncertainty.csv in an extracted/ folder.\")\n",
    "print(\"[FOUND] predictions:\", pred_file)\n",
    "if val_file is not None:\n",
    "    print(\"[FOUND] validation:\", val_file)\n",
    "else:\n",
    "    print(\"[WARN] validation file not found. Rank confusion correction will be limited/skipped where absent.\")\n",
    "\n",
    "# ---------- load predictions and normalize rank columns ----------\n",
    "df_pred = pd.read_csv(pred_file)\n",
    "# standardize known rank-like columns by normalizing strings\n",
    "for c in df_pred.columns:\n",
    "    if any(r in c.lower() for r in (\"kingdom\",\"phylum\",\"class\",\"order\",\"family\",\"genus\",\"species\")):\n",
    "        df_pred[c] = df_pred[c].apply(normalize_label)\n",
    "\n",
    "# detect species prediction column robustly\n",
    "cols_lower = {c.lower(): c for c in df_pred.columns}\n",
    "species_candidates = [\"species_pred_label\",\"species_label\",\"species_pred\",\"species\"]\n",
    "species_col = None\n",
    "for cand in species_candidates:\n",
    "    if cand in cols_lower:\n",
    "        species_col = cols_lower[cand]; break\n",
    "if species_col is None:\n",
    "    for c in df_pred.columns:\n",
    "        if \"species\" in c.lower():\n",
    "            species_col = c; break\n",
    "if species_col is None:\n",
    "    raise SystemExit(\"Could not detect species prediction column in predictions CSV (looked for species_pred_label/species_pred/species_label/etc).\")\n",
    "\n",
    "# create normalized species column for aggregation\n",
    "df_pred[\"_species_norm\"] = df_pred[species_col].astype(str).apply(normalize_label)\n",
    "\n",
    "# aggregation function (returns consistent columns: taxon,count,rel)\n",
    "def aggregate_pred_counts(df, col):\n",
    "    if col not in df.columns:\n",
    "        return pd.DataFrame({\"taxon\":[], \"count\":[], \"rel\":[]})\n",
    "    s = df[col].astype(str).apply(normalize_label)\n",
    "    dfc = s.value_counts().rename_axis(\"taxon\").reset_index(name=\"count\")\n",
    "    dfc[\"rel\"] = dfc[\"count\"] / dfc[\"count\"].sum() if dfc[\"count\"].sum() > 0 else 0.0\n",
    "    return dfc\n",
    "\n",
    "# build raw_by_rank: map rank -> (predicted_column_name, aggregated_df with columns 'taxon','count','rel')\n",
    "ranks = [\"kingdom\",\"phylum\",\"class\",\"order\",\"family\",\"genus\",\"species\"]\n",
    "raw_by_rank = {}\n",
    "for r in ranks:\n",
    "    col_found = None\n",
    "    # prefer explicit columns mentioning the rank\n",
    "    for c in df_pred.columns:\n",
    "        if r in c.lower():\n",
    "            if r == \"species\":\n",
    "                col_found = species_col\n",
    "            else:\n",
    "                col_found = c\n",
    "            break\n",
    "    if col_found is not None:\n",
    "        agg_df = aggregate_pred_counts(df_pred, col_found)\n",
    "        raw_by_rank[r] = (col_found, agg_df)\n",
    "\n",
    "# also save top-level species raw counts (for output)\n",
    "counts_df = aggregate_pred_counts(df_pred, \"_species_norm\")\n",
    "\n",
    "# optional weighted sums if confidence present\n",
    "conf_col = None\n",
    "for cand in (\"species_pred_conf\",\"species_conf\",\"species_prob\",\"species_probability\"):\n",
    "    if cand in cols_lower:\n",
    "        conf_col = cols_lower[cand]; break\n",
    "weighted_df = None\n",
    "if conf_col and conf_col in df_pred.columns:\n",
    "    df_pred[\"_conf_num\"] = pd.to_numeric(df_pred[conf_col], errors=\"coerce\").fillna(0.0)\n",
    "    w = df_pred.groupby(\"_species_norm\")[\"_conf_num\"].sum().reset_index().rename(columns={\"_conf_num\":\"conf_sum\", \"_species_norm\":\"taxon\"})\n",
    "    if w[\"conf_sum\"].sum() > 0:\n",
    "        w[\"rel\"] = w[\"conf_sum\"] / w[\"conf_sum\"].sum()\n",
    "    weighted_df = w\n",
    "\n",
    "# ---------- build deconvolution per rank using validation ----------\n",
    "true_candidates = {\n",
    "    \"kingdom\": [\"kingdom_true\",\"true_kingdom\",\"kingdom_label\",\"kingdom\"],\n",
    "    \"phylum\":  [\"phylum_true\",\"true_phylum\",\"phylum_label\",\"phylum\"],\n",
    "    \"class\":   [\"class_true\",\"true_class\",\"class_label\",\"class\"],\n",
    "    \"order\":   [\"order_true\",\"true_order\",\"order_label\",\"order\"],\n",
    "    \"family\":  [\"family_true\",\"true_family\",\"family_label\",\"family\"],\n",
    "    \"genus\":   [\"genus_true\",\"true_genus\",\"genus_label\",\"genus\"],\n",
    "    \"species\": [\"species_true\",\"true_species\",\"species_label\",\"label_species\",\"species\"]\n",
    "}\n",
    "pred_candidates = {\n",
    "    \"kingdom\": [\"kingdom_pred_label\",\"kingdom_pred\",\"kingdom_label\",\"kingdom\"],\n",
    "    \"phylum\":  [\"phylum_pred_label\",\"phylum_pred\",\"phylum_label\",\"phylum\"],\n",
    "    \"class\":   [\"class_pred_label\",\"class_pred\",\"class_label\",\"class\"],\n",
    "    \"order\":   [\"order_pred_label\",\"order_pred\",\"order_label\",\"order\"],\n",
    "    \"family\":  [\"family_pred_label\",\"family_pred\",\"family_label\",\"family\"],\n",
    "    \"genus\":   [\"genus_pred_label\",\"genus_pred\",\"genus_label\",\"genus\"],\n",
    "    \"species\": [\"species_pred_label\",\"species_pred\",\"species_label\",\"species\"]\n",
    "}\n",
    "\n",
    "deconv_by_rank = {}\n",
    "if val_file is not None:\n",
    "    df_val = pd.read_csv(val_file)\n",
    "    # normalize val rank columns\n",
    "    for c in df_val.columns:\n",
    "        if any(r in c.lower() for r in ranks):\n",
    "            df_val[c] = df_val[c].apply(normalize_label)\n",
    "\n",
    "    for r, (pred_col, agg_df) in list(raw_by_rank.items()):\n",
    "        # build confusion from validation for this rank\n",
    "        A, classes = build_confusion_from_val(df_val, true_candidates[r], pred_candidates[r], r)\n",
    "        if A is None:\n",
    "            print(f\"[INFO] validation lacks usable true/pred pair for rank '{r}' -> skipping rank deconv.\")\n",
    "            continue\n",
    "        # prepare Pred vector following classes ordering\n",
    "        Pred = np.zeros(len(classes), dtype=float)\n",
    "        # agg_df has columns 'taxon','count','rel'\n",
    "        for tup in agg_df.itertuples(index=False, name=None):\n",
    "            tax = normalize_label(tup[0])   # first column is taxon\n",
    "            cnt = float(tup[1]) if len(tup) > 1 else 0.0\n",
    "            if tax in classes:\n",
    "                Pred[classes.index(tax)] += cnt\n",
    "        # run deconv (NNLS)\n",
    "        est_true = nnls_pgd(A, Pred)\n",
    "        est_true = np.maximum(0.0, est_true)\n",
    "        est_rel = est_true / est_true.sum() if est_true.sum() > 0 else est_true\n",
    "        df_out = pd.DataFrame({\n",
    "            \"taxon\": classes,\n",
    "            \"pred_count\": Pred,\n",
    "            \"pred_rel\": Pred / max(1.0, Pred.sum()),\n",
    "            \"est_true_count\": est_true,\n",
    "            \"est_true_rel\": est_rel\n",
    "        })\n",
    "        deconv_by_rank[r] = (pred_col, df_out)\n",
    "        outp = pred_file.parent / f\"abundance_{r}_deconvolved.csv\"\n",
    "        df_out.to_csv(outp, index=False)\n",
    "        print(f\"[SAVED] deconvolved at rank '{r}' -> {outp}\")\n",
    "\n",
    "# Save raw aggregates (species-level counts and optionally weighted)\n",
    "out_dir = pred_file.parent\n",
    "out_counts = out_dir / \"abundance_from_predictions.csv\"\n",
    "counts_df.rename(columns={\"taxon\":\"species\",\"count\":\"count\",\"rel\":\"rel_abund_count\"}).to_csv(out_counts, index=False)\n",
    "print(\"[SAVED] raw count-based abundance ->\", out_counts)\n",
    "if weighted_df is not None:\n",
    "    out_w = out_dir / \"abundance_from_predictions_weighted.csv\"\n",
    "    weighted_df.rename(columns={\"taxon\":\"species\",\"conf_sum\":\"conf_sum\",\"rel\":\"rel_abund_weighted\"}).to_csv(out_w, index=False)\n",
    "    print(\"[SAVED] raw confidence-weighted abundance ->\", out_w)\n",
    "\n",
    "# ---------- Reconciliation (species-level final estimates) ----------\n",
    "# base species estimates: prefer species deconv, else weighted, else raw counts\n",
    "if \"species\" in deconv_by_rank:\n",
    "    base_df = deconv_by_rank[\"species\"][1]\n",
    "    species_classes = base_df[\"taxon\"].tolist()\n",
    "    base_rel = base_df[\"est_true_rel\"].astype(float).values\n",
    "    print(\"[INFO] using species-level deconvolved estimates as base.\")\n",
    "elif weighted_df is not None:\n",
    "    species_classes = weighted_df[\"taxon\"].tolist()\n",
    "    svals = weighted_df[\"conf_sum\"].astype(float).values\n",
    "    base_rel = svals / svals.sum() if svals.sum() > 0 else np.ones(len(svals)) / max(1,len(svals))\n",
    "    print(\"[INFO] species-level deconv missing: using confidence-weighted as base.\")\n",
    "else:\n",
    "    species_classes = counts_df[\"taxon\"].tolist()\n",
    "    cvals = counts_df[\"count\"].astype(float).values\n",
    "    base_rel = cvals / cvals.sum() if cvals.sum() > 0 else np.ones(len(cvals)) / max(1,len(cvals))\n",
    "    print(\"[INFO] species-level deconv & weighted missing: using raw counts as base.\")\n",
    "\n",
    "species_df = pd.DataFrame({\"species\": species_classes, \"base_rel\": base_rel})\n",
    "species_df[\"species\"] = species_df[\"species\"].apply(normalize_label)\n",
    "\n",
    "# try to map species -> higher ranks using predictions (majority)\n",
    "rank_cols = {}\n",
    "for r in (\"genus\",\"family\",\"order\",\"class\",\"phylum\",\"kingdom\"):\n",
    "    for c in df_pred.columns:\n",
    "        if r in c.lower():\n",
    "            rank_cols[r] = c\n",
    "            break\n",
    "\n",
    "# build mapping by the most frequent value per species from df_pred\n",
    "mapping = {}\n",
    "if len(rank_cols)>0:\n",
    "    temp = df_pred[[species_col] + list(rank_cols.values())].copy()\n",
    "    temp[\"_sp\"] = temp[species_col].astype(str).apply(normalize_label)\n",
    "    for sp, g in temp.groupby(\"_sp\"):\n",
    "        mapping[sp] = {}\n",
    "        for rc in rank_cols.values():\n",
    "            vals = g[rc].astype(str).apply(normalize_label).value_counts()\n",
    "            mapping[sp][rc] = vals.index[0] if len(vals)>0 else \"UNASSIGNED\"\n",
    "\n",
    "# attach rank columns to species_df\n",
    "for rc_name, rc_col in rank_cols.items():\n",
    "    species_df[rc_name] = species_df[\"species\"].apply(lambda s: mapping.get(s, {}).get(rc_col, \"UNASSIGNED\"))\n",
    "\n",
    "# produce final species distribution and then apply rank-based redistribution if deconvolved rank exists\n",
    "final_rel = pd.Series(species_df[\"base_rel\"].values, index=species_df[\"species\"]).astype(float)\n",
    "\n",
    "def distribute_rank_to_species(rank, df_rank):\n",
    "    # df_rank: taxon, est_true_rel\n",
    "    rank_to_rel = dict(zip(df_rank[\"taxon\"].apply(normalize_label), df_rank[\"est_true_rel\"].astype(float)))\n",
    "    # build groups\n",
    "    groups = {}\n",
    "    for sp in final_rel.index:\n",
    "        if rank in rank_cols:\n",
    "            mapped = species_df.loc[species_df[\"species\"]==sp, rank].values\n",
    "            rname = normalize_label(mapped[0]) if len(mapped)>0 else \"UNASSIGNED\"\n",
    "        else:\n",
    "            rname = \"UNASSIGNED\"\n",
    "        groups.setdefault(rname, []).append(sp)\n",
    "    new_rel = final_rel.copy()\n",
    "    for rname, spp in groups.items():\n",
    "        target = rank_to_rel.get(rname, None)\n",
    "        if target is None:\n",
    "            continue\n",
    "        cur_sum = float(final_rel.loc[spp].sum())\n",
    "        if cur_sum <= 0:\n",
    "            if len(spp) > 0:\n",
    "                each = target / len(spp)\n",
    "                for sp in spp:\n",
    "                    new_rel.loc[sp] = each\n",
    "        else:\n",
    "            for sp in spp:\n",
    "                new_rel.loc[sp] = final_rel.loc[sp] * (target / cur_sum)\n",
    "    if new_rel.sum() > 0:\n",
    "        new_rel = new_rel / new_rel.sum()\n",
    "    return new_rel\n",
    "\n",
    "rank_apply_order = [\"genus\",\"family\",\"order\",\"class\",\"phylum\",\"kingdom\"]\n",
    "applied = False\n",
    "for r in rank_apply_order:\n",
    "    if r in deconv_by_rank:\n",
    "        _, df_r = deconv_by_rank[r]\n",
    "        before = final_rel.sum()\n",
    "        final_rel = distribute_rank_to_species(r, df_r)\n",
    "        applied = True\n",
    "        print(f\"[RECONCILE] applied rank '{r}' -> sum before={before:.6f} after={final_rel.sum():.6f}\")\n",
    "\n",
    "if not applied:\n",
    "    print(\"[INFO] no higher-rank deconvolved corrections applied; final = base species estimates.\")\n",
    "\n",
    "# final output DataFrame\n",
    "out_df = final_rel.reset_index().rename(columns={\"index\":\"species\", 0:\"est_rel\"})\n",
    "out_df[\"est_pct\"] = out_df[\"est_rel\"] * 100.0\n",
    "# attach pred_count and pred_conf_sum if available (maps normalized names)\n",
    "counts_map = dict(zip(counts_df[\"taxon\"].apply(normalize_label), counts_df[\"count\"]))\n",
    "out_df[\"pred_count\"] = out_df[\"species\"].apply(lambda s: counts_map.get(normalize_label(s), 0))\n",
    "if weighted_df is not None:\n",
    "    weighted_map = dict(zip(weighted_df[\"taxon\"].apply(normalize_label), weighted_df[\"conf_sum\"]))\n",
    "    out_df[\"pred_conf_sum\"] = out_df[\"species\"].apply(lambda s: weighted_map.get(normalize_label(s), 0.0))\n",
    "else:\n",
    "    out_df[\"pred_conf_sum\"] = np.nan\n",
    "\n",
    "out_path = pred_file.parent / \"abundance_reconciled_species.csv\"\n",
    "out_df.to_csv(out_path, index=False)\n",
    "print(\"[SAVED] reconciled species abundance ->\", out_path)\n",
    "print(\"\\nTop 20 final reconciled species:\")\n",
    "print(out_df.sort_values(\"est_rel\", ascending=False).head(20).to_string(index=False))\n",
    "\n",
    "# per-sample output if sample id found\n",
    "sample_col = None\n",
    "for sc in [\"sample_id\",\"sample\",\"samp\",\"run\",\"readset\",\"accession\",\"id\",\"seqid\"]:\n",
    "    if sc in cols_lower:\n",
    "        sample_col = cols_lower[sc]; break\n",
    "if sample_col is None:\n",
    "    for c in df_pred.columns:\n",
    "        if c.lower().endswith(\"_id\") or c.lower().endswith(\"id\"):\n",
    "            sample_col = c; break\n",
    "\n",
    "if sample_col and sample_col in df_pred.columns:\n",
    "    df_pred[\"_sample_norm\"] = df_pred[sample_col].astype(str)\n",
    "    ps = df_pred.groupby([\"_sample_norm\",\"_species_norm\"]).size().reset_index(name=\"count\")\n",
    "    sample_table = ps.pivot(index=\"_sample_norm\", columns=\"_species_norm\", values=\"count\").fillna(0).astype(int)\n",
    "    sample_out = pred_file.parent / \"abundance_per_sample_species_counts.csv\"\n",
    "    sample_table.to_csv(sample_out)\n",
    "    print(\"[SAVED] per-sample species counts ->\", sample_out)\n",
    "else:\n",
    "    print(\"[INFO] no sample identifier detected; skipped per-sample table.\")\n",
    "\n",
    "print(\"\\nDONE. All outputs are in:\", pred_file.parent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b4e823a4-3031-4d36-a1d4-cb055d38dbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FOUND] using: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\abundance_reconciled_species.csv\n",
      "[SAVED] publication-ready CSV -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\abundance_publication_ready.csv\n",
      "[SAVED] top-20 barplot -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\abundance_top20.png\n",
      "[SAVED] per-sample relative abundance -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\abundance_per_sample_species_relative.csv\n",
      "[SAVED] packaged outputs -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\abundance_results_package.zip\n",
      "\n",
      "SUMMARY of outputs saved in: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\n",
      " - publication CSV: abundance_publication_ready.csv\n",
      " - top-20 plot    : abundance_top20.png\n",
      " - per-sample relative table: abundance_per_sample_species_relative.csv\n",
      " - package ZIP    : abundance_results_package.zip\n",
      "\n",
      "DONE. If you want I can now:\n",
      "  • produce a PDF/PNG figure layout and a short report (1 more cell),\n",
      "  • create a small helper cell that loads a new FASTA, runs the classifier and returns abundances (1 more cell),\n",
      "  • or just stop here (no more cells needed).\n"
     ]
    }
   ],
   "source": [
    "# Cell: finalize abundance outputs (publication CSV, top-20 barplot, per-sample normalized table, zip)\n",
    "# Paste & run in the same Jupyter kernel you used for previous cells.\n",
    "import pandas as pd, numpy as np, zipfile, textwrap, sys\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def find_extracted_candidates():\n",
    "    cand = []\n",
    "    cand.append(Path.cwd() / \"sih\" / \"ncbi_blast_db\" / \"extracted\")\n",
    "    cand.append(Path.cwd() / \"ncbi_blast_db\" / \"extracted\")\n",
    "    cand.append(Path.cwd() / \"extracted\")\n",
    "    cand.append(Path.cwd())\n",
    "    cand.append(Path(r\"C:\\Users\\Srijit\\OneDrive\\Desktop\\sihtaxa\\sihabundance\\ncbi_blast_db\\extracted\"))\n",
    "    cand.append(Path(r\"C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\"))\n",
    "    cand.append(Path.home() / \"OneDrive\" / \"Desktop\" / \"sihtaxa\" / \"sihabundance\" / \"ncbi_blast_db\" / \"extracted\")\n",
    "    # also add any extracted under cwd\n",
    "    for p in Path.cwd().glob(\"**/extracted\"):\n",
    "        cand.append(p.resolve())\n",
    "    # keep unique, existing\n",
    "    seen = []\n",
    "    out = []\n",
    "    for p in cand:\n",
    "        if p in seen: continue\n",
    "        seen.append(p)\n",
    "        if p.exists() and p.is_dir():\n",
    "            out.append(p)\n",
    "    return out\n",
    "\n",
    "def pick_file(base_names):\n",
    "    \"\"\"Return first existing file path from base_names searching candidate extracted folders.\"\"\"\n",
    "    for d in find_extracted_candidates():\n",
    "        for name in base_names:\n",
    "            p = d / name\n",
    "            if p.exists():\n",
    "                return p\n",
    "    # last resort: search recursively from cwd\n",
    "    for name in base_names:\n",
    "        for p in Path.cwd().rglob(name):\n",
    "            return p.resolve()\n",
    "    return None\n",
    "\n",
    "# locate final reconciled abundance (prefer reconciled, else deconvolved, else raw)\n",
    "final_path = pick_file([\"abundance_reconciled_species.csv\", \"abundance_from_predictions_deconvolved.csv\",\n",
    "                        \"abundance_from_predictions.csv\", \"abundance_from_predictions_weighted.csv\"])\n",
    "if final_path is None:\n",
    "    print(\"ERROR: Could not find any abundance CSV (search looked for reconciled/deconvolved/raw).\")\n",
    "    print(\"Place 'abundance_reconciled_species.csv' in an 'extracted/' folder or run previous cells to create it.\")\n",
    "    raise SystemExit(0)\n",
    "\n",
    "out_dir = final_path.parent\n",
    "print(\"[FOUND] using:\", final_path)\n",
    "\n",
    "# load final table (handle different column names gracefully)\n",
    "df = pd.read_csv(final_path)\n",
    "# normalize column names to expected set (species, est_rel, est_pct, pred_count, pred_conf_sum)\n",
    "cols_lower = {c.lower(): c for c in df.columns}\n",
    "# Heuristics for species column\n",
    "if \"species\" in cols_lower:\n",
    "    sp_col = cols_lower[\"species\"]\n",
    "else:\n",
    "    # try first column if it's text-like\n",
    "    sp_col = df.columns[0]\n",
    "# find est_rel or est_true_rel or percent\n",
    "if \"est_rel\" in cols_lower:\n",
    "    rel_col = cols_lower[\"est_rel\"]\n",
    "elif \"est_true_rel\" in cols_lower:\n",
    "    rel_col = cols_lower[\"est_true_rel\"]\n",
    "elif \"pred_count_rel\" in cols_lower:\n",
    "    rel_col = cols_lower[\"pred_count_rel\"]\n",
    "else:\n",
    "    # fallback: try any numeric column after species\n",
    "    other_numeric = [c for c in df.columns if c!=sp_col and pd.api.types.is_numeric_dtype(df[c])]\n",
    "    rel_col = other_numeric[0] if other_numeric else None\n",
    "\n",
    "# pred_count\n",
    "pred_count_col = cols_lower.get(\"pred_count\") or cols_lower.get(\"count\") if \"count\" in cols_lower else None\n",
    "pred_conf_col  = cols_lower.get(\"pred_conf_sum\") or cols_lower.get(\"conf_sum\") or cols_lower.get(\"pred_conf\")\n",
    "\n",
    "# build publication-ready dataframe\n",
    "pub = pd.DataFrame()\n",
    "pub[\"species\"] = df[sp_col].astype(str).apply(lambda s: \" \".join(str(s).split()))\n",
    "if rel_col is not None:\n",
    "    pub[\"est_rel\"] = pd.to_numeric(df[rel_col], errors=\"coerce\").fillna(0.0)\n",
    "else:\n",
    "    pub[\"est_rel\"] = 0.0\n",
    "# ensure fraction between 0..1; if >1 it may already be percent\n",
    "if pub[\"est_rel\"].max() > 1.001:\n",
    "    # assume percents -> convert\n",
    "    pub[\"est_rel\"] = pub[\"est_rel\"] / 100.0\n",
    "pub[\"est_pct\"] = pub[\"est_rel\"] * 100.0\n",
    "if pred_count_col:\n",
    "    pub[\"pred_count\"] = pd.to_numeric(df[pred_count_col], errors=\"coerce\").fillna(0).astype(int)\n",
    "else:\n",
    "    pub[\"pred_count\"] = pub[\"species\"].map(df.set_index(sp_col).get(pred_count_col, pd.Series())).fillna(0).astype(int)\n",
    "if pred_conf_col and pred_conf_col in df.columns:\n",
    "    pub[\"pred_conf_sum\"] = pd.to_numeric(df[pred_conf_col], errors=\"coerce\").fillna(0.0)\n",
    "else:\n",
    "    pub[\"pred_conf_sum\"] = np.nan\n",
    "\n",
    "# preserve sorting by estimated abundance\n",
    "pub = pub.sort_values(\"est_rel\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Save publication-ready CSV\n",
    "pub_out = out_dir / \"abundance_publication_ready.csv\"\n",
    "pub.to_csv(pub_out, index=False)\n",
    "print(\"[SAVED] publication-ready CSV ->\", pub_out)\n",
    "\n",
    "# Top-N barplot (top 20)\n",
    "top_n = 20\n",
    "top = pub.head(top_n)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title(f\"Top {min(top_n,len(top))} species by estimated relative abundance\")\n",
    "plt.barh(range(len(top)), top[\"est_rel\"].values[::-1])   # horizontal bar; default colors\n",
    "plt.yticks(range(len(top)), top[\"species\"].values[::-1])\n",
    "plt.xlabel(\"Relative abundance (fraction)\")\n",
    "plt.tight_layout()\n",
    "img_out = out_dir / \"abundance_top20.png\"\n",
    "plt.savefig(img_out, dpi=200)\n",
    "plt.close()\n",
    "print(\"[SAVED] top-20 barplot ->\", img_out)\n",
    "\n",
    "# Per-sample normalized abundance (if per-sample counts present)\n",
    "per_sample_counts = pick_file([\"abundance_per_sample_species_counts.csv\", \"abundance_per_sample_species_counts.csv\"])\n",
    "if per_sample_counts is not None and per_sample_counts.exists():\n",
    "    try:\n",
    "        samp_df = pd.read_csv(per_sample_counts, index_col=0)\n",
    "        # Row-normalize to relative abundances per sample\n",
    "        rel = samp_df.div(samp_df.sum(axis=1).replace(0,1), axis=0)\n",
    "        rel_out = out_dir / \"abundance_per_sample_species_relative.csv\"\n",
    "        rel.to_csv(rel_out, index=True)\n",
    "        print(\"[SAVED] per-sample relative abundance ->\", rel_out)\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] failed to create per-sample relative abundance:\", e)\n",
    "else:\n",
    "    print(\"[INFO] no per-sample count file found (skipping per-sample normalization).\")\n",
    "\n",
    "# Package main files into a zip\n",
    "zip_path = out_dir / \"abundance_results_package.zip\"\n",
    "to_package = [\n",
    "    pub_out,\n",
    "    img_out,\n",
    "    out_dir / \"abundance_from_predictions.csv\",\n",
    "    out_dir / \"abundance_from_predictions_weighted.csv\",\n",
    "    out_dir / \"abundance_from_predictions_deconvolved.csv\",\n",
    "    out_dir / \"abundance_from_predictions_deconvolved_weighted.csv\",\n",
    "    out_dir / \"abundance_reconciled_species.csv\",\n",
    "]\n",
    "# keep only those that exist\n",
    "to_package = [p for p in to_package if p is not None and p.exists()]\n",
    "with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
    "    for p in to_package:\n",
    "        z.write(p, arcname=p.name)\n",
    "print(\"[SAVED] packaged outputs ->\", zip_path)\n",
    "\n",
    "# final summary\n",
    "print(\"\\nSUMMARY of outputs saved in:\", out_dir)\n",
    "print(\" - publication CSV:\", pub_out.name)\n",
    "print(\" - top-20 plot    :\", img_out.name)\n",
    "if per_sample_counts is not None and per_sample_counts.exists():\n",
    "    print(\" - per-sample relative table:\", rel_out.name)\n",
    "print(\" - package ZIP    :\", zip_path.name)\n",
    "print(\"\\nDONE. If you want I can now:\")\n",
    "print(\"  • produce a PDF/PNG figure layout and a short report (1 more cell),\")\n",
    "print(\"  • create a small helper cell that loads a new FASTA, runs the classifier and returns abundances (1 more cell),\")\n",
    "print(\"  • or just stop here (no more cells needed).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6c096204-5c6d-4cc4-9d59-f516d64d5292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\predictions_with_uncertainty.csv\n",
      "validation : C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\val_predictions_calibrated.csv\n",
      "reconciled : C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\abundance_reconciled_species.csv\n",
      "------------------------------------------------------------\n",
      "Loaded predictions rows: 380\n",
      "Detected species prediction column: species_pred_label\n",
      "Detected confidence column: species_pred_conf\n",
      "\n",
      "Validation: true = species_true_idx , pred = species_pred_label\n",
      "\n",
      "Species-level metrics on validation:\n",
      "  Accuracy = 0.7737    Macro-F1 = 0.3942    Micro-F1 = 0.7737\n",
      "\n",
      "Top 15 classes by true-support:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UNASSIGNED</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maylandia zebra</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chaetodon auriga</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arvicanthis niloticus</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Morchella sp.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Aonchotheca annulosa</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Pseudopestalotiopsis sp.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Deuterostichococcus epilithicus</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Aspergillus costaricensis</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Chloroidium saccharophilum</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Cortinarius sp.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Amanita sp.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Baruscapillaria inflexa</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Inocybe miranda</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Morchella nipponensis</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            species  count\n",
       "0                        UNASSIGNED    201\n",
       "1                   Maylandia zebra     80\n",
       "2                  Chaetodon auriga     22\n",
       "3             Arvicanthis niloticus      6\n",
       "4                     Morchella sp.      5\n",
       "5              Aonchotheca annulosa      4\n",
       "6          Pseudopestalotiopsis sp.      3\n",
       "7   Deuterostichococcus epilithicus      3\n",
       "8         Aspergillus costaricensis      3\n",
       "9        Chloroidium saccharophilum      3\n",
       "10                  Cortinarius sp.      2\n",
       "11                      Amanita sp.      2\n",
       "12          Baruscapillaria inflexa      2\n",
       "13                  Inocybe miranda      2\n",
       "14            Morchella nipponensis      2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification report (top classes shown):\n",
      "\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                     UNASSIGNED       0.99      0.71      0.83       201\n",
      "                Maylandia zebra       1.00      1.00      1.00        80\n",
      "               Chaetodon auriga       0.48      1.00      0.65        22\n",
      "          Arvicanthis niloticus       0.75      1.00      0.86         6\n",
      "                  Morchella sp.       0.40      0.80      0.53         5\n",
      "           Aonchotheca annulosa       0.00      0.00      0.00         4\n",
      "       Pseudopestalotiopsis sp.       1.00      1.00      1.00         3\n",
      "Deuterostichococcus epilithicus       1.00      1.00      1.00         3\n",
      "      Aspergillus costaricensis       0.43      1.00      0.60         3\n",
      "     Chloroidium saccharophilum       1.00      1.00      1.00         3\n",
      "                Cortinarius sp.       1.00      1.00      1.00         2\n",
      "                    Amanita sp.       0.00      0.00      0.00         2\n",
      "        Baruscapillaria inflexa       0.00      0.00      0.00         2\n",
      "                Inocybe miranda       1.00      1.00      1.00         2\n",
      "          Morchella nipponensis       1.00      1.00      1.00         2\n",
      "            Amanita fuscozonata       0.67      1.00      0.80         2\n",
      "    Cardimyxobolus iriomotensis       1.00      1.00      1.00         2\n",
      "                    Inocybe sp.       0.00      0.00      0.00         2\n",
      "               Pluteus seticeps       0.00      0.00      0.00         1\n",
      "             Lactifluus griseus       1.00      1.00      1.00         1\n",
      "        Metschnikowia reukaufii       1.00      1.00      1.00         1\n",
      "              Clavulinopsis sp.       0.00      0.00      0.00         1\n",
      "            Laeticutis cristata       0.00      0.00      0.00         1\n",
      "         Hysterothylacium fabri       0.17      1.00      0.29         1\n",
      "                   Entoloma sp.       0.20      1.00      0.33         1\n",
      "               Cladosporium sp.       1.00      1.00      1.00         1\n",
      "          Nannizziopsis guarroi       0.00      0.00      0.00         1\n",
      "        Stichococcus bacillaris       1.00      1.00      1.00         1\n",
      "               Russula subtilis       0.00      0.00      0.00         1\n",
      "                   Clavaria sp.       0.00      0.00      0.00         1\n",
      "            Phyllostachys nigra       0.00      0.00      0.00         1\n",
      "            Morchella clivicola       1.00      1.00      1.00         1\n",
      "             Beauveria bassiana       1.00      1.00      1.00         1\n",
      "                Inocybe favoris       0.00      0.00      0.00         1\n",
      "        Volvariella pilosipilea       0.00      0.00      0.00         1\n",
      "           Phyllostachys edulis       0.00      0.00      0.00         1\n",
      "     Pseudosperma aureocitrinum       1.00      1.00      1.00         1\n",
      "      Pseudoboletus parasiticus       0.00      0.00      0.00         1\n",
      "              Galerina hypnorum       0.00      0.00      0.00         1\n",
      "    Clavulinopsis manus-buddhae       0.00      0.00      0.00         1\n",
      "            Tetranychus urticae       1.00      1.00      1.00         1\n",
      "            Plasmopara viticola       1.00      1.00      1.00         1\n",
      "       Strobilurus conigenoides       1.00      1.00      1.00         1\n",
      "          Diplosphaera chodatii       1.00      1.00      1.00         1\n",
      "           Pseudosperma curreyi       1.00      1.00      1.00         1\n",
      "      Omphalotus flagelliformis       1.00      1.00      1.00         1\n",
      "           Klebsormidium nitens       1.00      1.00      1.00         1\n",
      "             Trichoderma viride       0.33      1.00      0.50         1\n",
      "         Pseudosperma squamatum       0.00      0.00      0.00         1\n",
      "             Laccaria striatula       1.00      1.00      1.00         1\n",
      "            Morchella pulchella       0.00      0.00      0.00         1\n",
      "                 Armillaria sp.       0.00      0.00      0.00         1\n",
      "\n",
      "                      micro avg       0.85      0.77      0.81       380\n",
      "                      macro avg       0.53      0.61      0.55       380\n",
      "                   weighted avg       0.87      0.77      0.80       380\n",
      "\n",
      "\n",
      "Top 12 confusion pairs (true -> predicted):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true</th>\n",
       "      <th>pred</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UNASSIGNED</td>\n",
       "      <td>Chaetodon auriga</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UNASSIGNED</td>\n",
       "      <td>Morchella sp.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>UNASSIGNED</td>\n",
       "      <td>Hysterothylacium fabri</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>UNASSIGNED</td>\n",
       "      <td>Aspergillus costaricensis</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>UNASSIGNED</td>\n",
       "      <td>Trichoderma viride</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UNASSIGNED</td>\n",
       "      <td>Arvicanthis niloticus</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>UNASSIGNED</td>\n",
       "      <td>Entoloma sp.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1532</th>\n",
       "      <td>Clavaria sp.</td>\n",
       "      <td>Entoloma sp.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>Inocybe sp.</td>\n",
       "      <td>Entoloma sp.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2604</th>\n",
       "      <td>Morchella pulchella</td>\n",
       "      <td>Morchella sp.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>Amanita sp.</td>\n",
       "      <td>Amanita fuscozonata</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>Pseudosperma squamatum</td>\n",
       "      <td>UNASSIGNED</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        true                       pred  count\n",
       "2                 UNASSIGNED           Chaetodon auriga     24\n",
       "4                 UNASSIGNED              Morchella sp.      5\n",
       "23                UNASSIGNED     Hysterothylacium fabri      5\n",
       "8                 UNASSIGNED  Aspergillus costaricensis      3\n",
       "47                UNASSIGNED         Trichoderma viride      2\n",
       "3                 UNASSIGNED      Arvicanthis niloticus      2\n",
       "24                UNASSIGNED               Entoloma sp.      2\n",
       "1532            Clavaria sp.               Entoloma sp.      1\n",
       "908              Inocybe sp.               Entoloma sp.      1\n",
       "2604     Morchella pulchella              Morchella sp.      1\n",
       "587              Amanita sp.        Amanita fuscozonata      1\n",
       "2496  Pseudosperma squamatum                 UNASSIGNED      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calibration check: could not match confidences to val rows or confidences not present.\n",
      "\n",
      "######## Assignment rates per rank ########\n",
      "\n",
      "Rank 'kingdom'  (column='kingdom_pred_label')  -> UNASSIGNED 216/380 = 56.84%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kingdom_pred_label</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UNASSIGNED</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eukaryota</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  kingdom_pred_label  count\n",
       "0         UNASSIGNED    216\n",
       "1          Eukaryota    164"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rank 'phylum'  (column='phylum_pred_label')  -> UNASSIGNED 163/380 = 42.89%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phylum_pred_label</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UNASSIGNED</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Metazoa</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fungi</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Viridiplantae</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sar</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  phylum_pred_label  count\n",
       "0        UNASSIGNED    163\n",
       "1           Metazoa    140\n",
       "2             Fungi     65\n",
       "3     Viridiplantae     11\n",
       "4               Sar      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rank 'class'  (column='class_pred_label')  -> UNASSIGNED 161/380 = 42.37%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_pred_label</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UNASSIGNED</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chordata</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dikarya</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ecdysozoa</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chlorophyta</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Streptophyta</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Cnidaria</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Stramenopiles</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class_pred_label  count\n",
       "0       UNASSIGNED    161\n",
       "1         Chordata    131\n",
       "2          Dikarya     64\n",
       "3        Ecdysozoa     10\n",
       "4      Chlorophyta      8\n",
       "5     Streptophyta      3\n",
       "6         Cnidaria      2\n",
       "7    Stramenopiles      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rank 'order'  (column='order_pred_label')  -> UNASSIGNED 157/380 = 41.32%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_pred_label</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UNASSIGNED</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Craniata</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ascomycota</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basidiomycota</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nematoda</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>core chlorophytes</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Myxozoa</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Embryophyta</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    order_pred_label  count\n",
       "0         UNASSIGNED    157\n",
       "1           Craniata    133\n",
       "2         Ascomycota     33\n",
       "3      Basidiomycota     32\n",
       "4           Nematoda     10\n",
       "5  core chlorophytes      8\n",
       "6            Myxozoa      2\n",
       "7        Embryophyta      2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rank 'family'  (column='family_pred_label')  -> UNASSIGNED 155/380 = 40.79%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>family_pred_label</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UNASSIGNED</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vertebrata</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pezizomycotina</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Agaricomycotina</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trebouxiophyceae</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Enoplea</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Chromadorea</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Myxosporea</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  family_pred_label  count\n",
       "0        UNASSIGNED    155\n",
       "1        Vertebrata    133\n",
       "2    Pezizomycotina     34\n",
       "3   Agaricomycotina     32\n",
       "4  Trebouxiophyceae      8\n",
       "5           Enoplea      6\n",
       "6       Chromadorea      4\n",
       "7        Myxosporea      2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rank 'genus'  (column='genus_pred_label')  -> UNASSIGNED 155/380 = 40.79%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genus_pred_label</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UNASSIGNED</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Euteleostomi</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Agaricomycetes</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pezizomycetes</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sordariomycetes</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Eurotiomycetes</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dorylaimia</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Prasiolales</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  genus_pred_label  count\n",
       "0       UNASSIGNED    155\n",
       "1     Euteleostomi    133\n",
       "2   Agaricomycetes     32\n",
       "3    Pezizomycetes     15\n",
       "4  Sordariomycetes      7\n",
       "5   Eurotiomycetes      6\n",
       "6       Dorylaimia      6\n",
       "7      Prasiolales      5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rank 'species'  (column='species_pred_label')  -> UNASSIGNED 144/380 = 37.89%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species_pred_label</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UNASSIGNED</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maylandia zebra</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chaetodon auriga</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Morchella sp.</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arvicanthis niloticus</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Aspergillus costaricensis</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Callospermophilus lateralis</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hysterothylacium fabri</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            species_pred_label  count\n",
       "0                   UNASSIGNED    144\n",
       "1              Maylandia zebra     80\n",
       "2             Chaetodon auriga     46\n",
       "3                Morchella sp.     10\n",
       "4        Arvicanthis niloticus      9\n",
       "5    Aspergillus costaricensis      7\n",
       "6  Callospermophilus lateralis      6\n",
       "7       Hysterothylacium fabri      5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary assignment rates (by rank):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>pred_col</th>\n",
       "      <th>total</th>\n",
       "      <th>assigned</th>\n",
       "      <th>unassigned</th>\n",
       "      <th>pct_unassigned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kingdom</td>\n",
       "      <td>kingdom_pred_label</td>\n",
       "      <td>380</td>\n",
       "      <td>164</td>\n",
       "      <td>216</td>\n",
       "      <td>0.568421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>phylum</td>\n",
       "      <td>phylum_pred_label</td>\n",
       "      <td>380</td>\n",
       "      <td>217</td>\n",
       "      <td>163</td>\n",
       "      <td>0.428947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>class</td>\n",
       "      <td>class_pred_label</td>\n",
       "      <td>380</td>\n",
       "      <td>219</td>\n",
       "      <td>161</td>\n",
       "      <td>0.423684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>order</td>\n",
       "      <td>order_pred_label</td>\n",
       "      <td>380</td>\n",
       "      <td>223</td>\n",
       "      <td>157</td>\n",
       "      <td>0.413158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>family</td>\n",
       "      <td>family_pred_label</td>\n",
       "      <td>380</td>\n",
       "      <td>225</td>\n",
       "      <td>155</td>\n",
       "      <td>0.407895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>genus</td>\n",
       "      <td>genus_pred_label</td>\n",
       "      <td>380</td>\n",
       "      <td>225</td>\n",
       "      <td>155</td>\n",
       "      <td>0.407895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>species</td>\n",
       "      <td>species_pred_label</td>\n",
       "      <td>380</td>\n",
       "      <td>236</td>\n",
       "      <td>144</td>\n",
       "      <td>0.378947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      rank            pred_col  total  assigned  unassigned  pct_unassigned\n",
       "0  kingdom  kingdom_pred_label    380       164         216        0.568421\n",
       "1   phylum   phylum_pred_label    380       217         163        0.428947\n",
       "2    class    class_pred_label    380       219         161        0.423684\n",
       "3    order    order_pred_label    380       223         157        0.413158\n",
       "4   family   family_pred_label    380       225         155        0.407895\n",
       "5    genus    genus_pred_label    380       225         155        0.407895\n",
       "6  species  species_pred_label    380       236         144        0.378947"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of UNASSIGNED reads with species_pred_conf >= 0.5: 86\n",
      "Examples (first 8):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>species_pred_conf</th>\n",
       "      <th>species_novel_component</th>\n",
       "      <th>genus_novel_component</th>\n",
       "      <th>novel_score</th>\n",
       "      <th>_species_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>JBPZNU010001377.1</td>\n",
       "      <td>0.750098</td>\n",
       "      <td>0.206548</td>\n",
       "      <td>0.015048</td>\n",
       "      <td>0.110798</td>\n",
       "      <td>UNASSIGNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LC876572.1</td>\n",
       "      <td>0.863004</td>\n",
       "      <td>0.098259</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>0.049607</td>\n",
       "      <td>UNASSIGNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>PX277059.1</td>\n",
       "      <td>0.795968</td>\n",
       "      <td>0.208243</td>\n",
       "      <td>0.014360</td>\n",
       "      <td>0.111301</td>\n",
       "      <td>UNASSIGNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LC876609.1</td>\n",
       "      <td>0.773430</td>\n",
       "      <td>0.174087</td>\n",
       "      <td>0.003924</td>\n",
       "      <td>0.089006</td>\n",
       "      <td>UNASSIGNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>PX273803.1</td>\n",
       "      <td>0.676611</td>\n",
       "      <td>0.265072</td>\n",
       "      <td>0.068322</td>\n",
       "      <td>0.166697</td>\n",
       "      <td>UNASSIGNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>JF836109.1</td>\n",
       "      <td>0.878108</td>\n",
       "      <td>0.103703</td>\n",
       "      <td>0.002911</td>\n",
       "      <td>0.053307</td>\n",
       "      <td>UNASSIGNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>JBPZNU010001350.1</td>\n",
       "      <td>0.779167</td>\n",
       "      <td>0.167112</td>\n",
       "      <td>0.010263</td>\n",
       "      <td>0.088687</td>\n",
       "      <td>UNASSIGNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>JBPZNU010001331.1</td>\n",
       "      <td>0.647170</td>\n",
       "      <td>0.215164</td>\n",
       "      <td>0.015348</td>\n",
       "      <td>0.115256</td>\n",
       "      <td>UNASSIGNED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id  species_pred_conf  species_novel_component  \\\n",
       "5   JBPZNU010001377.1           0.750098                 0.206548   \n",
       "8          LC876572.1           0.863004                 0.098259   \n",
       "17         PX277059.1           0.795968                 0.208243   \n",
       "18         LC876609.1           0.773430                 0.174087   \n",
       "20         PX273803.1           0.676611                 0.265072   \n",
       "21         JF836109.1           0.878108                 0.103703   \n",
       "23  JBPZNU010001350.1           0.779167                 0.167112   \n",
       "24  JBPZNU010001331.1           0.647170                 0.215164   \n",
       "\n",
       "    genus_novel_component  novel_score _species_norm  \n",
       "5                0.015048     0.110798    UNASSIGNED  \n",
       "8                0.000956     0.049607    UNASSIGNED  \n",
       "17               0.014360     0.111301    UNASSIGNED  \n",
       "18               0.003924     0.089006    UNASSIGNED  \n",
       "20               0.068322     0.166697    UNASSIGNED  \n",
       "21               0.002911     0.053307    UNASSIGNED  \n",
       "23               0.010263     0.088687    UNASSIGNED  \n",
       "24               0.015348     0.115256    UNASSIGNED  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average species_pred_conf: assigned = 0.576, unassigned = 0.621\n",
      "\n",
      "\n",
      "DONE. Summary:\n",
      " - If UNASSIGNED fraction is high at species but low at higher ranks, that suggests the model can place reads to genus/family but not to species (database coverage or species-level ambiguity).\n",
      " - If UNASSIGNED is high at all ranks, look at the input sequences (short sequences, contamination, novel clades) or the reference DB used for training.\n"
     ]
    }
   ],
   "source": [
    "# Fixed diagnostic + assignment-rate cell\n",
    "# Paste & run in your notebook (same kernel you used earlier).\n",
    "import math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "\n",
    "def find_extracted_candidates():\n",
    "    cand = []\n",
    "    cand.append(Path.cwd() / \"sih\" / \"ncbi_blast_db\" / \"extracted\")\n",
    "    cand.append(Path.cwd() / \"ncbi_blast_db\" / \"extracted\")\n",
    "    cand.append(Path.cwd() / \"extracted\")\n",
    "    cand.append(Path.cwd())\n",
    "    cand.append(Path(r\"C:\\Users\\Srijit\\OneDrive\\Desktop\\sihtaxa\\sihabundance\\ncbi_blast_db\\extracted\"))\n",
    "    cand.append(Path(r\"C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\"))\n",
    "    cand.append(Path.home() / \"OneDrive\" / \"Desktop\" / \"sihtaxa\" / \"sihabundance\" / \"ncbi_blast_db\" / \"extracted\")\n",
    "    for p in Path.cwd().glob(\"**/extracted\"):\n",
    "        cand.append(p.resolve())\n",
    "    uniq = []\n",
    "    for p in cand:\n",
    "        if p in uniq:\n",
    "            continue\n",
    "        uniq.append(p)\n",
    "    return uniq\n",
    "\n",
    "def pick_file(names):\n",
    "    for d in find_extracted_candidates():\n",
    "        for name in names:\n",
    "            p = d / name\n",
    "            if p.exists():\n",
    "                return p\n",
    "    for name in names:\n",
    "        for p in Path.cwd().rglob(name):\n",
    "            return p.resolve()\n",
    "    return None\n",
    "\n",
    "def normalize_label(x):\n",
    "    if pd.isna(x):\n",
    "        return \"UNASSIGNED\"\n",
    "    s = str(x).strip()\n",
    "    s = \" \".join(s.split())\n",
    "    return s if s!=\"\" else \"UNASSIGNED\"\n",
    "\n",
    "# locate files\n",
    "pred_path = pick_file([\"predictions_with_uncertainty.csv\", \"predictions.csv\"])\n",
    "val_path  = pick_file([\"val_predictions_calibrated.csv\", \"val_predictions.csv\", \"val_predictions_with_uncertainty.csv\"])\n",
    "reconciled_path = pick_file([\"abundance_reconciled_species.csv\",\"abundance_from_predictions_deconvolved.csv\"])\n",
    "\n",
    "print(\"predictions:\", pred_path)\n",
    "print(\"validation :\", val_path)\n",
    "print(\"reconciled :\", reconciled_path)\n",
    "print(\"-\"*60)\n",
    "\n",
    "if pred_path is None:\n",
    "    raise SystemExit(\"No predictions CSV found. Run the inference cell first.\")\n",
    "df_pred = pd.read_csv(pred_path)\n",
    "print(\"Loaded predictions rows:\", len(df_pred))\n",
    "cols_lower = {c.lower():c for c in df_pred.columns}\n",
    "\n",
    "# detect predicted species and confidence columns\n",
    "species_pred_col = None\n",
    "for cand in (\"species_pred_label\",\"species_label\",\"species_pred\",\"species_predicted\",\"species\"):\n",
    "    if cand in cols_lower:\n",
    "        species_pred_col = cols_lower[cand]; break\n",
    "if species_pred_col is None:\n",
    "    for c in df_pred.columns:\n",
    "        if \"species\" in c.lower():\n",
    "            species_pred_col = c; break\n",
    "\n",
    "conf_col = None\n",
    "for cand in (\"species_pred_conf\",\"species_conf\",\"species_prob\",\"species_probability\",\"pred_conf\"):\n",
    "    if cand in cols_lower:\n",
    "        conf_col = cols_lower[cand]; break\n",
    "\n",
    "print(\"Detected species prediction column:\", species_pred_col)\n",
    "print(\"Detected confidence column:\", conf_col)\n",
    "df_pred[\"_species_norm\"] = df_pred[species_pred_col].astype(str).apply(normalize_label) if species_pred_col else \"<no-species>\"\n",
    "\n",
    "if val_path is None:\n",
    "    print(\"\\nNo validation file found -> skipping classification accuracy checks.\")\n",
    "else:\n",
    "    df_val = pd.read_csv(val_path)\n",
    "    # Normalize likely rank columns\n",
    "    for c in df_val.columns:\n",
    "        if any(r in c.lower() for r in (\"species\",\"genus\",\"family\",\"order\",\"class\",\"phylum\",\"kingdom\")):\n",
    "            try:\n",
    "                df_val[c] = df_val[c].apply(normalize_label)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # detect true & pred species columns in validation\n",
    "    cols_val_lower = {c.lower(): c for c in df_val.columns}\n",
    "    true_species_col = None\n",
    "    for cand in (\"species_true\",\"true_species\",\"species_label\",\"species_gold\",\"label_species\",\"species_true_idx\",\"species_true_label\"):\n",
    "        if cand in cols_val_lower:\n",
    "            true_species_col = cols_val_lower[cand]; break\n",
    "\n",
    "    pred_species_col_val = None\n",
    "    for cand in (\"species_pred_label\",\"species_pred\",\"species_prediction\",\"species_label_pred\",\"pred_species\",\"species_pred_label\"):\n",
    "        if cand in cols_val_lower:\n",
    "            pred_species_col_val = cols_val_lower[cand]; break\n",
    "\n",
    "    # fallback if both not found\n",
    "    if (true_species_col is None or pred_species_col_val is None):\n",
    "        species_like = [c for c in df_val.columns if \"species\" in c.lower()]\n",
    "        if len(species_like) >= 2:\n",
    "            if true_species_col is None: true_species_col = species_like[0]\n",
    "            if pred_species_col_val is None: pred_species_col_val = species_like[1]\n",
    "\n",
    "    print(\"\\nValidation: true =\", true_species_col, \", pred =\", pred_species_col_val)\n",
    "    if true_species_col and pred_species_col_val:\n",
    "        y_true = df_val[true_species_col].astype(str).apply(normalize_label)\n",
    "        y_pred_val = df_val[pred_species_col_val].astype(str).apply(normalize_label)\n",
    "\n",
    "        acc = accuracy_score(y_true, y_pred_val)\n",
    "        macro_f1 = f1_score(y_true, y_pred_val, average=\"macro\", zero_division=0)\n",
    "        micro_f1 = f1_score(y_true, y_pred_val, average=\"micro\", zero_division=0)\n",
    "        print(\"\\nSpecies-level metrics on validation:\")\n",
    "        print(f\"  Accuracy = {acc:.4f}    Macro-F1 = {macro_f1:.4f}    Micro-F1 = {micro_f1:.4f}\")\n",
    "\n",
    "        # top-classes by support (build safe dataframe)\n",
    "        top_counts = y_true.value_counts().reset_index().rename(columns={\"index\":\"species\",\"0\":\"count\"}) \n",
    "        top_counts.columns = [\"species\",\"count\"]\n",
    "        # labels_by_freq as list (safe)\n",
    "        labels_by_freq = top_counts[\"species\"].tolist()\n",
    "        print(\"\\nTop 15 classes by true-support:\")\n",
    "        display(top_counts.head(15))\n",
    "\n",
    "        # classification report for top classes (safe usage)\n",
    "        try:\n",
    "            report = classification_report(y_true, y_pred_val, labels=labels_by_freq[:100], zero_division=0)\n",
    "            print(\"\\nClassification report (top classes shown):\\n\")\n",
    "            print(report)\n",
    "        except Exception as e:\n",
    "            # fallback: full report (may be large)\n",
    "            print(\"Could not generate trimmed classification report (fallback to full report). Error:\", e)\n",
    "            print(classification_report(y_true, y_pred_val, zero_division=0))\n",
    "\n",
    "        # confusion pairs (top mistakes)\n",
    "        n_labels = min(len(labels_by_freq), 200)\n",
    "        cm = confusion_matrix(y_true, y_pred_val, labels=labels_by_freq[:n_labels])\n",
    "        cm_df = pd.DataFrame(cm, index=labels_by_freq[:n_labels], columns=labels_by_freq[:n_labels])\n",
    "        stacked = cm_df.stack().reset_index()\n",
    "        stacked.columns = [\"true\",\"pred\",\"count\"]\n",
    "        mistakes = stacked[stacked[\"true\"] != stacked[\"pred\"]].sort_values(\"count\", ascending=False)\n",
    "        print(\"\\nTop 12 confusion pairs (true -> predicted):\")\n",
    "        display(mistakes.head(12))\n",
    "\n",
    "        # try calibration (ECE) if confidences can be matched\n",
    "        matched = None\n",
    "        id_cols_pred = [c for c in df_pred.columns if c.lower() in (\"id\",\"seqid\",\"accession\",\"read_id\",\"readid\")]\n",
    "        id_cols_val = [c for c in df_val.columns if c.lower() in (\"id\",\"seqid\",\"accession\",\"read_id\",\"readid\")]\n",
    "        if id_cols_pred and id_cols_val:\n",
    "            p_id, v_id = id_cols_pred[0], id_cols_val[0]\n",
    "            df_pred_small = df_pred[[p_id, \"_species_norm\"] + ([\"_conf_num\"] if conf_col and \"_conf_num\" in df_pred.columns else [])].copy()\n",
    "            df_val_small = df_val[[v_id, true_species_col]].copy()\n",
    "            merged = pd.merge(df_val_small, df_pred_small, left_on=v_id, right_on=p_id, how=\"inner\")\n",
    "            if len(merged) > 0:\n",
    "                matched = merged\n",
    "        if matched is None and len(df_val)==len(df_pred):\n",
    "            # fallback row-wise (risky but sometimes ok)\n",
    "            merged = pd.DataFrame({\n",
    "                \"true\": df_val[true_species_col].astype(str).apply(normalize_label).reset_index(drop=True),\n",
    "                \"pred\": df_pred[\"_species_norm\"].reset_index(drop=True),\n",
    "            })\n",
    "            if \"_conf_num\" in df_pred.columns:\n",
    "                merged[\"conf\"] = df_pred[\"_conf_num\"].reset_index(drop=True)\n",
    "            matched = merged\n",
    "\n",
    "        if matched is not None and \"conf\" in matched.columns:\n",
    "            matched[\"correct\"] = (matched[\"pred\"].astype(str).apply(normalize_label) == matched[\"true\"].astype(str).apply(normalize_label)).astype(int)\n",
    "            bins = np.linspace(0.0, 1.0, 11)\n",
    "            matched[\"_bin\"] = pd.cut(matched[\"conf\"], bins, include_lowest=True)\n",
    "            bin_summary = matched.groupby(\"_bin\").agg(conf_mean=('conf','mean'), acc=('correct','mean'), n=('correct','size')).reset_index().fillna(0)\n",
    "            N = len(matched)\n",
    "            ece = ((bin_summary['n']/N) * (bin_summary['acc'] - bin_summary['conf_mean']).abs()).sum()\n",
    "            print(f\"\\nCalibration (on matched rows): ECE = {ece:.4f}\")\n",
    "            display(bin_summary[[\"conf_mean\",\"acc\",\"n\"]])\n",
    "            # reliability plot\n",
    "            plt.figure(figsize=(6,4))\n",
    "            plt.plot(bin_summary['conf_mean'], bin_summary['acc'], marker='o')\n",
    "            plt.plot([0,1],[0,1], linestyle='--')\n",
    "            plt.xlabel('Mean predicted confidence'); plt.ylabel('Observed accuracy')\n",
    "            plt.title('Reliability diagram (binned)')\n",
    "            plt.tight_layout(); plt.show()\n",
    "        else:\n",
    "            print(\"\\nCalibration check: could not match confidences to val rows or confidences not present.\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nValidation file did not contain recognisable true/pred species columns. Skipping species-level accuracy checks.\")\n",
    "\n",
    "# Section: compute assignment rates per rank (UNASSIGNED fraction)\n",
    "print(\"\\n\" + \"#\"*8 + \" Assignment rates per rank \" + \"#\"*8)\n",
    "ranks = [\"kingdom\",\"phylum\",\"class\",\"order\",\"family\",\"genus\",\"species\"]\n",
    "rank_assignments = []\n",
    "for r in ranks:\n",
    "    # find a column in df_pred that looks like this rank\n",
    "    candidate = None\n",
    "    for c in df_pred.columns:\n",
    "        if r in c.lower():\n",
    "            # skip short columns like 'species_pred_idx' etc; use the label-like columns (contain 'label') where possible\n",
    "            if 'label' in c.lower() or c.lower().endswith(r):\n",
    "                candidate = c\n",
    "                break\n",
    "            if candidate is None:\n",
    "                candidate = c\n",
    "    if candidate is None:\n",
    "        continue\n",
    "    col_norm = df_pred[candidate].astype(str).apply(normalize_label)\n",
    "    total = len(col_norm)\n",
    "    unassigned = (col_norm == \"UNASSIGNED\").sum()\n",
    "    assigned = total - unassigned\n",
    "    rank_assignments.append({\"rank\": r, \"pred_col\": candidate, \"total\": total, \"assigned\": assigned, \"unassigned\": unassigned, \"pct_unassigned\": unassigned/total})\n",
    "    # show top taxa for the rank\n",
    "    top = col_norm.value_counts().head(8).reset_index().rename(columns={\"index\":\"taxon\",0:\"count\"})\n",
    "    print(f\"\\nRank '{r}'  (column='{candidate}')  -> UNASSIGNED {unassigned}/{total} = {unassigned/total:.2%}\")\n",
    "    display(top)\n",
    "\n",
    "rank_df = pd.DataFrame(rank_assignments)\n",
    "print(\"\\nSummary assignment rates (by rank):\")\n",
    "display(rank_df)\n",
    "\n",
    "# Optional: check UNASSIGNED entries that have high species confidence (investigate novel or mismatch)\n",
    "if conf_col and conf_col in df_pred.columns:\n",
    "    df_pred[\"_conf_num\"] = pd.to_numeric(df_pred[conf_col], errors='coerce').fillna(0.0)\n",
    "    high_conf_unassigned = df_pred[(df_pred[\"_species_norm\"]==\"UNASSIGNED\") & (df_pred[\"_conf_num\"] >= 0.5)]\n",
    "    print(\"\\nNumber of UNASSIGNED reads with species_pred_conf >= 0.5:\", len(high_conf_unassigned))\n",
    "    if len(high_conf_unassigned) > 0:\n",
    "        print(\"Examples (first 8):\")\n",
    "        display(high_conf_unassigned[[col for col in df_pred.columns if col in (\"id\",\"_species_norm\",conf_col,\"novel_score\",\"species_novel_component\",\"genus_novel_component\")][:8]].head(8))\n",
    "    # average confidence for assigned vs unassigned\n",
    "    avg_assigned = df_pred[df_pred[\"_species_norm\"]!=\"UNASSIGNED\"][\"_conf_num\"].mean()\n",
    "    avg_unassigned = df_pred[df_pred[\"_species_norm\"]==\"UNASSIGNED\"][\"_conf_num\"].mean()\n",
    "    print(f\"\\nAverage species_pred_conf: assigned = {avg_assigned:.3f}, unassigned = {avg_unassigned:.3f}\")\n",
    "else:\n",
    "    print(\"\\nConfidence column not present or not numeric; skipping high-confidence UNASSIGNED check.\")\n",
    "\n",
    "print(\"\\n\\nDONE. Summary:\")\n",
    "print(\" - If UNASSIGNED fraction is high at species but low at higher ranks, that suggests the model can place reads to genus/family but not to species (database coverage or species-level ambiguity).\")\n",
    "print(\" - If UNASSIGNED is high at all ranks, look at the input sequences (short sequences, contamination, novel clades) or the reference DB used for training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "54b81772-5ffc-4e38-9f3a-56bdff5f9301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using predictions file: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\predictions_with_uncertainty.csv\n",
      "Loaded rows: 380\n",
      "Wrote all UNASSIGNED rows -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\unassigned_rows.csv  (count = 144)\n",
      "Wrote high-confidence UNASSIGNED rows (conf >= 0.5) -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\high_conf_unassigned_rows.csv  (count = 86)\n",
      "Wrote high-confidence UNASSIGNED ID list -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\high_conf_unassigned_ids.txt\n",
      "\n",
      "SAFE MODE: force_assign=False — no labels changed. To try forced relabeling, set force_assign=True and re-run the cell.\n",
      "Conservative example settings: force_assign=True, assign_conf_threshold=0.60, novel_comp_max=0.20\n",
      "\n",
      "DONE. Files written to: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\n",
      " - unassigned_rows.csv\n",
      " - high_conf_unassigned_rows.csv\n",
      " - high_conf_unassigned_ids.txt  (if high-confidence IDs exist)\n",
      " - abundance_forced_assignments.csv  (only if force_assign=True)\n"
     ]
    }
   ],
   "source": [
    "# CELL: Inspect & handle UNASSIGNED reads (safe by default)\n",
    "# - Finds your predictions CSV, exports UNASSIGNED rows + high-confidence subset,\n",
    "#   writes an ID list for BLAST/lookup.\n",
    "# - Optionally (only if you set force_assign=True) conservatively force-assigns some UNASSIGNED -> predicted species.\n",
    "#\n",
    "# Edit the top parameters if you want to try forced reassignment; default is non-destructive (force_assign=False).\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "try:\n",
    "    # ---------- User parameters ----------\n",
    "    force_assign = False           # Set True to attempt conservative forced-assignment (not recommended unless you understand the risk)\n",
    "    assign_conf_threshold = 0.60   # if forced assignment: require species_pred_conf >= this\n",
    "    novel_comp_max = 0.20          # if present: require species_novel_component <= this to allow forced assignment\n",
    "    high_conf_cutoff = 0.50        # threshold for \"high-confidence UNASSIGNED\" export\n",
    "    # -------------------------------------\n",
    "\n",
    "    # ---------- find predictions CSV (includes paths you reported) ----------\n",
    "    candidate_paths = [\n",
    "        Path.cwd() / \"sih\" / \"ncbi_blast_db\" / \"extracted\" / \"predictions_with_uncertainty.csv\",\n",
    "        Path.cwd() / \"ncbi_blast_db\" / \"extracted\" / \"predictions_with_uncertainty.csv\",\n",
    "        Path.cwd() / \"extracted\" / \"predictions_with_uncertainty.csv\",\n",
    "        Path.cwd() / \"predictions_with_uncertainty.csv\",\n",
    "        Path(r\"C:\\Users\\Srijit\\OneDrive\\Desktop\\sihtaxa\\sihabundance\\ncbi_blast_db\\extracted\\predictions_with_uncertainty.csv\"),\n",
    "        Path(r\"C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\predictions_with_uncertainty.csv\"),\n",
    "        Path(r\"C:\\Users\\HP\\sihabundance\\ncbi_blast_db\\extracted\\predictions_with_uncertainty.csv\"),\n",
    "        Path.cwd() / \"sih\" / \"ncbi_blast_db\" / \"extracted\" / \"predictions.csv\",\n",
    "        Path.cwd() / \"ncbi_blast_db\" / \"extracted\" / \"predictions.csv\",\n",
    "    ]\n",
    "    # add any file named predictions_with_uncertainty.csv under cwd recursively as fallback\n",
    "    for p in Path.cwd().rglob(\"predictions_with_uncertainty.csv\"):\n",
    "        candidate_paths.append(p)\n",
    "    predictions_file = None\n",
    "    for p in candidate_paths:\n",
    "        if p.exists():\n",
    "            predictions_file = p.resolve()\n",
    "            break\n",
    "    if predictions_file is None:\n",
    "        # last resort: recursive search for common names\n",
    "        for fname in (\"predictions_with_uncertainty.csv\",\"predictions.csv\"):\n",
    "            for p in Path.cwd().rglob(fname):\n",
    "                predictions_file = p.resolve()\n",
    "                break\n",
    "            if predictions_file is not None:\n",
    "                break\n",
    "\n",
    "    if predictions_file is None:\n",
    "        raise FileNotFoundError(\"Could not find predictions CSV. Put 'predictions_with_uncertainty.csv' into an extracted/ folder and re-run.\")\n",
    "\n",
    "    print(\"Using predictions file:\", predictions_file)\n",
    "    out_dir = predictions_file.parent\n",
    "\n",
    "    # ---------- load ----------\n",
    "    df = pd.read_csv(predictions_file)\n",
    "    print(\"Loaded rows:\", len(df))\n",
    "\n",
    "    # detect useful columns\n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "    # species prediction column\n",
    "    species_col = None\n",
    "    for cand in (\"species_pred_label\",\"species_label\",\"species_pred\",\"species\"):\n",
    "        if cand in cols_lower:\n",
    "            species_col = cols_lower[cand]; break\n",
    "    if species_col is None:\n",
    "        for c in df.columns:\n",
    "            if \"species\" in c.lower():\n",
    "                species_col = c; break\n",
    "    if species_col is None:\n",
    "        raise RuntimeError(\"No species prediction column detected in predictions CSV.\")\n",
    "\n",
    "    # confidence column (optional)\n",
    "    conf_col = None\n",
    "    for cand in (\"species_pred_conf\",\"species_conf\",\"species_prob\",\"species_probability\",\"pred_conf\"):\n",
    "        if cand in cols_lower:\n",
    "            conf_col = cols_lower[cand]; break\n",
    "\n",
    "    # id column for BLAST lookup (optional)\n",
    "    id_col = None\n",
    "    for cand in (\"id\",\"accession\",\"seqid\",\"read_id\",\"readid\",\"global_index\"):\n",
    "        if cand in cols_lower:\n",
    "            id_col = cols_lower[cand]; break\n",
    "\n",
    "    # normalize working columns\n",
    "    df[\"_species_norm\"] = df[species_col].astype(str).fillna(\"UNASSIGNED\").apply(lambda s: \" \".join(str(s).split()))\n",
    "    if conf_col:\n",
    "        df[\"_conf_num\"] = pd.to_numeric(df[conf_col], errors=\"coerce\").fillna(0.0)\n",
    "    else:\n",
    "        df[\"_conf_num\"] = np.nan\n",
    "\n",
    "    # select UNASSIGNED rows\n",
    "    is_unassigned = df[\"_species_norm\"].str.upper() == \"UNASSIGNED\"\n",
    "    unassigned_df = df[is_unassigned].copy().reset_index(drop=True)\n",
    "\n",
    "    # write all UNASSIGNED rows\n",
    "    out_unassigned = out_dir / \"unassigned_rows.csv\"\n",
    "    unassigned_df.to_csv(out_unassigned, index=False)\n",
    "    print(f\"Wrote all UNASSIGNED rows -> {out_unassigned}  (count = {len(unassigned_df)})\")\n",
    "\n",
    "    # high-confidence subset (for BLAST/inspection)\n",
    "    high_conf_unassigned = unassigned_df[unassigned_df[\"_conf_num\"] >= high_conf_cutoff].copy()\n",
    "    out_high_conf = out_dir / \"high_conf_unassigned_rows.csv\"\n",
    "    high_conf_unassigned.to_csv(out_high_conf, index=False)\n",
    "    print(f\"Wrote high-confidence UNASSIGNED rows (conf >= {high_conf_cutoff}) -> {out_high_conf}  (count = {len(high_conf_unassigned)})\")\n",
    "\n",
    "    # write simple ID list for BLAST/lookup (we do NOT have sequence FASTA here)\n",
    "    if len(high_conf_unassigned) > 0:\n",
    "        if id_col:\n",
    "            ids = high_conf_unassigned[id_col].astype(str).fillna(\"\")\n",
    "        elif \"global_index\" in df.columns:\n",
    "            ids = high_conf_unassigned[\"global_index\"].astype(str).fillna(\"\")\n",
    "        else:\n",
    "            ids = high_conf_unassigned.index.astype(str)\n",
    "        out_ids = out_dir / \"high_conf_unassigned_ids.txt\"\n",
    "        ids.to_csv(out_ids, index=False, header=False)\n",
    "        print(\"Wrote high-confidence UNASSIGNED ID list ->\", out_ids)\n",
    "    else:\n",
    "        print(\"No high-confidence UNASSIGNED rows to export as ID list (threshold may be too high).\")\n",
    "\n",
    "    # If you have original FASTA and want FASTA created for BLAST, place the FASTA in the same extracted/ folder\n",
    "    # with filename 'predictions_source_sequences.fasta' where header contains the same accession/id column value.\n",
    "    # This cell does not attempt to reconstruct sequences automatically.\n",
    "\n",
    "    # ---------- Optional conservative forced-assignment ----------\n",
    "    if force_assign:\n",
    "        print(\"\\nFORCE-ASSIGN MODE: attempting conservative re-labeling of some UNASSIGNED reads.\")\n",
    "        # novel component column if present\n",
    "        novel_col = None\n",
    "        for cand in (\"species_novel_component\",\"novel_component\",\"novel_score\"):\n",
    "            if cand in cols_lower:\n",
    "                novel_col = cols_lower[cand]; break\n",
    "        if novel_col:\n",
    "            df[\"_novel_comp\"] = pd.to_numeric(df.get(novel_col), errors=\"coerce\").fillna(1.0)\n",
    "        else:\n",
    "            df[\"_novel_comp\"] = 1.0\n",
    "        candidates = df[is_unassigned & (df[\"_conf_num\"] >= assign_conf_threshold) & (df[\"_novel_comp\"] <= novel_comp_max)].copy()\n",
    "        print(\"Forced-assignment candidates (conservative rule):\", len(candidates))\n",
    "        if len(candidates) > 0:\n",
    "            # create forced copy and assign predicted species label into _species_norm\n",
    "            df_forced = df.copy()\n",
    "            df_forced.loc[candidates.index, \"_species_norm\"] = df_forced.loc[candidates.index, species_col].astype(str).apply(lambda s: \" \".join(str(s).split()))\n",
    "            species_counts = df_forced[\"_species_norm\"].value_counts().reset_index().rename(columns={\"index\":\"species\", \"_species_norm\":\"count\"})\n",
    "            out_forced = out_dir / \"abundance_forced_assignments.csv\"\n",
    "            species_counts.to_csv(out_forced, index=False)\n",
    "            print(\"Wrote species counts after forced-assignment ->\", out_forced)\n",
    "            display(species_counts.head(40))\n",
    "        else:\n",
    "            print(\"No candidates met forced-assignment criteria; nothing changed.\")\n",
    "    else:\n",
    "        print(\"\\nSAFE MODE: force_assign=False — no labels changed. To try forced relabeling, set force_assign=True and re-run the cell.\")\n",
    "        print(\"Conservative example settings: force_assign=True, assign_conf_threshold=0.60, novel_comp_max=0.20\")\n",
    "\n",
    "    print(\"\\nDONE. Files written to:\", out_dir)\n",
    "    print(\" - unassigned_rows.csv\")\n",
    "    print(\" - high_conf_unassigned_rows.csv\")\n",
    "    print(\" - high_conf_unassigned_ids.txt  (if high-confidence IDs exist)\")\n",
    "    print(\" - abundance_forced_assignments.csv  (only if force_assign=True)\")\n",
    "\n",
    "except Exception as exc:\n",
    "    print(\"ERROR during processing:\", exc)\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "23df80d4-cff0-4d6c-af2c-93be00886dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using predictions: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\predictions_with_uncertainty.csv\n",
      "Output directory: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\n",
      "Loaded rows: 380\n",
      "UNASSIGNED count: 144 (37.9%)\n",
      "Saved all UNASSIGNED rows -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\unassigned_rows.csv\n",
      "Saved high-confidence UNASSIGNED rows (conf >= 0.5) -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\high_conf_unassigned_rows.csv\n",
      "Saved ID list for BLAST -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\high_conf_unassigned_ids.txt\n",
      "\n",
      "Found FASTA: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\its_combined.fasta\n",
      "FASTA header count: 699\n",
      "Matched sequences: 0 / 86 requested (high-conf IDs).\n",
      "Unmatched IDs (sample up to 20): ['JBPZNU010001377.1', 'LC876572.1', 'PX277059.1', 'LC876609.1', 'PX273803.1', 'JF836109.1', 'JBPZNU010001350.1', 'JBPZNU010001331.1', 'JF836111.1', 'PX278865.1', 'PX277079.1', 'PX278850.1', 'PX278853.1', 'LC876580.1', 'LC876528.1', 'LC876609.1', 'LC876593.1', 'JBPZNU010001303.1', 'LC876579.1', 'LC876562.1']\n",
      "Common cause: FASTA headers use different accession formats/contain versions or database prefixes.\n",
      "\n",
      "Saved genus-fallback abundance -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\abundance_species_genus_fallback.csv\n",
      "  species_or_genus_fallback  count      rel\n",
      "                 UNASSIGNED    143 0.376316\n",
      "            Maylandia zebra     80 0.210526\n",
      "           Chaetodon auriga     46 0.121053\n",
      "              Morchella sp.     10 0.026316\n",
      "      Arvicanthis niloticus      9 0.023684\n",
      "  Aspergillus costaricensis      7 0.018421\n",
      "Callospermophilus lateralis      6 0.015789\n",
      "               Entoloma sp.      5 0.013158\n",
      "     Hysterothylacium fabri      5 0.013158\n",
      "        Amanita fuscozonata      4 0.010526\n",
      "               Eucoleus sp.      4 0.010526\n",
      "         Trichoderma viride      3 0.007895\n",
      "\n",
      "Saved family-fallback abundance -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\abundance_species_family_fallback.csv\n",
      " species_or_family_fallback  count      rel\n",
      "                 UNASSIGNED    142 0.373684\n",
      "            Maylandia zebra     80 0.210526\n",
      "           Chaetodon auriga     46 0.121053\n",
      "              Morchella sp.     10 0.026316\n",
      "      Arvicanthis niloticus      9 0.023684\n",
      "  Aspergillus costaricensis      7 0.018421\n",
      "Callospermophilus lateralis      6 0.015789\n",
      "     Hysterothylacium fabri      5 0.013158\n",
      "               Entoloma sp.      5 0.013158\n",
      "               Eucoleus sp.      4 0.010526\n",
      "        Amanita fuscozonata      4 0.010526\n",
      " Chloroidium saccharophilum      3 0.007895\n",
      "\n",
      "Saved redistributed-by-reconciled -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\abundance_unassigned_redistributed_by_reconciled.csv\n",
      "                        species  orig_count  added_from_unassigned  new_count  new_rel\n",
      "                Maylandia zebra          80                     74        154 0.405263\n",
      "               Chaetodon auriga          46                     20         66 0.173684\n",
      "          Arvicanthis niloticus           9                      6         15 0.039474\n",
      "                  Morchella sp.          10                      4         14 0.036842\n",
      "      Aspergillus costaricensis           7                      2          9 0.023684\n",
      "            Amanita fuscozonata           4                      3          7 0.018421\n",
      "                   Entoloma sp.           5                      1          6 0.015789\n",
      "Deuterostichococcus epilithicus           3                      3          6 0.015789\n",
      "    Callospermophilus lateralis           6                      0          6 0.015789\n",
      "     Chloroidium saccharophilum           3                      3          6 0.015789\n",
      "       Pseudopestalotiopsis sp.           3                      3          6 0.015789\n",
      "         Hysterothylacium fabri           5                      0          5 0.013158\n",
      "\n",
      "Done. Created/updated files are in: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\n",
      "- unassigned_rows.csv\n",
      "- high_conf_unassigned_rows.csv\n",
      "- high_conf_unassigned_ids.txt (if high-conf IDs exist)\n",
      "- high_conf_unassigned_seqs.fasta (if source FASTA matched headers)\n",
      "- abundance_species_genus_fallback.csv\n",
      "- abundance_species_family_fallback.csv\n",
      "- abundance_unassigned_redistributed_by_reconciled.csv (if reconciled file present)\n",
      "- abundance_forced_assignments.csv (only if you enable force_assign=True)\n"
     ]
    }
   ],
   "source": [
    "# FIXED cell: robust UNASSIGNED handling + FASTA matching + fixed rel computation\n",
    "# Paste & run in the same notebook/kernel you used earlier.\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, traceback\n",
    "\n",
    "# ---------- User params (edit if you want) ----------\n",
    "force_assign = False           # default: do NOT change labels\n",
    "high_conf_cutoff = 0.50        # used for \"high-confidence UNASSIGNED\" export\n",
    "assign_conf_threshold = 0.60   # only used if force_assign=True\n",
    "novel_comp_max = 0.20          # only used if force_assign=True\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def find_predictions():\n",
    "    candidates = [\n",
    "        Path(r\"C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\predictions_with_uncertainty.csv\"),\n",
    "        Path(r\"C:\\Users\\Srijit\\OneDrive\\Desktop\\sihtaxa\\sihabundance\\ncbi_blast_db\\extracted\\predictions_with_uncertainty.csv\"),\n",
    "        Path(r\"C:\\Users\\HP\\sihabundance\\ncbi_blast_db\\extracted\\predictions_with_uncertainty.csv\"),\n",
    "        Path.cwd() / \"sih\" / \"ncbi_blast_db\" / \"extracted\" / \"predictions_with_uncertainty.csv\",\n",
    "        Path.cwd() / \"ncbi_blast_db\" / \"extracted\" / \"predictions_with_uncertainty.csv\",\n",
    "        Path.cwd() / \"extracted\" / \"predictions_with_uncertainty.csv\",\n",
    "        Path.cwd() / \"predictions_with_uncertainty.csv\",\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if p.exists(): return p.resolve()\n",
    "    # fallback recursive search (within cwd)\n",
    "    for p in Path.cwd().rglob(\"predictions_with_uncertainty.csv\"):\n",
    "        return p.resolve()\n",
    "    for p in Path.cwd().rglob(\"predictions.csv\"):\n",
    "        return p.resolve()\n",
    "    return None\n",
    "\n",
    "def find_fasta(folder):\n",
    "    # look for common names first, then any fasta-like file\n",
    "    names = [\"predictions_source_sequences.fasta\",\"source_sequences.fasta\",\"sequences.fasta\",\n",
    "             \"input.fasta\",\"reads.fasta\",\"predictions_seqs.fasta\",\"its_combined.fasta\"]\n",
    "    for n in names:\n",
    "        p = folder / n\n",
    "        if p.exists(): return p.resolve()\n",
    "    for ext in (\"*.fasta\",\"*.fa\",\"*.fna\",\"*.ffn\"):\n",
    "        for p in folder.rglob(ext):\n",
    "            return p.resolve()\n",
    "    return None\n",
    "\n",
    "def parse_fasta_simple(path):\n",
    "    seqs = {}\n",
    "    with open(path, \"r\", encoding=\"utf8\", errors=\"replace\") as fh:\n",
    "        header = None; seq_lines=[]\n",
    "        for line in fh:\n",
    "            line = line.rstrip(\"\\n\\r\")\n",
    "            if not line: continue\n",
    "            if line.startswith(\">\"):\n",
    "                if header is not None:\n",
    "                    key = header.split()[0]\n",
    "                    seqs[key] = (header, \"\".join(seq_lines))\n",
    "                header = line[1:].strip()\n",
    "                seq_lines=[]\n",
    "            else:\n",
    "                seq_lines.append(line.strip())\n",
    "        if header is not None:\n",
    "            key = header.split()[0]\n",
    "            seqs[key] = (header, \"\".join(seq_lines))\n",
    "    return seqs\n",
    "\n",
    "def canonicalize_acc(a):\n",
    "    # canonical forms: original, without version (strip trailing .\\d+), lower-case\n",
    "    a = str(a).strip()\n",
    "    forms = [a]\n",
    "    # strip leading db qualifiers (e.g. \"gi|...\", \"ref|\", \"gb|\")\n",
    "    if \"|\" in a:\n",
    "        parts = a.split(\"|\")\n",
    "        # try each token\n",
    "        for tok in parts:\n",
    "            tok = tok.strip()\n",
    "            if tok: forms.append(tok)\n",
    "    # without version\n",
    "    m = re.match(r\"^(.+?)(?:\\.\\d+)$\", a)\n",
    "    if m:\n",
    "        forms.append(m.group(1))\n",
    "    return list(dict.fromkeys([f for f in forms if f]))  # unique preserving order\n",
    "\n",
    "# ----------- begin main -----------\n",
    "pred_file = find_predictions()\n",
    "if pred_file is None:\n",
    "    raise SystemExit(\"Could not find predictions CSV (searched common paths). Place predictions_with_uncertainty.csv into an extracted/ folder and re-run.\")\n",
    "\n",
    "out_dir = pred_file.parent\n",
    "print(\"Using predictions:\", pred_file)\n",
    "print(\"Output directory:\", out_dir)\n",
    "\n",
    "df = pd.read_csv(pred_file)\n",
    "print(\"Loaded rows:\", len(df))\n",
    "\n",
    "# detect important columns robustly\n",
    "cols_lower = {c.lower(): c for c in df.columns}\n",
    "species_col = next((cols_lower[k] for k in (\"species_pred_label\",\"species_label\",\"species_pred\",\"species\") if k in cols_lower), None)\n",
    "if not species_col:\n",
    "    species_col = next((c for c in df.columns if \"species\" in c.lower()), None)\n",
    "genus_col = next((cols_lower[k] for k in (\"genus_pred_label\",\"genus_label\",\"genus_pred\",\"genus\") if k in cols_lower), None)\n",
    "family_col = next((cols_lower[k] for k in (\"family_pred_label\",\"family_label\",\"family_pred\",\"family\") if k in cols_lower), None)\n",
    "conf_col = next((cols_lower[k] for k in (\"species_pred_conf\",\"species_conf\",\"species_prob\",\"species_probability\",\"pred_conf\") if k in cols_lower), None)\n",
    "id_col = next((cols_lower[k] for k in (\"id\",\"accession\",\"seqid\",\"read_id\",\"readid\",\"global_index\") if k in cols_lower), None)\n",
    "\n",
    "if not species_col:\n",
    "    raise SystemExit(\"Could not detect species prediction column in the predictions CSV.\")\n",
    "\n",
    "# normalize columns\n",
    "df[\"_species_norm\"] = df[species_col].astype(str).fillna(\"UNASSIGNED\").apply(lambda s: \" \".join(str(s).split()))\n",
    "df[\"_conf_num\"] = pd.to_numeric(df[conf_col], errors=\"coerce\").fillna(0.0) if conf_col else np.nan\n",
    "df[\"_genus_norm\"] = df[genus_col].astype(str).fillna(\"UNASSIGNED\").apply(lambda s: \" \".join(str(s).split())) if genus_col else \"UNASSIGNED\"\n",
    "df[\"_family_norm\"] = df[family_col].astype(str).fillna(\"UNASSIGNED\").apply(lambda s: \" \".join(str(s).split())) if family_col else \"UNASSIGNED\"\n",
    "\n",
    "# export unassigned files (again)\n",
    "is_unassigned = df[\"_species_norm\"].str.upper() == \"UNASSIGNED\"\n",
    "unassigned_df = df[is_unassigned].copy().reset_index(drop=True)\n",
    "print(\"UNASSIGNED count:\", len(unassigned_df), f\"({len(unassigned_df)/len(df):.1%})\")\n",
    "\n",
    "out_unassigned = out_dir / \"unassigned_rows.csv\"\n",
    "unassigned_df.to_csv(out_unassigned, index=False)\n",
    "print(\"Saved all UNASSIGNED rows ->\", out_unassigned)\n",
    "\n",
    "hc_df = unassigned_df[unassigned_df[\"_conf_num\"] >= high_conf_cutoff].copy()\n",
    "out_hc = out_dir / \"high_conf_unassigned_rows.csv\"\n",
    "hc_df.to_csv(out_hc, index=False)\n",
    "print(f\"Saved high-confidence UNASSIGNED rows (conf >= {high_conf_cutoff}) ->\", out_hc)\n",
    "\n",
    "# write id list for BLAST (if ids exist)\n",
    "if len(hc_df) > 0:\n",
    "    if id_col and id_col in hc_df.columns:\n",
    "        ids = hc_df[id_col].astype(str).fillna(\"\")\n",
    "    elif \"global_index\" in hc_df.columns:\n",
    "        ids = hc_df[\"global_index\"].astype(str).fillna(\"\")\n",
    "    else:\n",
    "        ids = hc_df.index.astype(str)\n",
    "    out_ids = out_dir / \"high_conf_unassigned_ids.txt\"\n",
    "    ids.to_csv(out_ids, index=False, header=False)\n",
    "    print(\"Saved ID list for BLAST ->\", out_ids)\n",
    "else:\n",
    "    print(\"No high-confidence UNASSIGNED rows to write ID list for (threshold {}).\".format(high_conf_cutoff))\n",
    "\n",
    "# ---------- attempt robust FASTA matching and export ----------\n",
    "fasta_file = find_fasta(out_dir)\n",
    "if fasta_file is None:\n",
    "    print(\"\\nNo FASTA found in extracted/ folder. If you have the original FASTA, place it in the same folder and re-run to extract sequences for BLAST.\")\n",
    "else:\n",
    "    print(\"\\nFound FASTA:\", fasta_file)\n",
    "    seqs = parse_fasta_simple(fasta_file)\n",
    "    print(\"FASTA header count:\", len(seqs))\n",
    "    # build header lookup variants for fast matching\n",
    "    header_map = {}   # key -> (header, seq)\n",
    "    for hid, (hdr, seq) in seqs.items():\n",
    "        # canonical forms for header id\n",
    "        forms = canonicalize_acc(hid)\n",
    "        # also include header token without db prefixes if it contains '|'\n",
    "        if \"|\" in hid:\n",
    "            for tok in hid.split(\"|\"):\n",
    "                tok = tok.strip()\n",
    "                if tok:\n",
    "                    forms.extend(canonicalize_acc(tok))\n",
    "        # push all forms into lookup\n",
    "        for f in set(forms):\n",
    "            header_map[f] = (hid, hdr, seq)\n",
    "        # also map the full header string as a fallback\n",
    "        header_map[hdr] = (hid, hdr, seq)\n",
    "\n",
    "    # prepare ID list we want to match (from hc_df)\n",
    "    if len(hc_df) > 0:\n",
    "        if id_col and id_col in hc_df.columns:\n",
    "            id_list_orig = hc_df[id_col].astype(str).tolist()\n",
    "        elif \"global_index\" in hc_df.columns:\n",
    "            id_list_orig = hc_df[\"global_index\"].astype(str).tolist()\n",
    "        else:\n",
    "            id_list_orig = hc_df.index.astype(str).tolist()\n",
    "\n",
    "        matched = {}\n",
    "        unmatched = []\n",
    "        for q in id_list_orig:\n",
    "            q_forms = canonicalize_acc(q)\n",
    "            found = False\n",
    "            for qf in q_forms:\n",
    "                if qf in header_map:\n",
    "                    hid, hdr, seq = header_map[qf]\n",
    "                    matched[q] = (hid, hdr, seq)\n",
    "                    found = True\n",
    "                    break\n",
    "            if found:\n",
    "                continue\n",
    "            # try substring search in header strings (fallback): look for q or q without version\n",
    "            q_nov = re.sub(r'\\.\\d+$', '', q)\n",
    "            for hid0, (hdr0, seq0) in seqs.items():\n",
    "                if q in hid0 or q in hdr0 or q_nov in hid0 or q_nov in hdr0:\n",
    "                    matched[q] = (hid0, hdr0, seq0)\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                unmatched.append(q)\n",
    "\n",
    "        print(f\"Matched sequences: {len(matched)} / {len(id_list_orig)} requested (high-conf IDs).\")\n",
    "        if len(matched) > 0:\n",
    "            out_fa = out_dir / \"high_conf_unassigned_seqs.fasta\"\n",
    "            with open(out_fa, \"w\", encoding=\"utf8\") as fh:\n",
    "                for q,(hid, hdr, seq) in matched.items():\n",
    "                    fh.write(f\">{q} {hdr}\\n\")\n",
    "                    for i in range(0, len(seq), 80):\n",
    "                        fh.write(seq[i:i+80] + \"\\n\")\n",
    "            print(\"Wrote FASTA for matched high-confidence UNASSIGNED IDs ->\", out_fa)\n",
    "        if len(unmatched) > 0:\n",
    "            print(\"Unmatched IDs (sample up to 20):\", unmatched[:20])\n",
    "            print(\"Common cause: FASTA headers use different accession formats/contain versions or database prefixes.\")\n",
    "    else:\n",
    "        print(\"No high-confidence IDs to extract from FASTA.\")\n",
    "\n",
    "# ---------- genus fallback (fixed numeric dtype and rel calculation) ----------\n",
    "try:\n",
    "    df_genus_fb = df.copy()\n",
    "    df_genus_fb[\"_species_genus_fallback\"] = df_genus_fb[\"_species_norm\"]\n",
    "    mask_genus = (df_genus_fb[\"_species_genus_fallback\"].str.upper()==\"UNASSIGNED\") & (df_genus_fb[\"_genus_norm\"].str.upper()!=\"UNASSIGNED\")\n",
    "    df_genus_fb.loc[mask_genus, \"_species_genus_fallback\"] = df_genus_fb.loc[mask_genus, \"_genus_norm\"].apply(lambda g: f\"GENUS::{g}\")\n",
    "\n",
    "    counts_genus_fb = df_genus_fb[\"_species_genus_fallback\"].value_counts().reset_index()\n",
    "    counts_genus_fb.columns = [\"species_or_genus_fallback\", \"count\"]\n",
    "    # enforce numeric dtype\n",
    "    counts_genus_fb[\"count\"] = pd.to_numeric(counts_genus_fb[\"count\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    total = counts_genus_fb[\"count\"].sum()\n",
    "    counts_genus_fb[\"rel\"] = counts_genus_fb[\"count\"] / total if total>0 else 0.0\n",
    "    out_genus = out_dir / \"abundance_species_genus_fallback.csv\"\n",
    "    counts_genus_fb.to_csv(out_genus, index=False)\n",
    "    print(\"\\nSaved genus-fallback abundance ->\", out_genus)\n",
    "    print(counts_genus_fb.head(12).to_string(index=False))\n",
    "except Exception as e:\n",
    "    print(\"Error creating genus-fallback table:\", e)\n",
    "    traceback.print_exc()\n",
    "\n",
    "# ---------- family fallback (fixed) ----------\n",
    "try:\n",
    "    df_family_fb = df.copy()\n",
    "    df_family_fb[\"_species_family_fallback\"] = df_family_fb[\"_species_norm\"]\n",
    "    mask_family = (df_family_fb[\"_species_family_fallback\"].str.upper()==\"UNASSIGNED\") & (df_family_fb[\"_family_norm\"].str.upper()!=\"UNASSIGNED\")\n",
    "    df_family_fb.loc[mask_family, \"_species_family_fallback\"] = df_family_fb.loc[mask_family, \"_family_norm\"].apply(lambda f: f\"FAMILY::{f}\")\n",
    "\n",
    "    counts_family_fb = df_family_fb[\"_species_family_fallback\"].value_counts().reset_index()\n",
    "    counts_family_fb.columns = [\"species_or_family_fallback\", \"count\"]\n",
    "    counts_family_fb[\"count\"] = pd.to_numeric(counts_family_fb[\"count\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    totalf = counts_family_fb[\"count\"].sum()\n",
    "    counts_family_fb[\"rel\"] = counts_family_fb[\"count\"] / totalf if totalf>0 else 0.0\n",
    "    out_family = out_dir / \"abundance_species_family_fallback.csv\"\n",
    "    counts_family_fb.to_csv(out_family, index=False)\n",
    "    print(\"\\nSaved family-fallback abundance ->\", out_family)\n",
    "    print(counts_family_fb.head(12).to_string(index=False))\n",
    "except Exception as e:\n",
    "    print(\"Error creating family-fallback table:\", e)\n",
    "    traceback.print_exc()\n",
    "\n",
    "# ---------- optional proportional redistribution using reconciled proportions ----------\n",
    "recon_candidates = [\"abundance_reconciled_species.csv\",\"abundance_from_predictions_deconvolved.csv\",\"abundance_from_predictions.csv\"]\n",
    "recon_file = None\n",
    "for fn in recon_candidates:\n",
    "    p = out_dir / fn\n",
    "    if p.exists():\n",
    "        recon_file = p; break\n",
    "\n",
    "if recon_file is None:\n",
    "    print(\"\\nNo reconciled abundance CSV found; skipping proportional redistribution step.\")\n",
    "else:\n",
    "    try:\n",
    "        recon = pd.read_csv(recon_file)\n",
    "        cols_low = {c.lower(): c for c in recon.columns}\n",
    "        sp_col_recon = cols_low.get(\"species\", list(recon.columns)[0])\n",
    "        # pick a likely relative column\n",
    "        rel_col_candidates = [\"est_rel\",\"est_true_rel\",\"rel\",\"relative_abundance\",\"relative_abundance_weighted\",\"pred_count_rel\",\"relative_abundance_count\"]\n",
    "        rel_col = None\n",
    "        for cand in rel_col_candidates:\n",
    "            if cand in cols_low:\n",
    "                rel_col = cols_low[cand]; break\n",
    "        if rel_col is None:\n",
    "            numeric_cols = [c for c in recon.columns if pd.api.types.is_numeric_dtype(recon[c])]\n",
    "            rel_col = numeric_cols[0] if numeric_cols else None\n",
    "        if rel_col is None:\n",
    "            print(\"Could not identify a rel-abundance column in reconciled file; skipping redistribution.\")\n",
    "        else:\n",
    "            recon2 = recon[[sp_col_recon, rel_col]].rename(columns={sp_col_recon:\"species\", rel_col:\"est_rel\"})\n",
    "            recon2[\"species\"] = recon2[\"species\"].astype(str).apply(lambda s: \" \".join(str(s).split()))\n",
    "            recon_nonun = recon2[recon2[\"species\"].str.upper()!=\"UNASSIGNED\"].copy()\n",
    "            if recon_nonun[\"est_rel\"].sum() <= 0:\n",
    "                print(\"Reconciled file has zero non-UNASSIGNED mass; cannot redistribute.\")\n",
    "            else:\n",
    "                recon_nonun[\"renorm\"] = recon_nonun[\"est_rel\"] / recon_nonun[\"est_rel\"].sum()\n",
    "                total_unassigned = len(unassigned_df)\n",
    "                recon_nonun[\"alloc_float\"] = recon_nonun[\"renorm\"] * total_unassigned\n",
    "                recon_nonun[\"alloc_int\"] = recon_nonun[\"alloc_float\"].round().astype(int)\n",
    "                diff = int(total_unassigned - recon_nonun[\"alloc_int\"].sum())\n",
    "                if diff != 0:\n",
    "                    rem = recon_nonun[\"alloc_float\"] - recon_nonun[\"alloc_int\"]\n",
    "                    if diff > 0:\n",
    "                        idxs = rem.sort_values(ascending=False).index.tolist()\n",
    "                        for i in range(diff):\n",
    "                            recon_nonun.at[idxs[i % len(idxs)], \"alloc_int\"] += 1\n",
    "                    else:\n",
    "                        idxs = rem.sort_values(ascending=True).index.tolist()\n",
    "                        for i in range(-diff):\n",
    "                            recon_nonun.at[idxs[i % len(idxs)], \"alloc_int\"] -= 1\n",
    "                raw_counts = df[\"_species_norm\"].value_counts().to_dict()\n",
    "                rows=[]\n",
    "                for _,r in recon_nonun.iterrows():\n",
    "                    sp = r[\"species\"]\n",
    "                    orig = int(raw_counts.get(sp,0))\n",
    "                    add = int(r[\"alloc_int\"])\n",
    "                    rows.append({\"species\":sp,\"orig_count\":orig,\"added_from_unassigned\":add,\"new_count\":orig+add})\n",
    "                new_df = pd.DataFrame(rows)\n",
    "                new_df[\"new_rel\"] = new_df[\"new_count\"] / new_df[\"new_count\"].sum() if new_df[\"new_count\"].sum()>0 else 0.0\n",
    "                out_redis = out_dir / \"abundance_unassigned_redistributed_by_reconciled.csv\"\n",
    "                new_df.to_csv(out_redis, index=False)\n",
    "                print(\"\\nSaved redistributed-by-reconciled ->\", out_redis)\n",
    "                print(new_df.sort_values(\"new_count\", ascending=False).head(12).to_string(index=False))\n",
    "    except Exception as e:\n",
    "        print(\"Error while redistributing:\", e)\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\nDone. Created/updated files are in:\", out_dir)\n",
    "print(\"- unassigned_rows.csv\")\n",
    "print(\"- high_conf_unassigned_rows.csv\")\n",
    "print(\"- high_conf_unassigned_ids.txt (if high-conf IDs exist)\")\n",
    "print(\"- high_conf_unassigned_seqs.fasta (if source FASTA matched headers)\")\n",
    "print(\"- abundance_species_genus_fallback.csv\")\n",
    "print(\"- abundance_species_family_fallback.csv\")\n",
    "print(\"- abundance_unassigned_redistributed_by_reconciled.csv (if reconciled file present)\")\n",
    "print(\"- abundance_forced_assignments.csv (only if you enable force_assign=True)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bc2bce2a-36a3-49c8-b017-8b903b2050e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using extracted folder: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\n",
      "Read 86 high-confidence UNASSIGNED IDs from high_conf_unassigned_ids.txt\n",
      "Using FASTA: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\its_combined.fasta\n",
      "Parsed FASTA headers: 699\n",
      "\n",
      "--- Preview high-conf IDs (first 80) ---\n",
      "  1. JBPZNU010001377.1\n",
      "  2. LC876572.1\n",
      "  3. PX277059.1\n",
      "  4. LC876609.1\n",
      "  5. PX273803.1\n",
      "  6. JF836109.1\n",
      "  7. JBPZNU010001350.1\n",
      "  8. JBPZNU010001331.1\n",
      "  9. JF836111.1\n",
      " 10. PX278865.1\n",
      " 11. PX277079.1\n",
      " 12. PX278850.1\n",
      " 13. PX278853.1\n",
      " 14. LC876580.1\n",
      " 15. LC876528.1\n",
      " 16. LC876609.1\n",
      " 17. LC876593.1\n",
      " 18. JBPZNU010001303.1\n",
      " 19. LC876579.1\n",
      " 20. LC876562.1\n",
      " 21. LC876572.1\n",
      " 22. LC876528.1\n",
      " 23. LC876567.1\n",
      " 24. LC876579.1\n",
      " 25. JF836094.1\n",
      " 26. JBPZNU010001397.1\n",
      " 27. LC876593.1\n",
      " 28. PX278864.1\n",
      " 29. JBPZNU010001375.1\n",
      " 30. JF836094.1\n",
      " 31. LC876521.1\n",
      " 32. OQ241938.1\n",
      " 33. LC876568.1\n",
      " 34. XR_013090605.1\n",
      " 35. LC876580.1\n",
      " 36. JF836095.1\n",
      " 37. JBPZNU010001366.1\n",
      " 38. LC876553.1\n",
      " 39. JBPZNU010001353.1\n",
      " 40. JBPZNU010001377.1\n",
      " 41. LC876519.1\n",
      " 42. LC876630.1\n",
      " 43. LC876630.1\n",
      " 44. LC876619.1\n",
      " 45. PX278852.1\n",
      " 46. LC876566.1\n",
      " 47. JF836117.1\n",
      " 48. LC876566.1\n",
      " 49. LC876553.1\n",
      " 50. PX278084.1\n",
      " 51. JBPZNU010001364.1\n",
      " 52. JBPZNU010001353.1\n",
      " 53. LC876619.1\n",
      " 54. JBPZNU010001397.1\n",
      " 55. JBPZNU010001339.1\n",
      " 56. JBPZNU010001350.1\n",
      " 57. LC876623.1\n",
      " 58. PQ523743.1\n",
      " 59. JBPZNU010001386.1\n",
      " 60. LC876519.1\n",
      " 61. JBPZNU010001386.1\n",
      " 62. LC876601.1\n",
      " 63. JBPZNU010001345.1\n",
      " 64. JF836095.1\n",
      " 65. JBPZNU010001339.1\n",
      " 66. LC876567.1\n",
      " 67. JBPZNU010001331.1\n",
      " 68. JBPZNU010001328.1\n",
      " 69. LC876521.1\n",
      " 70. JF836104.1\n",
      " 71. JF836109.1\n",
      " 72. LC876568.1\n",
      " 73. JF836104.1\n",
      " 74. JBPZNU010001328.1\n",
      " 75. JBPZNU010001303.1\n",
      " 76. JF836117.1\n",
      " 77. JBPZNU010001366.1\n",
      " 78. LC876623.1\n",
      " 79. JBPZNU010001375.1\n",
      " 80. LC876563.1\n",
      "\n",
      "--- Preview FASTA header keys (first 80) ---\n",
      "  1. LC890403.1\n",
      "  2. PX285876.1\n",
      "  3. PX285871.1\n",
      "  4. PX285870.1\n",
      "  5. PX285869.1\n",
      "  6. PX285868.1\n",
      "  7. PX285867.1\n",
      "  8. PX285866.1\n",
      "  9. PX285865.1\n",
      " 10. PX285864.1\n",
      " 11. PX285863.1\n",
      " 12. PX285862.1\n",
      " 13. PX285861.1\n",
      " 14. PX285860.1\n",
      " 15. PX285859.1\n",
      " 16. PX285858.1\n",
      " 17. PX285857.1\n",
      " 18. PX285856.1\n",
      " 19. PX285855.1\n",
      " 20. PX285854.1\n",
      " 21. PX285853.1\n",
      " 22. PX285852.1\n",
      " 23. PX285851.1\n",
      " 24. PX285850.1\n",
      " 25. PX285849.1\n",
      " 26. PX285848.1\n",
      " 27. PX285846.1\n",
      " 28. PX285845.1\n",
      " 29. PX285844.1\n",
      " 30. PX285843.1\n",
      " 31. PX285841.1\n",
      " 32. PX285840.1\n",
      " 33. PX285838.1\n",
      " 34. PX285785.1\n",
      " 35. PX285783.1\n",
      " 36. PX285782.1\n",
      " 37. PX285781.1\n",
      " 38. PX285774.1\n",
      " 39. PX285769.1\n",
      " 40. PX113199.1\n",
      " 41. PV975055.1\n",
      " 42. PV975054.1\n",
      " 43. PV975053.1\n",
      " 44. PV975052.1\n",
      " 45. PV975051.1\n",
      " 46. PV650852.1\n",
      " 47. PV650851.1\n",
      " 48. PV650850.1\n",
      " 49. PV650849.1\n",
      " 50. PV650848.1\n",
      " 51. PV650847.1\n",
      " 52. PV650846.1\n",
      " 53. PV650845.1\n",
      " 54. PQ834537.1\n",
      " 55. PQ728411.1\n",
      " 56. PQ139111.1\n",
      " 57. PQ139110.1\n",
      " 58. PQ139109.1\n",
      " 59. PQ139108.1\n",
      " 60. PQ139107.1\n",
      " 61. PQ139106.1\n",
      " 62. PQ139105.1\n",
      " 63. PQ139104.1\n",
      " 64. PQ139103.1\n",
      " 65. OK104033.1\n",
      " 66. OK104032.1\n",
      " 67. OK104031.1\n",
      " 68. OK104030.1\n",
      " 69. OK104029.1\n",
      " 70. OK104028.1\n",
      " 71. OK104027.1\n",
      " 72. OK104026.1\n",
      " 73. OK104025.1\n",
      " 74. OK104024.1\n",
      " 75. OK104023.1\n",
      " 76. OK104022.1\n",
      " 77. OK104021.1\n",
      " 78. PX308181.1\n",
      " 79. PX308180.1\n",
      " 80. PV919998.1\n",
      "\n",
      "Matched 0 / 86 high-conf IDs by heuristics. Unmatched: 86\n",
      "Wrote mapping CSV -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\high_conf_unassigned_matched_mapping.csv\n",
      "No matched FASTA written (no matches).\n",
      "Wrote unmatched ID list -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\high_conf_unassigned_unmatched_ids.txt\n",
      "\n",
      "Done. Files written to: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\n",
      " - high_conf_unassigned_matched_mapping.csv\n",
      " - high_conf_unassigned_seqs_matched.fasta   (if any matches)\n",
      " - high_conf_unassigned_unmatched_ids.txt\n",
      " - (if force_assign=True) abundance_forced_assignments.csv and abundance_forced_assignments_reconciled.csv\n",
      "\n",
      "Next recommended actions (pick one):\n",
      "  1) Inspect 'high_conf_unassigned_matched_mapping.csv' to verify matches (VERY IMPORTANT).\n",
      "  2) Run BLAST locally on 'high_conf_unassigned_seqs_matched.fasta' or on unmatched IDs' sequences to confirm species identity.\n",
      "  3) If BLAST confirms match and you trust it, re-run with force_assign=True to absorb matched high-confidence UNASSIGNED into species counts.\n",
      "\n",
      "If you want, I can now (A) provide the local BLAST command to run against NCBI/your DB, (B) flip force_assign=True and re-run the assignment heuristics here, or (C) produce a small report comparing before/after abundances. Tell me which and I'll give the exact cell/commands.\n"
     ]
    }
   ],
   "source": [
    "# Cell: inspect FASTA headers vs high-conf UNASSIGNED IDs, attempt robust matching,\n",
    "# and (optionally) perform a conservative forced-assignment of matched UNASSIGNED reads.\n",
    "#\n",
    "# SAFE DEFAULTS: force_assign=False. Set force_assign=True only AFTER you inspect the mapping CSV\n",
    "# and understand that forcing will reassign some UNASSIGNED reads to predicted species.\n",
    "#\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------- USER FLAGS (edit only if you want to force assignments) ----------\n",
    "force_assign = False             # default False -> NO label changes\n",
    "assign_conf_threshold = 0.60     # if forcing: require species_pred_conf >= this\n",
    "novel_comp_max = 0.20            # if forcing: require species_novel_component <= this (if column present)\n",
    "high_conf_cutoff = 0.50          # high-confidence threshold used previously\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# ---------- find extracted folder (common locations) ----------\n",
    "candidates = [\n",
    "    Path.cwd() / \"sih\" / \"ncbi_blast_db\" / \"extracted\",\n",
    "    Path.cwd() / \"ncbi_blast_db\" / \"extracted\",\n",
    "    Path.cwd() / \"extracted\",\n",
    "    Path.cwd(),\n",
    "    Path(r\"C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\"),\n",
    "    Path(r\"C:\\Users\\Srijit\\OneDrive\\Desktop\\sihtaxa\\sihabundance\\ncbi_blast_db\\extracted\"),\n",
    "    Path(r\"C:\\Users\\HP\\sihabundance\\ncbi_blast_db\\extracted\"),\n",
    "]\n",
    "extracted = None\n",
    "for p in candidates:\n",
    "    if p and p.exists() and p.is_dir():\n",
    "        # verify predictions present\n",
    "        if (p / \"predictions_with_uncertainty.csv\").exists() or (p / \"predictions.csv\").exists() or (p / \"predictions_with_uncertainty.csv\").exists():\n",
    "            extracted = p.resolve()\n",
    "            break\n",
    "\n",
    "# fallback: try to find the id file recursively under cwd\n",
    "if extracted is None:\n",
    "    for f in Path.cwd().rglob(\"high_conf_unassigned_ids.txt\"):\n",
    "        extracted = f.parent.resolve(); break\n",
    "\n",
    "if extracted is None:\n",
    "    raise SystemExit(\"Could not find the 'extracted' folder containing prediction outputs. Run the earlier cells in this notebook or set the path here.\")\n",
    "\n",
    "print(\"Using extracted folder:\", extracted)\n",
    "\n",
    "# ---------- required files ----------\n",
    "preds_candidates = [extracted / \"predictions_with_uncertainty.csv\", extracted / \"predictions.csv\"]\n",
    "pred_file = next((p for p in preds_candidates if p.exists()), None)\n",
    "if pred_file is None:\n",
    "    raise SystemExit(\"predictions CSV not found in extracted/. Run the classification cells first.\")\n",
    "\n",
    "ids_file = extracted / \"high_conf_unassigned_ids.txt\"\n",
    "rows_file = extracted / \"high_conf_unassigned_rows.csv\"\n",
    "# if the simple id file doesn't exist, try to extract ids from the rows CSV\n",
    "if not ids_file.exists():\n",
    "    if rows_file.exists():\n",
    "        df_rows = pd.read_csv(rows_file)\n",
    "        id_col = next((c for c in df_rows.columns if c.lower() in (\"id\",\"accession\",\"seqid\",\"read_id\",\"readid\",\"accession_id\")), df_rows.columns[0])\n",
    "        ids = df_rows[id_col].astype(str).tolist()\n",
    "        print(f\"Extracted {len(ids)} IDs from {rows_file.name} using column '{id_col}'\")\n",
    "    else:\n",
    "        raise SystemExit(\"high_conf_unassigned_ids.txt and high_conf_unassigned_rows.csv not found. Run the previous 'UNASSIGNED handling' cell first.\")\n",
    "else:\n",
    "    with open(ids_file, \"r\", encoding=\"utf8\", errors=\"replace\") as fh:\n",
    "        ids = [line.strip() for line in fh if line.strip()]\n",
    "    print(f\"Read {len(ids)} high-confidence UNASSIGNED IDs from {ids_file.name}\")\n",
    "\n",
    "# ---------- search for FASTA ----------\n",
    "fasta_names = [\"its_combined.fasta\",\"predictions_source_sequences.fasta\",\"source_sequences.fasta\",\"sequences.fasta\",\"input.fasta\",\"reads.fasta\"]\n",
    "fasta_path = None\n",
    "for n in fasta_names:\n",
    "    p = extracted / n\n",
    "    if p.exists():\n",
    "        fasta_path = p; break\n",
    "if fasta_path is None:\n",
    "    # pick any fasta-like file in folder\n",
    "    for ext in (\"*.fasta\",\"*.fa\",\"*.fna\",\"*.ffn\"):\n",
    "        found = list(extracted.rglob(ext))\n",
    "        if found:\n",
    "            fasta_path = found[0]; break\n",
    "\n",
    "if fasta_path is None:\n",
    "    print(\"No FASTA found in extracted/. Place the FASTA with your source sequences in that folder (common name: its_combined.fasta) and re-run this cell.\")\n",
    "else:\n",
    "    print(\"Using FASTA:\", fasta_path)\n",
    "\n",
    "# ---------- parse FASTA headers (simple parser) ----------\n",
    "def parse_fasta_headers(path):\n",
    "    seqs = {}\n",
    "    with open(path, \"r\", encoding=\"utf8\", errors=\"replace\") as fh:\n",
    "        header = None; seq_lines=[]\n",
    "        for line in fh:\n",
    "            line = line.rstrip(\"\\n\\r\")\n",
    "            if not line: continue\n",
    "            if line.startswith(\">\"):\n",
    "                if header is not None:\n",
    "                    seqs[header.split()[0]] = (header, \"\".join(seq_lines))\n",
    "                header = line[1:].strip()\n",
    "                seq_lines = []\n",
    "            else:\n",
    "                seq_lines.append(line.strip())\n",
    "        if header is not None:\n",
    "            seqs[header.split()[0]] = (header, \"\".join(seq_lines))\n",
    "    return seqs\n",
    "\n",
    "seqs = parse_fasta_headers(fasta_path)\n",
    "print(\"Parsed FASTA headers:\", len(seqs))\n",
    "\n",
    "# Preview first 80 IDs & first 80 FASTA header keys\n",
    "print(\"\\n--- Preview high-conf IDs (first 80) ---\")\n",
    "for i, x in enumerate(ids[:80], 1):\n",
    "    print(f\"{i:3d}. {x}\")\n",
    "print(\"\\n--- Preview FASTA header keys (first 80) ---\")\n",
    "header_keys = list(seqs.keys())\n",
    "for i, h in enumerate(header_keys[:80], 1):\n",
    "    print(f\"{i:3d}. {h}\")\n",
    "\n",
    "# ---------- canonicalization and matching heuristics ----------\n",
    "def canonical_forms(x):\n",
    "    x = str(x).strip()\n",
    "    forms = [x]\n",
    "    # strip trailing version like .1 .2\n",
    "    nov = re.sub(r'\\.\\d+$', '', x)\n",
    "    if nov != x: forms.append(nov)\n",
    "    # split on DB pipe tokens\n",
    "    if \"|\" in x:\n",
    "        toks = [t.strip() for t in re.split(r'[|/]', x) if t.strip()]\n",
    "        forms.extend(toks)\n",
    "    # drop common prefixes like 'ref|', 'gb|' etc.\n",
    "    forms2 = []\n",
    "    for f in list(forms):\n",
    "        f2 = re.sub(r'^(?:ref|gi|gb|emb|dbj|accession)[\\|:]', '', f, flags=re.IGNORECASE)\n",
    "        forms2.append(f2)\n",
    "    # lower-case variants\n",
    "    out = []\n",
    "    for s in forms + forms2:\n",
    "        if s and s not in out:\n",
    "            out.append(s)\n",
    "        s_low = s.lower() if s else s\n",
    "        if s_low and s_low not in out:\n",
    "            out.append(s_low)\n",
    "    return out\n",
    "\n",
    "# build header lookup\n",
    "header_map = {}\n",
    "for hid, (hdr, seq) in seqs.items():\n",
    "    for f in canonical_forms(hid):\n",
    "        header_map[f] = (hid, hdr, seq)\n",
    "    header_map[hdr] = (hid, hdr, seq)\n",
    "    header_map[hdr.lower()] = (hid, hdr, seq)\n",
    "\n",
    "matched = {}\n",
    "unmatched = []\n",
    "for q in ids:\n",
    "    found = False\n",
    "    for qf in canonical_forms(q):\n",
    "        if qf in header_map:\n",
    "            matched[q] = header_map[qf]; found = True; break\n",
    "    if found: continue\n",
    "    # fallback: substring match in header strings (strip version first)\n",
    "    q_nov = re.sub(r'\\.\\d+$', '', q)\n",
    "    for hid, (hdr, seq) in seqs.items():\n",
    "        if q in hid or q in hdr or q_nov in hid or q_nov in hdr:\n",
    "            matched[q] = (hid, hdr, seq); found = True; break\n",
    "    if not found:\n",
    "        unmatched.append(q)\n",
    "\n",
    "print(f\"\\nMatched {len(matched)} / {len(ids)} high-conf IDs by heuristics. Unmatched: {len(unmatched)}\")\n",
    "\n",
    "# ---------- save mapping & matched FASTA ----------\n",
    "mapped_rows = []\n",
    "for q, (hid, hdr, seq) in matched.items():\n",
    "    mapped_rows.append({\"requested_id\": q, \"matched_header_id\": hid, \"matched_header\": hdr, \"seq_len\": len(seq)})\n",
    "mapping_df = pd.DataFrame(mapped_rows)\n",
    "mapping_out = extracted / \"high_conf_unassigned_matched_mapping.csv\"\n",
    "mapping_df.to_csv(mapping_out, index=False)\n",
    "print(\"Wrote mapping CSV ->\", mapping_out)\n",
    "\n",
    "if len(matched) > 0:\n",
    "    out_fa = extracted / \"high_conf_unassigned_seqs_matched.fasta\"\n",
    "    with open(out_fa, \"w\", encoding=\"utf8\") as fh:\n",
    "        for q, (hid, hdr, seq) in matched.items():\n",
    "            fh.write(f\">{q} matched_header={hid} {hdr}\\n\")\n",
    "            for i in range(0, len(seq), 80):\n",
    "                fh.write(seq[i:i+80] + \"\\n\")\n",
    "    print(\"Wrote matched FASTA ->\", out_fa)\n",
    "else:\n",
    "    print(\"No matched FASTA written (no matches).\")\n",
    "\n",
    "unmatched_out = extracted / \"high_conf_unassigned_unmatched_ids.txt\"\n",
    "with open(unmatched_out, \"w\", encoding=\"utf8\") as fh:\n",
    "    for q in unmatched:\n",
    "        fh.write(q + \"\\n\")\n",
    "print(\"Wrote unmatched ID list ->\", unmatched_out)\n",
    "\n",
    "# ---------- (OPTIONAL) conservative forced assignment of matched IDs back to species_pred_label ----------\n",
    "if force_assign:\n",
    "    print(\"\\nFORCE_ASSIGN=True -> attempting conservative relabeling of matched UNASSIGNED reads.\")\n",
    "    # load predictions CSV\n",
    "    preds = pd.read_csv(pred_file)\n",
    "    # detect columns\n",
    "    cols_lower = {c.lower(): c for c in preds.columns}\n",
    "    species_col = next((cols_lower[k] for k in (\"species_pred_label\",\"species_label\",\"species_pred\",\"species\") if k in cols_lower), None)\n",
    "    conf_col = next((cols_lower[k] for k in (\"species_pred_conf\",\"species_conf\",\"species_prob\",\"pred_conf\") if k in cols_lower), None)\n",
    "    novel_col = next((cols_lower[k] for k in (\"species_novel_component\",\"novel_component\",\"novel_score\") if k in cols_lower), None)\n",
    "    id_col = next((cols_lower[k] for k in (\"id\",\"accession\",\"seqid\",\"read_id\",\"readid\",\"global_index\") if k in cols_lower), None)\n",
    "    if species_col is None:\n",
    "        raise SystemExit(\"Could not detect species prediction column in predictions CSV.\")\n",
    "\n",
    "    preds[\"_species_norm\"] = preds[species_col].astype(str).fillna(\"UNASSIGNED\").apply(lambda s: \" \".join(str(s).split()))\n",
    "    if conf_col:\n",
    "        preds[\"_conf_num\"] = pd.to_numeric(preds[conf_col], errors=\"coerce\").fillna(0.0)\n",
    "    else:\n",
    "        preds[\"_conf_num\"] = 0.0\n",
    "    if novel_col:\n",
    "        preds[\"_novel_comp\"] = pd.to_numeric(preds[novel_col], errors=\"coerce\").fillna(1.0)\n",
    "    else:\n",
    "        preds[\"_novel_comp\"] = 1.0\n",
    "\n",
    "    # build lookup of matched header -> requested id(s)\n",
    "    matched_header_to_q = {}\n",
    "    for q,(hid,hdr,seq) in matched.items():\n",
    "        matched_header_to_q.setdefault(hid, []).append(q)\n",
    "\n",
    "    # now find rows in preds whose id matches one of the requested ids (we must infer id_col)\n",
    "    if id_col:\n",
    "        ids_in_preds = preds[id_col].astype(str).tolist()\n",
    "        # build mapping from requested id to row indices\n",
    "        q_to_rows = {}\n",
    "        for q in matched.keys():\n",
    "            # find rows where id == q (exact) or id contains q token\n",
    "            mask_eq = preds[id_col].astype(str) == str(q)\n",
    "            mask_cont = preds[id_col].astype(str).str.contains(str(q), na=False)\n",
    "            rows_idx = preds.index[mask_eq | mask_cont].tolist()\n",
    "            if rows_idx:\n",
    "                q_to_rows[q] = rows_idx\n",
    "    else:\n",
    "        # try matching by global_index or fallback to nothing\n",
    "        if \"global_index\" in preds.columns:\n",
    "            q_to_rows = {}\n",
    "            for q in matched.keys():\n",
    "                rows_idx = preds.index[preds[\"global_index\"].astype(str) == str(q)].tolist()\n",
    "                if rows_idx: q_to_rows[q] = rows_idx\n",
    "        else:\n",
    "            raise SystemExit(\"Could not find an 'id' column in predictions CSV to align IDs to rows. For safety we will not force-assign.\")\n",
    "\n",
    "    # assemble list of candidate (row index) to new_species (predicted species)\n",
    "    to_assign = []\n",
    "    for q, rows_idx in q_to_rows.items():\n",
    "        # new species label = the predicted species in preds row (species_col) or the matched header if needed\n",
    "        for ri in rows_idx:\n",
    "            curr = preds.loc[ri, \"_species_norm\"]\n",
    "            # only consider if currently UNASSIGNED\n",
    "            if str(curr).upper() == \"UNASSIGNED\":\n",
    "                conf_val = float(preds.loc[ri, \"_conf_num\"])\n",
    "                novel_val = float(preds.loc[ri, \"_novel_comp\"])\n",
    "                # conservative criteria\n",
    "                if conf_val >= assign_conf_threshold and novel_val <= novel_comp_max:\n",
    "                    new_label = preds.loc[ri, species_col]\n",
    "                    to_assign.append((ri, new_label, conf_val, novel_val, q))\n",
    "\n",
    "    print(f\"Candidates meeting conservative criteria to assign: {len(to_assign)}\")\n",
    "    if len(to_assign) == 0:\n",
    "        print(\"No safe candidates met the thresholds (no changes made).\")\n",
    "    else:\n",
    "        # perform assignment on copy and save species counts\n",
    "        preds_forced = preds.copy()\n",
    "        for ri, new_label, conf_val, novel_val, q in to_assign:\n",
    "            preds_forced.at[ri, \"_species_norm\"] = str(new_label)\n",
    "        # compute species counts and normalized abundances\n",
    "        counts = preds_forced[\"_species_norm\"].value_counts().reset_index()\n",
    "        counts.columns = [\"species\",\"count\"]\n",
    "        counts[\"rel\"] = counts[\"count\"] / counts[\"count\"].sum()\n",
    "        out_forced = extracted / \"abundance_forced_assignments.csv\"\n",
    "        counts.to_csv(out_forced, index=False)\n",
    "        print(\"Saved forced-assignment species counts ->\", out_forced)\n",
    "\n",
    "        # also save a simple reconciled-style normalized file\n",
    "        out_recon = extracted / \"abundance_forced_assignments_reconciled.csv\"\n",
    "        counts.to_csv(out_recon, index=False)\n",
    "        print(\"Saved reconciled-style normalized abundance ->\", out_recon)\n",
    "\n",
    "# ---------- final message ----------\n",
    "print(\"\\nDone. Files written to:\", extracted)\n",
    "print(\" - high_conf_unassigned_matched_mapping.csv\")\n",
    "print(\" - high_conf_unassigned_seqs_matched.fasta   (if any matches)\")\n",
    "print(\" - high_conf_unassigned_unmatched_ids.txt\")\n",
    "print(\" - (if force_assign=True) abundance_forced_assignments.csv and abundance_forced_assignments_reconciled.csv\")\n",
    "\n",
    "print(\"\\nNext recommended actions (pick one):\")\n",
    "print(\"  1) Inspect 'high_conf_unassigned_matched_mapping.csv' to verify matches (VERY IMPORTANT).\")\n",
    "print(\"  2) Run BLAST locally on 'high_conf_unassigned_seqs_matched.fasta' or on unmatched IDs' sequences to confirm species identity.\")\n",
    "print(\"  3) If BLAST confirms match and you trust it, re-run with force_assign=True to absorb matched high-confidence UNASSIGNED into species counts.\")\n",
    "print(\"\\nIf you want, I can now (A) provide the local BLAST command to run against NCBI/your DB, (B) flip force_assign=True and re-run the assignment heuristics here, or (C) produce a small report comparing before/after abundances. Tell me which and I'll give the exact cell/commands.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "43fcb3e9-b30e-4913-90dc-f84f8d97e40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using extracted folder: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\n",
      "Read 86 high-confidence UNASSIGNED IDs from high_conf_unassigned_ids.txt\n",
      "Using FASTA: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\its_combined.fasta\n",
      "Parsed FASTA headers: 699\n",
      "\n",
      "--- Preview high-conf IDs (first 80) ---\n",
      "  1. JBPZNU010001377.1\n",
      "  2. LC876572.1\n",
      "  3. PX277059.1\n",
      "  4. LC876609.1\n",
      "  5. PX273803.1\n",
      "  6. JF836109.1\n",
      "  7. JBPZNU010001350.1\n",
      "  8. JBPZNU010001331.1\n",
      "  9. JF836111.1\n",
      " 10. PX278865.1\n",
      " 11. PX277079.1\n",
      " 12. PX278850.1\n",
      " 13. PX278853.1\n",
      " 14. LC876580.1\n",
      " 15. LC876528.1\n",
      " 16. LC876609.1\n",
      " 17. LC876593.1\n",
      " 18. JBPZNU010001303.1\n",
      " 19. LC876579.1\n",
      " 20. LC876562.1\n",
      " 21. LC876572.1\n",
      " 22. LC876528.1\n",
      " 23. LC876567.1\n",
      " 24. LC876579.1\n",
      " 25. JF836094.1\n",
      " 26. JBPZNU010001397.1\n",
      " 27. LC876593.1\n",
      " 28. PX278864.1\n",
      " 29. JBPZNU010001375.1\n",
      " 30. JF836094.1\n",
      " 31. LC876521.1\n",
      " 32. OQ241938.1\n",
      " 33. LC876568.1\n",
      " 34. XR_013090605.1\n",
      " 35. LC876580.1\n",
      " 36. JF836095.1\n",
      " 37. JBPZNU010001366.1\n",
      " 38. LC876553.1\n",
      " 39. JBPZNU010001353.1\n",
      " 40. JBPZNU010001377.1\n",
      " 41. LC876519.1\n",
      " 42. LC876630.1\n",
      " 43. LC876630.1\n",
      " 44. LC876619.1\n",
      " 45. PX278852.1\n",
      " 46. LC876566.1\n",
      " 47. JF836117.1\n",
      " 48. LC876566.1\n",
      " 49. LC876553.1\n",
      " 50. PX278084.1\n",
      " 51. JBPZNU010001364.1\n",
      " 52. JBPZNU010001353.1\n",
      " 53. LC876619.1\n",
      " 54. JBPZNU010001397.1\n",
      " 55. JBPZNU010001339.1\n",
      " 56. JBPZNU010001350.1\n",
      " 57. LC876623.1\n",
      " 58. PQ523743.1\n",
      " 59. JBPZNU010001386.1\n",
      " 60. LC876519.1\n",
      " 61. JBPZNU010001386.1\n",
      " 62. LC876601.1\n",
      " 63. JBPZNU010001345.1\n",
      " 64. JF836095.1\n",
      " 65. JBPZNU010001339.1\n",
      " 66. LC876567.1\n",
      " 67. JBPZNU010001331.1\n",
      " 68. JBPZNU010001328.1\n",
      " 69. LC876521.1\n",
      " 70. JF836104.1\n",
      " 71. JF836109.1\n",
      " 72. LC876568.1\n",
      " 73. JF836104.1\n",
      " 74. JBPZNU010001328.1\n",
      " 75. JBPZNU010001303.1\n",
      " 76. JF836117.1\n",
      " 77. JBPZNU010001366.1\n",
      " 78. LC876623.1\n",
      " 79. JBPZNU010001375.1\n",
      " 80. LC876563.1\n",
      "\n",
      "--- Preview FASTA header keys (first 80) ---\n",
      "  1. LC890403.1\n",
      "  2. PX285876.1\n",
      "  3. PX285871.1\n",
      "  4. PX285870.1\n",
      "  5. PX285869.1\n",
      "  6. PX285868.1\n",
      "  7. PX285867.1\n",
      "  8. PX285866.1\n",
      "  9. PX285865.1\n",
      " 10. PX285864.1\n",
      " 11. PX285863.1\n",
      " 12. PX285862.1\n",
      " 13. PX285861.1\n",
      " 14. PX285860.1\n",
      " 15. PX285859.1\n",
      " 16. PX285858.1\n",
      " 17. PX285857.1\n",
      " 18. PX285856.1\n",
      " 19. PX285855.1\n",
      " 20. PX285854.1\n",
      " 21. PX285853.1\n",
      " 22. PX285852.1\n",
      " 23. PX285851.1\n",
      " 24. PX285850.1\n",
      " 25. PX285849.1\n",
      " 26. PX285848.1\n",
      " 27. PX285846.1\n",
      " 28. PX285845.1\n",
      " 29. PX285844.1\n",
      " 30. PX285843.1\n",
      " 31. PX285841.1\n",
      " 32. PX285840.1\n",
      " 33. PX285838.1\n",
      " 34. PX285785.1\n",
      " 35. PX285783.1\n",
      " 36. PX285782.1\n",
      " 37. PX285781.1\n",
      " 38. PX285774.1\n",
      " 39. PX285769.1\n",
      " 40. PX113199.1\n",
      " 41. PV975055.1\n",
      " 42. PV975054.1\n",
      " 43. PV975053.1\n",
      " 44. PV975052.1\n",
      " 45. PV975051.1\n",
      " 46. PV650852.1\n",
      " 47. PV650851.1\n",
      " 48. PV650850.1\n",
      " 49. PV650849.1\n",
      " 50. PV650848.1\n",
      " 51. PV650847.1\n",
      " 52. PV650846.1\n",
      " 53. PV650845.1\n",
      " 54. PQ834537.1\n",
      " 55. PQ728411.1\n",
      " 56. PQ139111.1\n",
      " 57. PQ139110.1\n",
      " 58. PQ139109.1\n",
      " 59. PQ139108.1\n",
      " 60. PQ139107.1\n",
      " 61. PQ139106.1\n",
      " 62. PQ139105.1\n",
      " 63. PQ139104.1\n",
      " 64. PQ139103.1\n",
      " 65. OK104033.1\n",
      " 66. OK104032.1\n",
      " 67. OK104031.1\n",
      " 68. OK104030.1\n",
      " 69. OK104029.1\n",
      " 70. OK104028.1\n",
      " 71. OK104027.1\n",
      " 72. OK104026.1\n",
      " 73. OK104025.1\n",
      " 74. OK104024.1\n",
      " 75. OK104023.1\n",
      " 76. OK104022.1\n",
      " 77. OK104021.1\n",
      " 78. PX308181.1\n",
      " 79. PX308180.1\n",
      " 80. PV919998.1\n",
      "\n",
      "Matched 0 / 86 high-conf IDs by heuristics. Unmatched: 86\n",
      "Wrote mapping CSV -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\high_conf_unassigned_matched_mapping.csv\n",
      "No matched FASTA written (no matches).\n",
      "Wrote unmatched ID list -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\high_conf_unassigned_unmatched_ids.txt\n",
      "\n",
      "Done. Files written to: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\n",
      " - high_conf_unassigned_matched_mapping.csv\n",
      " - high_conf_unassigned_seqs_matched.fasta   (if any matches)\n",
      " - high_conf_unassigned_unmatched_ids.txt\n",
      " - (if force_assign=True) abundance_forced_assignments.csv and abundance_forced_assignments_reconciled.csv\n",
      "\n",
      "Next recommended actions (pick one):\n",
      "  1) Inspect 'high_conf_unassigned_matched_mapping.csv' to verify matches (VERY IMPORTANT).\n",
      "  2) Run BLAST locally on 'high_conf_unassigned_seqs_matched.fasta' or on unmatched IDs' sequences to confirm species identity.\n",
      "  3) If BLAST confirms match and you trust it, re-run with force_assign=True to absorb matched high-confidence UNASSIGNED into species counts.\n",
      "\n",
      "If you want, I can now (A) provide the local BLAST command to run against NCBI/your DB, (B) flip force_assign=True and re-run the assignment heuristics here, or (C) produce a small report comparing before/after abundances. Tell me which and I'll give the exact cell/commands.\n"
     ]
    }
   ],
   "source": [
    "# Cell: inspect FASTA headers vs high-conf UNASSIGNED IDs, attempt robust matching,\n",
    "# and (optionally) perform a conservative forced-assignment of matched UNASSIGNED reads.\n",
    "#\n",
    "# SAFE DEFAULTS: force_assign=False. Set force_assign=True only AFTER you inspect the mapping CSV\n",
    "# and understand that forcing will reassign some UNASSIGNED reads to predicted species.\n",
    "#\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------- USER FLAGS (edit only if you want to force assignments) ----------\n",
    "force_assign = False             # default False -> NO label changes\n",
    "assign_conf_threshold = 0.60     # if forcing: require species_pred_conf >= this\n",
    "novel_comp_max = 0.20            # if forcing: require species_novel_component <= this (if column present)\n",
    "high_conf_cutoff = 0.50          # high-confidence threshold used previously\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# ---------- find extracted folder (common locations) ----------\n",
    "candidates = [\n",
    "    Path.cwd() / \"sih\" / \"ncbi_blast_db\" / \"extracted\",\n",
    "    Path.cwd() / \"ncbi_blast_db\" / \"extracted\",\n",
    "    Path.cwd() / \"extracted\",\n",
    "    Path.cwd(),\n",
    "    Path(r\"C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\"),\n",
    "    Path(r\"C:\\Users\\Srijit\\OneDrive\\Desktop\\sihtaxa\\sihabundance\\ncbi_blast_db\\extracted\"),\n",
    "    Path(r\"C:\\Users\\HP\\sihabundance\\ncbi_blast_db\\extracted\"),\n",
    "]\n",
    "extracted = None\n",
    "for p in candidates:\n",
    "    if p and p.exists() and p.is_dir():\n",
    "        # verify predictions present\n",
    "        if (p / \"predictions_with_uncertainty.csv\").exists() or (p / \"predictions.csv\").exists() or (p / \"predictions_with_uncertainty.csv\").exists():\n",
    "            extracted = p.resolve()\n",
    "            break\n",
    "\n",
    "# fallback: try to find the id file recursively under cwd\n",
    "if extracted is None:\n",
    "    for f in Path.cwd().rglob(\"high_conf_unassigned_ids.txt\"):\n",
    "        extracted = f.parent.resolve(); break\n",
    "\n",
    "if extracted is None:\n",
    "    raise SystemExit(\"Could not find the 'extracted' folder containing prediction outputs. Run the earlier cells in this notebook or set the path here.\")\n",
    "\n",
    "print(\"Using extracted folder:\", extracted)\n",
    "\n",
    "# ---------- required files ----------\n",
    "preds_candidates = [extracted / \"predictions_with_uncertainty.csv\", extracted / \"predictions.csv\"]\n",
    "pred_file = next((p for p in preds_candidates if p.exists()), None)\n",
    "if pred_file is None:\n",
    "    raise SystemExit(\"predictions CSV not found in extracted/. Run the classification cells first.\")\n",
    "\n",
    "ids_file = extracted / \"high_conf_unassigned_ids.txt\"\n",
    "rows_file = extracted / \"high_conf_unassigned_rows.csv\"\n",
    "# if the simple id file doesn't exist, try to extract ids from the rows CSV\n",
    "if not ids_file.exists():\n",
    "    if rows_file.exists():\n",
    "        df_rows = pd.read_csv(rows_file)\n",
    "        id_col = next((c for c in df_rows.columns if c.lower() in (\"id\",\"accession\",\"seqid\",\"read_id\",\"readid\",\"accession_id\")), df_rows.columns[0])\n",
    "        ids = df_rows[id_col].astype(str).tolist()\n",
    "        print(f\"Extracted {len(ids)} IDs from {rows_file.name} using column '{id_col}'\")\n",
    "    else:\n",
    "        raise SystemExit(\"high_conf_unassigned_ids.txt and high_conf_unassigned_rows.csv not found. Run the previous 'UNASSIGNED handling' cell first.\")\n",
    "else:\n",
    "    with open(ids_file, \"r\", encoding=\"utf8\", errors=\"replace\") as fh:\n",
    "        ids = [line.strip() for line in fh if line.strip()]\n",
    "    print(f\"Read {len(ids)} high-confidence UNASSIGNED IDs from {ids_file.name}\")\n",
    "\n",
    "# ---------- search for FASTA ----------\n",
    "fasta_names = [\"its_combined.fasta\",\"predictions_source_sequences.fasta\",\"source_sequences.fasta\",\"sequences.fasta\",\"input.fasta\",\"reads.fasta\"]\n",
    "fasta_path = None\n",
    "for n in fasta_names:\n",
    "    p = extracted / n\n",
    "    if p.exists():\n",
    "        fasta_path = p; break\n",
    "if fasta_path is None:\n",
    "    # pick any fasta-like file in folder\n",
    "    for ext in (\"*.fasta\",\"*.fa\",\"*.fna\",\"*.ffn\"):\n",
    "        found = list(extracted.rglob(ext))\n",
    "        if found:\n",
    "            fasta_path = found[0]; break\n",
    "\n",
    "if fasta_path is None:\n",
    "    print(\"No FASTA found in extracted/. Place the FASTA with your source sequences in that folder (common name: its_combined.fasta) and re-run this cell.\")\n",
    "else:\n",
    "    print(\"Using FASTA:\", fasta_path)\n",
    "\n",
    "# ---------- parse FASTA headers (simple parser) ----------\n",
    "def parse_fasta_headers(path):\n",
    "    seqs = {}\n",
    "    with open(path, \"r\", encoding=\"utf8\", errors=\"replace\") as fh:\n",
    "        header = None; seq_lines=[]\n",
    "        for line in fh:\n",
    "            line = line.rstrip(\"\\n\\r\")\n",
    "            if not line: continue\n",
    "            if line.startswith(\">\"):\n",
    "                if header is not None:\n",
    "                    seqs[header.split()[0]] = (header, \"\".join(seq_lines))\n",
    "                header = line[1:].strip()\n",
    "                seq_lines = []\n",
    "            else:\n",
    "                seq_lines.append(line.strip())\n",
    "        if header is not None:\n",
    "            seqs[header.split()[0]] = (header, \"\".join(seq_lines))\n",
    "    return seqs\n",
    "\n",
    "seqs = parse_fasta_headers(fasta_path)\n",
    "print(\"Parsed FASTA headers:\", len(seqs))\n",
    "\n",
    "# Preview first 80 IDs & first 80 FASTA header keys\n",
    "print(\"\\n--- Preview high-conf IDs (first 80) ---\")\n",
    "for i, x in enumerate(ids[:80], 1):\n",
    "    print(f\"{i:3d}. {x}\")\n",
    "print(\"\\n--- Preview FASTA header keys (first 80) ---\")\n",
    "header_keys = list(seqs.keys())\n",
    "for i, h in enumerate(header_keys[:80], 1):\n",
    "    print(f\"{i:3d}. {h}\")\n",
    "\n",
    "# ---------- canonicalization and matching heuristics ----------\n",
    "def canonical_forms(x):\n",
    "    x = str(x).strip()\n",
    "    forms = [x]\n",
    "    # strip trailing version like .1 .2\n",
    "    nov = re.sub(r'\\.\\d+$', '', x)\n",
    "    if nov != x: forms.append(nov)\n",
    "    # split on DB pipe tokens\n",
    "    if \"|\" in x:\n",
    "        toks = [t.strip() for t in re.split(r'[|/]', x) if t.strip()]\n",
    "        forms.extend(toks)\n",
    "    # drop common prefixes like 'ref|', 'gb|' etc.\n",
    "    forms2 = []\n",
    "    for f in list(forms):\n",
    "        f2 = re.sub(r'^(?:ref|gi|gb|emb|dbj|accession)[\\|:]', '', f, flags=re.IGNORECASE)\n",
    "        forms2.append(f2)\n",
    "    # lower-case variants\n",
    "    out = []\n",
    "    for s in forms + forms2:\n",
    "        if s and s not in out:\n",
    "            out.append(s)\n",
    "        s_low = s.lower() if s else s\n",
    "        if s_low and s_low not in out:\n",
    "            out.append(s_low)\n",
    "    return out\n",
    "\n",
    "# build header lookup\n",
    "header_map = {}\n",
    "for hid, (hdr, seq) in seqs.items():\n",
    "    for f in canonical_forms(hid):\n",
    "        header_map[f] = (hid, hdr, seq)\n",
    "    header_map[hdr] = (hid, hdr, seq)\n",
    "    header_map[hdr.lower()] = (hid, hdr, seq)\n",
    "\n",
    "matched = {}\n",
    "unmatched = []\n",
    "for q in ids:\n",
    "    found = False\n",
    "    for qf in canonical_forms(q):\n",
    "        if qf in header_map:\n",
    "            matched[q] = header_map[qf]; found = True; break\n",
    "    if found: continue\n",
    "    # fallback: substring match in header strings (strip version first)\n",
    "    q_nov = re.sub(r'\\.\\d+$', '', q)\n",
    "    for hid, (hdr, seq) in seqs.items():\n",
    "        if q in hid or q in hdr or q_nov in hid or q_nov in hdr:\n",
    "            matched[q] = (hid, hdr, seq); found = True; break\n",
    "    if not found:\n",
    "        unmatched.append(q)\n",
    "\n",
    "print(f\"\\nMatched {len(matched)} / {len(ids)} high-conf IDs by heuristics. Unmatched: {len(unmatched)}\")\n",
    "\n",
    "# ---------- save mapping & matched FASTA ----------\n",
    "mapped_rows = []\n",
    "for q, (hid, hdr, seq) in matched.items():\n",
    "    mapped_rows.append({\"requested_id\": q, \"matched_header_id\": hid, \"matched_header\": hdr, \"seq_len\": len(seq)})\n",
    "mapping_df = pd.DataFrame(mapped_rows)\n",
    "mapping_out = extracted / \"high_conf_unassigned_matched_mapping.csv\"\n",
    "mapping_df.to_csv(mapping_out, index=False)\n",
    "print(\"Wrote mapping CSV ->\", mapping_out)\n",
    "\n",
    "if len(matched) > 0:\n",
    "    out_fa = extracted / \"high_conf_unassigned_seqs_matched.fasta\"\n",
    "    with open(out_fa, \"w\", encoding=\"utf8\") as fh:\n",
    "        for q, (hid, hdr, seq) in matched.items():\n",
    "            fh.write(f\">{q} matched_header={hid} {hdr}\\n\")\n",
    "            for i in range(0, len(seq), 80):\n",
    "                fh.write(seq[i:i+80] + \"\\n\")\n",
    "    print(\"Wrote matched FASTA ->\", out_fa)\n",
    "else:\n",
    "    print(\"No matched FASTA written (no matches).\")\n",
    "\n",
    "unmatched_out = extracted / \"high_conf_unassigned_unmatched_ids.txt\"\n",
    "with open(unmatched_out, \"w\", encoding=\"utf8\") as fh:\n",
    "    for q in unmatched:\n",
    "        fh.write(q + \"\\n\")\n",
    "print(\"Wrote unmatched ID list ->\", unmatched_out)\n",
    "\n",
    "# ---------- (OPTIONAL) conservative forced assignment of matched IDs back to species_pred_label ----------\n",
    "if force_assign:\n",
    "    print(\"\\nFORCE_ASSIGN=True -> attempting conservative relabeling of matched UNASSIGNED reads.\")\n",
    "    # load predictions CSV\n",
    "    preds = pd.read_csv(pred_file)\n",
    "    # detect columns\n",
    "    cols_lower = {c.lower(): c for c in preds.columns}\n",
    "    species_col = next((cols_lower[k] for k in (\"species_pred_label\",\"species_label\",\"species_pred\",\"species\") if k in cols_lower), None)\n",
    "    conf_col = next((cols_lower[k] for k in (\"species_pred_conf\",\"species_conf\",\"species_prob\",\"pred_conf\") if k in cols_lower), None)\n",
    "    novel_col = next((cols_lower[k] for k in (\"species_novel_component\",\"novel_component\",\"novel_score\") if k in cols_lower), None)\n",
    "    id_col = next((cols_lower[k] for k in (\"id\",\"accession\",\"seqid\",\"read_id\",\"readid\",\"global_index\") if k in cols_lower), None)\n",
    "    if species_col is None:\n",
    "        raise SystemExit(\"Could not detect species prediction column in predictions CSV.\")\n",
    "\n",
    "    preds[\"_species_norm\"] = preds[species_col].astype(str).fillna(\"UNASSIGNED\").apply(lambda s: \" \".join(str(s).split()))\n",
    "    if conf_col:\n",
    "        preds[\"_conf_num\"] = pd.to_numeric(preds[conf_col], errors=\"coerce\").fillna(0.0)\n",
    "    else:\n",
    "        preds[\"_conf_num\"] = 0.0\n",
    "    if novel_col:\n",
    "        preds[\"_novel_comp\"] = pd.to_numeric(preds[novel_col], errors=\"coerce\").fillna(1.0)\n",
    "    else:\n",
    "        preds[\"_novel_comp\"] = 1.0\n",
    "\n",
    "    # build lookup of matched header -> requested id(s)\n",
    "    matched_header_to_q = {}\n",
    "    for q,(hid,hdr,seq) in matched.items():\n",
    "        matched_header_to_q.setdefault(hid, []).append(q)\n",
    "\n",
    "    # now find rows in preds whose id matches one of the requested ids (we must infer id_col)\n",
    "    if id_col:\n",
    "        ids_in_preds = preds[id_col].astype(str).tolist()\n",
    "        # build mapping from requested id to row indices\n",
    "        q_to_rows = {}\n",
    "        for q in matched.keys():\n",
    "            # find rows where id == q (exact) or id contains q token\n",
    "            mask_eq = preds[id_col].astype(str) == str(q)\n",
    "            mask_cont = preds[id_col].astype(str).str.contains(str(q), na=False)\n",
    "            rows_idx = preds.index[mask_eq | mask_cont].tolist()\n",
    "            if rows_idx:\n",
    "                q_to_rows[q] = rows_idx\n",
    "    else:\n",
    "        # try matching by global_index or fallback to nothing\n",
    "        if \"global_index\" in preds.columns:\n",
    "            q_to_rows = {}\n",
    "            for q in matched.keys():\n",
    "                rows_idx = preds.index[preds[\"global_index\"].astype(str) == str(q)].tolist()\n",
    "                if rows_idx: q_to_rows[q] = rows_idx\n",
    "        else:\n",
    "            raise SystemExit(\"Could not find an 'id' column in predictions CSV to align IDs to rows. For safety we will not force-assign.\")\n",
    "\n",
    "    # assemble list of candidate (row index) to new_species (predicted species)\n",
    "    to_assign = []\n",
    "    for q, rows_idx in q_to_rows.items():\n",
    "        # new species label = the predicted species in preds row (species_col) or the matched header if needed\n",
    "        for ri in rows_idx:\n",
    "            curr = preds.loc[ri, \"_species_norm\"]\n",
    "            # only consider if currently UNASSIGNED\n",
    "            if str(curr).upper() == \"UNASSIGNED\":\n",
    "                conf_val = float(preds.loc[ri, \"_conf_num\"])\n",
    "                novel_val = float(preds.loc[ri, \"_novel_comp\"])\n",
    "                # conservative criteria\n",
    "                if conf_val >= assign_conf_threshold and novel_val <= novel_comp_max:\n",
    "                    new_label = preds.loc[ri, species_col]\n",
    "                    to_assign.append((ri, new_label, conf_val, novel_val, q))\n",
    "\n",
    "    print(f\"Candidates meeting conservative criteria to assign: {len(to_assign)}\")\n",
    "    if len(to_assign) == 0:\n",
    "        print(\"No safe candidates met the thresholds (no changes made).\")\n",
    "    else:\n",
    "        # perform assignment on copy and save species counts\n",
    "        preds_forced = preds.copy()\n",
    "        for ri, new_label, conf_val, novel_val, q in to_assign:\n",
    "            preds_forced.at[ri, \"_species_norm\"] = str(new_label)\n",
    "        # compute species counts and normalized abundances\n",
    "        counts = preds_forced[\"_species_norm\"].value_counts().reset_index()\n",
    "        counts.columns = [\"species\",\"count\"]\n",
    "        counts[\"rel\"] = counts[\"count\"] / counts[\"count\"].sum()\n",
    "        out_forced = extracted / \"abundance_forced_assignments.csv\"\n",
    "        counts.to_csv(out_forced, index=False)\n",
    "        print(\"Saved forced-assignment species counts ->\", out_forced)\n",
    "\n",
    "        # also save a simple reconciled-style normalized file\n",
    "        out_recon = extracted / \"abundance_forced_assignments_reconciled.csv\"\n",
    "        counts.to_csv(out_recon, index=False)\n",
    "        print(\"Saved reconciled-style normalized abundance ->\", out_recon)\n",
    "\n",
    "# ---------- final message ----------\n",
    "print(\"\\nDone. Files written to:\", extracted)\n",
    "print(\" - high_conf_unassigned_matched_mapping.csv\")\n",
    "print(\" - high_conf_unassigned_seqs_matched.fasta   (if any matches)\")\n",
    "print(\" - high_conf_unassigned_unmatched_ids.txt\")\n",
    "print(\" - (if force_assign=True) abundance_forced_assignments.csv and abundance_forced_assignments_reconciled.csv\")\n",
    "\n",
    "print(\"\\nNext recommended actions (pick one):\")\n",
    "print(\"  1) Inspect 'high_conf_unassigned_matched_mapping.csv' to verify matches (VERY IMPORTANT).\")\n",
    "print(\"  2) Run BLAST locally on 'high_conf_unassigned_seqs_matched.fasta' or on unmatched IDs' sequences to confirm species identity.\")\n",
    "print(\"  3) If BLAST confirms match and you trust it, re-run with force_assign=True to absorb matched high-confidence UNASSIGNED into species counts.\")\n",
    "print(\"\\nIf you want, I can now (A) provide the local BLAST command to run against NCBI/your DB, (B) flip force_assign=True and re-run the assignment heuristics here, or (C) produce a small report comparing before/after abundances. Tell me which and I'll give the exact cell/commands.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "36f78e8c-abe4-46ab-8339-0d4779471bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using extracted folder: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\n",
      "Read 86 IDs from high_conf_unassigned_ids.txt\n",
      "Found FASTA: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\its_combined.fasta\n",
      "Parsed FASTA headers: 699\n",
      "Matched 0 / 86 IDs. Unmatched: 86\n",
      "Wrote unmatched ID list -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\high_conf_unassigned_unmatched_ids.txt\n",
      "No direct header matches: copied the full FASTA for BLAST -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\all_unassigned_for_blast.fasta\n",
      "\n",
      "Files written in: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: Export unmatched / high-conf UNASSIGNED sequences for BLAST\n",
    "# Safe by default: just writes FASTA(s) and mapping files. Do not change labels here.\n",
    "from pathlib import Path\n",
    "import re, csv\n",
    "\n",
    "# --- user-tweakable (usually no need) ---\n",
    "EXTRACTED_CANDIDATES = [\n",
    "    Path.cwd() / \"sih\" / \"ncbi_blast_db\" / \"extracted\",\n",
    "    Path.cwd() / \"ncbi_blast_db\" / \"extracted\",\n",
    "    Path.cwd() / \"extracted\",\n",
    "    Path(r\"C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\"),\n",
    "    Path(r\"C:\\Users\\Srijit\\OneDrive\\Desktop\\sihtaxa\\sihabundance\\ncbi_blast_db\\extracted\"),\n",
    "]\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "def find_extracted():\n",
    "    for p in EXTRACTED_CANDIDATES:\n",
    "        if p.exists() and p.is_dir():\n",
    "            # need predictions file present to be confident it's the right folder\n",
    "            if (p/\"predictions_with_uncertainty.csv\").exists() or (p/\"predictions.csv\").exists():\n",
    "                return p.resolve()\n",
    "    # fallback: find any folder with high_conf_unassigned_ids.txt\n",
    "    for f in Path.cwd().rglob(\"high_conf_unassigned_ids.txt\"):\n",
    "        return f.parent.resolve()\n",
    "    # fallback: any folder named extracted under cwd\n",
    "    for f in Path.cwd().rglob(\"**/extracted\"):\n",
    "        if f.is_dir(): return f.resolve()\n",
    "    raise SystemExit(\"Could not locate extracted/ folder. Place previous outputs in an extracted/ folder and re-run.\")\n",
    "\n",
    "extracted = find_extracted()\n",
    "print(\"Using extracted folder:\", extracted)\n",
    "\n",
    "# read high-conf ids\n",
    "ids_path = extracted / \"high_conf_unassigned_ids.txt\"\n",
    "rows_path = extracted / \"high_conf_unassigned_rows.csv\"\n",
    "ids = []\n",
    "if ids_path.exists():\n",
    "    with open(ids_path, \"r\", encoding=\"utf8\", errors=\"replace\") as fh:\n",
    "        ids = [line.strip() for line in fh if line.strip()]\n",
    "    print(f\"Read {len(ids)} IDs from {ids_path.name}\")\n",
    "elif rows_path.exists():\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(rows_path)\n",
    "    # choose a likely id column\n",
    "    id_col = next((c for c in df.columns if c.lower() in (\"id\",\"accession\",\"seqid\",\"read_id\",\"readid\")), df.columns[0])\n",
    "    ids = df[id_col].astype(str).tolist()\n",
    "    print(f\"Extracted {len(ids)} IDs from {rows_path.name} using column '{id_col}'\")\n",
    "else:\n",
    "    raise SystemExit(\"No high_conf_unassigned_ids.txt or high_conf_unassigned_rows.csv found in extracted/. Re-run earlier cell.\")\n",
    "\n",
    "# find a FASTA\n",
    "fasta = None\n",
    "for name in (\"its_combined.fasta\",\"predictions_source_sequences.fasta\",\"source_sequences.fasta\",\"sequences.fasta\",\"reads.fasta\"):\n",
    "    p = extracted / name\n",
    "    if p.exists(): fasta = p; break\n",
    "if fasta is None:\n",
    "    # pick any fasta in folder\n",
    "    fastas = list(extracted.rglob(\"*.fasta\")) + list(extracted.rglob(\"*.fa\"))\n",
    "    if fastas:\n",
    "        fasta = fastas[0]\n",
    "if fasta is None:\n",
    "    raise SystemExit(\"No FASTA found in extracted/. Place your source FASTA there (common name: its_combined.fasta) and re-run.\")\n",
    "\n",
    "print(\"Found FASTA:\", fasta)\n",
    "\n",
    "# simple FASTA parser\n",
    "def parse_fasta(path):\n",
    "    seqs = {}\n",
    "    with open(path, \"r\", encoding=\"utf8\", errors=\"replace\") as fh:\n",
    "        header=None; seq_lines=[]\n",
    "        for line in fh:\n",
    "            line = line.rstrip(\"\\n\\r\")\n",
    "            if not line: continue\n",
    "            if line.startswith(\">\"):\n",
    "                if header is not None:\n",
    "                    seqs[header.split()[0]] = (header, \"\".join(seq_lines))\n",
    "                header = line[1:].strip()\n",
    "                seq_lines=[]\n",
    "            else:\n",
    "                seq_lines.append(line.strip())\n",
    "        if header is not None:\n",
    "            seqs[header.split()[0]] = (header, \"\".join(seq_lines))\n",
    "    return seqs\n",
    "\n",
    "seqs = parse_fasta(fasta)\n",
    "print(\"Parsed FASTA headers:\", len(seqs))\n",
    "\n",
    "def canonical_forms(x):\n",
    "    x = str(x).strip()\n",
    "    forms = [x]\n",
    "    # strip version .1 .2\n",
    "    nov = re.sub(r'\\.\\d+$','', x)\n",
    "    if nov != x: forms.append(nov)\n",
    "    if \"|\" in x:\n",
    "        parts = re.split(r\"[|/]\", x)\n",
    "        for t in parts:\n",
    "            t = t.strip()\n",
    "            if t: forms.append(t)\n",
    "    # drop common prefixes\n",
    "    for f in list(forms):\n",
    "        f2 = re.sub(r'^(?:ref|gi|gb|emb|dbj)[\\|:]', '', f, flags=re.IGNORECASE)\n",
    "        forms.append(f2)\n",
    "    # lower-case variants\n",
    "    for f in list(forms):\n",
    "        if f.lower() not in forms: forms.append(f.lower())\n",
    "    # unique preserve order\n",
    "    out=[]; seen=set()\n",
    "    for s in forms:\n",
    "        if s and s not in seen:\n",
    "            seen.add(s); out.append(s)\n",
    "    return out\n",
    "\n",
    "# build header map\n",
    "header_map = {}\n",
    "for hid,(hdr,seq) in seqs.items():\n",
    "    for f in canonical_forms(hid):\n",
    "        header_map[f] = (hid, hdr, seq)\n",
    "    header_map[hdr] = (hid, hdr, seq)\n",
    "    header_map[hdr.lower()] = (hid, hdr, seq)\n",
    "\n",
    "matched = {}\n",
    "unmatched = []\n",
    "for q in ids:\n",
    "    found=False\n",
    "    for qf in canonical_forms(q):\n",
    "        if qf in header_map:\n",
    "            matched[q] = header_map[qf]; found=True; break\n",
    "    if not found:\n",
    "        # substring fallback\n",
    "        q_nov = re.sub(r'\\.\\d+$','', q)\n",
    "        for hid,(hdr,seq) in seqs.items():\n",
    "            if q in hid or q in hdr or q_nov in hid or q_nov in hdr:\n",
    "                matched[q] = (hid, hdr, seq); found=True; break\n",
    "    if not found:\n",
    "        unmatched.append(q)\n",
    "\n",
    "print(f\"Matched {len(matched)} / {len(ids)} IDs. Unmatched: {len(unmatched)}\")\n",
    "\n",
    "# write matched FASTA if any\n",
    "if matched:\n",
    "    out_matched = extracted / \"high_conf_unassigned_seqs_matched.fasta\"\n",
    "    with open(out_matched, \"w\", encoding=\"utf8\") as fh:\n",
    "        for q,(hid,hdr,seq) in matched.items():\n",
    "            fh.write(f\">{q} matched_header={hid} {hdr}\\n\")\n",
    "            for i in range(0, len(seq), 80):\n",
    "                fh.write(seq[i:i+80] + \"\\n\")\n",
    "    print(\"Wrote matched FASTA ->\", out_matched)\n",
    "\n",
    "# write unmatched id list\n",
    "out_unmatched = extracted / \"high_conf_unassigned_unmatched_ids.txt\"\n",
    "with open(out_unmatched, \"w\", encoding=\"utf8\") as fh:\n",
    "    for q in unmatched:\n",
    "        fh.write(q + \"\\n\")\n",
    "print(\"Wrote unmatched ID list ->\", out_unmatched)\n",
    "\n",
    "# If no matched sequences, create an ALL-UNASSIGNED FASTA to BLAST (the entire its_combined.fasta)\n",
    "if len(matched)==0:\n",
    "    out_all = extracted / \"all_unassigned_for_blast.fasta\"\n",
    "    # just copy the original FASTA so you can BLAST everything (safer than failing)\n",
    "    import shutil\n",
    "    shutil.copyfile(fasta, out_all)\n",
    "    print(\"No direct header matches: copied the full FASTA for BLAST ->\", out_all)\n",
    "else:\n",
    "    print(\"You can BLAST the matched FASTA, and optionally BLAST unmatched IDs by extracting sequences manually or BLASTing the full FASTA as fallback.\")\n",
    "\n",
    "print(\"\\nFiles written in:\", extracted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9352418d-a92b-4a2a-b471-9b8ad60df106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query FASTA to BLAST: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\all_unassigned_for_blast.fasta\n",
      "\n",
      "Local BLAST (requires a local nt/your DB):\n",
      "blastn -query \"C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\all_unassigned_for_blast.fasta\" -db nt -outfmt \"6 qseqid sseqid pident length qlen qstart qend sstart send evalue bitscore stitle\" -max_target_seqs 5 -evalue 1e-10 -num_threads 8 -out \"C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\blast_results.tsv\"\n",
      "\n",
      "Remote BLAST using NCBI (blast+ supports -remote):\n",
      "blastn -query \"C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\all_unassigned_for_blast.fasta\" -db nt -remote -outfmt \"6 qseqid sseqid pident length qlen qstart qend sstart send evalue bitscore stitle\" -max_target_seqs 5 -evalue 1e-10 -out \"C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\blast_results.tsv\"\n",
      "\n",
      "blastn not found on PATH. To run BLAST locally install BLAST+ or run the printed command with -remote (requires internet).\n",
      "\n",
      "After running BLAST, place 'blast_results.tsv' into the extracted/ folder and run Cell 3 to parse and update abundances.\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: BLAST command builder (+ optional local run if blastn is installed)\n",
    "from pathlib import Path\n",
    "import shutil, subprocess, sys\n",
    "\n",
    "extracted = None\n",
    "for p in [\n",
    "    Path.cwd() / \"sih\" / \"ncbi_blast_db\" / \"extracted\",\n",
    "    Path.cwd() / \"ncbi_blast_db\" / \"extracted\",\n",
    "    Path.cwd() / \"extracted\",\n",
    "    Path(r\"C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\")\n",
    "]:\n",
    "    if p.exists(): extracted = p; break\n",
    "if extracted is None:\n",
    "    # try recursive\n",
    "    for p in Path.cwd().rglob(\"extracted\"):\n",
    "        if p.is_dir(): extracted = p; break\n",
    "if extracted is None:\n",
    "    raise SystemExit(\"Could not locate extracted/ folder.\")\n",
    "\n",
    "# choose query FASTA\n",
    "f_matched = extracted / \"high_conf_unassigned_seqs_matched.fasta\"\n",
    "f_all = extracted / \"all_unassigned_for_blast.fasta\"\n",
    "query = f_matched if f_matched.exists() else (f_all if f_all.exists() else None)\n",
    "if query is None:\n",
    "    raise SystemExit(\"No query FASTA found (run Cell 1).\")\n",
    "\n",
    "print(\"Query FASTA to BLAST:\", query)\n",
    "\n",
    "# recommended BLAST commands (print - do not run by default)\n",
    "print(\"\\nLocal BLAST (requires a local nt/your DB):\")\n",
    "print('blastn -query \"{}\" -db nt -outfmt \"6 qseqid sseqid pident length qlen qstart qend sstart send evalue bitscore stitle\" -max_target_seqs 5 -evalue 1e-10 -num_threads 8 -out \"{}\"'.format(query, extracted/\"blast_results.tsv\"))\n",
    "\n",
    "print(\"\\nRemote BLAST using NCBI (blast+ supports -remote):\")\n",
    "print('blastn -query \"{}\" -db nt -remote -outfmt \"6 qseqid sseqid pident length qlen qstart qend sstart send evalue bitscore stitle\" -max_target_seqs 5 -evalue 1e-10 -out \"{}\"'.format(query, extracted/\"blast_results.tsv\"))\n",
    "\n",
    "# Optional: try to run locally if blastn found and user approves\n",
    "blastn_path = shutil.which(\"blastn\")\n",
    "if blastn_path:\n",
    "    print(\"\\nblastn detected at:\", blastn_path)\n",
    "    run_now = False  # keep default False to avoid accidental long runs; set True to run immediately\n",
    "    if run_now:\n",
    "        out = extracted / \"blast_results.tsv\"\n",
    "        cmd = [\n",
    "            blastn_path, \"-query\", str(query), \"-db\", \"nt\",\n",
    "            \"-outfmt\", \"6 qseqid sseqid pident length qlen qstart qend sstart send evalue bitscore stitle\",\n",
    "            \"-max_target_seqs\", \"5\", \"-evalue\", \"1e-10\", \"-num_threads\", \"8\", \"-out\", str(out)\n",
    "        ]\n",
    "        print(\"Running:\", \" \".join(cmd))\n",
    "        subprocess.run(cmd, check=True)\n",
    "        print(\"blast finished, results:\", out)\n",
    "    else:\n",
    "        print(\"To run locally automatically: set run_now=True in this cell (only do this if you have a local BLAST DB and know what you're doing).\")\n",
    "else:\n",
    "    print(\"\\nblastn not found on PATH. To run BLAST locally install BLAST+ or run the printed command with -remote (requires internet).\")\n",
    "\n",
    "print(\"\\nAfter running BLAST, place 'blast_results.tsv' into the extracted/ folder and run Cell 3 to parse and update abundances.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "58b69f60-b57d-41b6-93b5-3c6ef14965a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using extracted folder: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\n",
      "Loading predictions: predictions_with_uncertainty.csv\n",
      "Detected species column: species_pred_idx | confidence column: kingdom_pred_conf | id column: global_index\n",
      "[SAVED] raw count-based abundance -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\abundance_from_predictions.csv  (rows=52)\n",
      "[SAVED] confidence-weighted abundance -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\abundance_from_predictions_weighted.csv  (rows=52)\n",
      "Loading validation predictions for confusion matrix: val_predictions_calibrated.csv\n",
      "Running NNLS (projected gradient) to estimate true counts ...\n",
      "[SAVED] deconvolved estimates -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\abundance_from_predictions_deconvolved.csv  (rows=52)\n",
      "[SAVED] reconciled/species-estimates -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\abundance_reconciled_species.csv\n",
      "\n",
      "SAMPLE: top 10 raw counts\n",
      "species  count  relative_abundance\n",
      "    180    144            0.378947\n",
      "    110     80            0.210526\n",
      "     22     46            0.121053\n",
      "    118     10            0.026316\n",
      "     12      9            0.023684\n",
      "     13      7            0.018421\n",
      "     19      6            0.015789\n",
      "     64      5            0.013158\n",
      "     47      5            0.013158\n",
      "     49      4            0.010526\n",
      "\n",
      "SAMPLE: top 10 confidence-weighted\n",
      "_species_norm   conf_sum  relative_abundance_weighted\n",
      "          180 138.778935                     0.415176\n",
      "          110  77.531938                     0.231947\n",
      "           22  25.948245                     0.077628\n",
      "           12   7.170390                     0.021451\n",
      "           13   6.823369                     0.020413\n",
      "          118   6.121686                     0.018314\n",
      "           19   4.341063                     0.012987\n",
      "           49   3.861940                     0.011554\n",
      "           64   3.775483                     0.011295\n",
      "           47   3.705058                     0.011084\n",
      "\n",
      "DONE. Files written to: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\n",
      " - abundance_from_predictions.csv\n",
      " - abundance_from_predictions_weighted.csv\n"
     ]
    }
   ],
   "source": [
    "# Robust cell: compute abundances (raw + confidence-weighted) and, if available,\n",
    "# perform confusion-aware NNLS deconvolution to estimate true species abundances.\n",
    "# - No BLAST, no external calls, no hardcoded single path.\n",
    "# - Writes CSVs into the same 'extracted' folder where the predictions file was found.\n",
    "# - Non-destructive: does NOT change per-read species labels (only writes abundance CSVs).\n",
    "import sys, os, math, time, re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def find_extracted_folder():\n",
    "    # Common candidate locations (non-exhaustive). If not found, search for predictions CSV recursively.\n",
    "    candidates = [\n",
    "        Path.cwd() / \"sih\" / \"ncbi_blast_db\" / \"extracted\",\n",
    "        Path.cwd() / \"ncbi_blast_db\" / \"extracted\",\n",
    "        Path.cwd() / \"extracted\",\n",
    "        Path(r\"C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\"),\n",
    "        Path(r\"C:\\Users\\Srijit\\OneDrive\\Desktop\\sihtaxa\\sihabundance\\ncbi_blast_db\\extracted\"),\n",
    "        Path(r\"C:\\Users\\HP\\sihabundance\\ncbi_blast_db\\extracted\"),\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if p.exists() and p.is_dir():\n",
    "            if (p/\"predictions_with_uncertainty.csv\").exists() or (p/\"predictions.csv\").exists():\n",
    "                return p.resolve()\n",
    "    # recursive search for predictions file\n",
    "    for p in Path.cwd().rglob(\"predictions_with_uncertainty.csv\"):\n",
    "        return p.parent.resolve()\n",
    "    for p in Path.cwd().rglob(\"predictions.csv\"):\n",
    "        return p.parent.resolve()\n",
    "    return None\n",
    "\n",
    "def detect_columns(df):\n",
    "    low = {c.lower(): c for c in df.columns}\n",
    "    species_col = next((low[k] for k in low if \"species_pred_label\" in k or k==\"species_pred_label\" or (\"species\" in k and \"pred\" in k)), None)\n",
    "    if species_col is None:\n",
    "        # fallback: any column with 'species' in name\n",
    "        species_col = next((c for c in df.columns if \"species\" in c.lower()), None)\n",
    "    conf_col = next((low[k] for k in low if \"species_pred_conf\" in k or \"species_conf\" in k or \"species_prob\" in k or \"pred_conf\" in k), None)\n",
    "    genus_col = next((low[k] for k in low if \"genus_pred\" in k or \"genus\" in k), None)\n",
    "    family_col = next((low[k] for k in low if \"family_pred\" in k or \"family\" in k), None)\n",
    "    id_col = next((low[k] for k in low if k in (\"id\",\"accession\",\"seqid\",\"read_id\",\"readid\",\"accession_id\",\"global_index\")), None)\n",
    "    return species_col, conf_col, genus_col, family_col, id_col\n",
    "\n",
    "def safe_to_string_series(s):\n",
    "    return s.fillna(\"\").astype(str).apply(lambda x: \" \".join(x.split())).replace({\"\": \"UNASSIGNED\"})\n",
    "\n",
    "def nnls_projected_gradient(M, b, max_iter=5000, tol=1e-6, verbose=False):\n",
    "    # Solve M x = b for x >= 0 by projected gradient descent.\n",
    "    # M: (m x n), b: (m,)\n",
    "    m, n = M.shape\n",
    "    # initialize with non-negative least squares-ish start:\n",
    "    # use least squares if possible\n",
    "    try:\n",
    "        x0 = np.linalg.lstsq(M, b, rcond=None)[0]\n",
    "        x = np.maximum(0.0, x0)\n",
    "    except Exception:\n",
    "        x = np.maximum(0.0, np.ones(n) * (b.sum() / max(1, n)))\n",
    "    # Lipschitz constant approx = ||M||_2^2\n",
    "    try:\n",
    "        s = np.linalg.svd(M, compute_uv=False)\n",
    "        L = (s[0] ** 2) if s.size>0 else (np.linalg.norm(M, ord=2)**2)\n",
    "    except Exception:\n",
    "        L = (np.linalg.norm(M, ord=2)**2) + 1e-8\n",
    "    lr = 1.0 / (L + 1e-12)\n",
    "    prev_norm = None\n",
    "    for it in range(max_iter):\n",
    "        r = M.dot(x) - b                    # residual (m,)\n",
    "        grad = M.T.dot(r)                   # (n,)\n",
    "        x -= lr * grad\n",
    "        # project\n",
    "        x = np.maximum(0.0, x)\n",
    "        # stopping check on residual norm change\n",
    "        rnorm = np.linalg.norm(r)\n",
    "        if prev_norm is not None and abs(prev_norm - rnorm) < tol:\n",
    "            if verbose: print(f\"nnls converged iter {it}, residual {rnorm:.6g}\")\n",
    "            break\n",
    "        prev_norm = rnorm\n",
    "    return x\n",
    "\n",
    "# === main ===\n",
    "extracted = find_extracted_folder()\n",
    "if extracted is None:\n",
    "    print(\"Could not automatically locate an 'extracted/' folder with predictions in the notebook tree.\")\n",
    "    print(\"Please ensure you ran earlier classification cells and that the 'extracted' folder is accessible from the notebook.\")\n",
    "else:\n",
    "    print(\"Using extracted folder:\", extracted)\n",
    "    # find predictions file\n",
    "    preds_path = (extracted / \"predictions_with_uncertainty.csv\")\n",
    "    if not preds_path.exists():\n",
    "        preds_path = (extracted / \"predictions.csv\") if (extracted / \"predictions.csv\").exists() else None\n",
    "    if preds_path is None:\n",
    "        print(\"No predictions CSV found in extracted/. Aborting abundance computation.\")\n",
    "    else:\n",
    "        print(\"Loading predictions:\", preds_path.name)\n",
    "        preds = pd.read_csv(preds_path)\n",
    "        species_col, conf_col, genus_col, family_col, id_col = detect_columns(preds)\n",
    "        if species_col is None:\n",
    "            # defensive fallback: take first column named 'species' or last column\n",
    "            species_col = next((c for c in preds.columns if \"species\" in c.lower()), preds.columns[-1])\n",
    "            print(\"Warning: could not auto-detect species_pred_label column. Using:\", species_col)\n",
    "        print(\"Detected species column:\", species_col, \"| confidence column:\", conf_col, \"| id column:\", id_col)\n",
    "\n",
    "        # normalize\n",
    "        preds[\"_species_norm\"] = safe_to_string_series(preds[species_col])\n",
    "        if conf_col and conf_col in preds.columns:\n",
    "            preds[\"_conf_num\"] = pd.to_numeric(preds[conf_col], errors=\"coerce\").fillna(0.0)\n",
    "        else:\n",
    "            preds[\"_conf_num\"] = 0.0\n",
    "\n",
    "        # RAW counts\n",
    "        counts = preds[\"_species_norm\"].value_counts().reset_index()\n",
    "        counts.columns = [\"species\",\"count\"]\n",
    "        counts[\"relative_abundance\"] = counts[\"count\"] / counts[\"count\"].sum() if counts[\"count\"].sum()>0 else 0.0\n",
    "        out_raw = extracted / \"abundance_from_predictions.csv\"\n",
    "        counts.to_csv(out_raw, index=False)\n",
    "        print(f\"[SAVED] raw count-based abundance -> {out_raw}  (rows={len(counts)})\")\n",
    "\n",
    "        # Confidence-weighted abundance (sum of confidences per predicted label)\n",
    "        weighted = preds.groupby(\"_species_norm\")[\"_conf_num\"].sum().reset_index().rename(columns={\"_conf_num\":\"conf_sum\"})\n",
    "        total_conf = weighted[\"conf_sum\"].sum() if weighted[\"conf_sum\"].sum()>0 else 1.0\n",
    "        weighted[\"relative_abundance_weighted\"] = weighted[\"conf_sum\"] / total_conf\n",
    "        out_weighted = extracted / \"abundance_from_predictions_weighted.csv\"\n",
    "        weighted.to_csv(out_weighted, index=False)\n",
    "        print(f\"[SAVED] confidence-weighted abundance -> {out_weighted}  (rows={len(weighted)})\")\n",
    "\n",
    "        # Attempt confusion-aware deconvolution if a validation predictions file exists\n",
    "        # Search for val_predictions_calibrated.csv, val_predictions.csv or val_predictions_*.csv\n",
    "        val_candidates = [\n",
    "            extracted / \"val_predictions_calibrated.csv\",\n",
    "            extracted / \"val_predictions.csv\",\n",
    "            extracted / \"val_predictions_calibrated.tsv\",\n",
    "            extracted / \"val_predictions.tsv\",\n",
    "            extracted / \"val_predictions_calibrated_*.csv\"\n",
    "        ]\n",
    "        val_path = None\n",
    "        for p in val_candidates:\n",
    "            if p.exists():\n",
    "                val_path = p; break\n",
    "        if val_path is None:\n",
    "            # recursive search for something that looks like validation preds\n",
    "            for p in extracted.rglob(\"val_predictions*\"):\n",
    "                if p.is_file():\n",
    "                    val_path = p; break\n",
    "\n",
    "        if val_path is None:\n",
    "            print(\"Validation predictions not found; skipping confusion-based deconvolution.\")\n",
    "            print(\"You can still use the raw and confidence-weighted abundance CSVs above.\")\n",
    "        else:\n",
    "            print(\"Loading validation predictions for confusion matrix:\", val_path.name)\n",
    "            try:\n",
    "                val = pd.read_csv(val_path)\n",
    "            except Exception:\n",
    "                val = pd.read_csv(val_path, sep=\"\\t\", engine=\"python\", error_bad_lines=False)\n",
    "\n",
    "            # detect val columns\n",
    "            # true column: look for 'true' + 'species'\n",
    "            val_true_col = next((c for c in val.columns if \"true\" in c.lower() and \"species\" in c.lower()), None)\n",
    "            if val_true_col is None:\n",
    "                # try other heuristics\n",
    "                val_true_col = next((c for c in val.columns if \"species_true\" in c.lower() or c.lower()==\"species_true_idx\"), None)\n",
    "            val_pred_col = next((c for c in val.columns if \"species_pred_label\" in c.lower() or \"species_pred\" in c.lower()), None)\n",
    "            if val_pred_col is None:\n",
    "                val_pred_col = next((c for c in val.columns if \"pred\" in c.lower() and \"species\" in c.lower()), None)\n",
    "\n",
    "            if val_true_col is None or val_pred_col is None:\n",
    "                print(\"Could not detect both true/pred columns in validation file; skipping deconvolution.\")\n",
    "            else:\n",
    "                # normalize\n",
    "                val[\"_true_norm\"] = safe_to_string_series(val[val_true_col])\n",
    "                val[\"_pred_norm\"] = safe_to_string_series(val[val_pred_col])\n",
    "\n",
    "                classes_true = sorted(val[\"_true_norm\"].unique().tolist())\n",
    "                classes_pred = sorted(preds[\"_species_norm\"].unique().tolist())\n",
    "\n",
    "                # Build confusion matrix A (shape n_true x n_pred): P(pred=j | true=i)\n",
    "                n_true = len(classes_true)\n",
    "                n_pred = len(classes_pred)\n",
    "                A = np.zeros((n_true, n_pred), dtype=float)\n",
    "                # for each true class, compute distribution of predicted labels\n",
    "                for i, t in enumerate(classes_true):\n",
    "                    subset = val[val[\"_true_norm\"] == t]\n",
    "                    if len(subset) == 0:\n",
    "                        continue\n",
    "                    vc = subset[\"_pred_norm\"].value_counts()\n",
    "                    for j, p_label in enumerate(classes_pred):\n",
    "                        A[i, j] = vc.get(p_label, 0) / len(subset)\n",
    "\n",
    "                # predicted counts vector p (length n_pred) from preds\n",
    "                pred_counts_map = dict(zip(counts[\"species\"], counts[\"count\"]))\n",
    "                p_vec = np.array([pred_counts_map.get(lbl, 0.0) for lbl in classes_pred], dtype=float)\n",
    "\n",
    "                # Solve A^T x = p  -> M x = p where M = A^T (shape n_pred x n_true)\n",
    "                M = A.T  # shape (n_pred, n_true)\n",
    "                # if matrix is all zeros (rare), skip\n",
    "                if M.size == 0 or np.allclose(M, 0.0):\n",
    "                    print(\"Confusion matrix is all zeros (no valid entries); cannot deconvolve. Skipping.\")\n",
    "                else:\n",
    "                    print(\"Running NNLS (projected gradient) to estimate true counts ...\")\n",
    "                    x = nnls_projected_gradient(M, p_vec, max_iter=5000, tol=1e-6, verbose=False)\n",
    "                    # numeric cleanup\n",
    "                    x = np.maximum(0.0, x)\n",
    "                    total_est = x.sum()\n",
    "                    # If estimated total is 0, skip\n",
    "                    if total_est <= 0:\n",
    "                        print(\"Deconvolution produced zero total mass; skipping result save.\")\n",
    "                    else:\n",
    "                        # produce dataframe mapping true species -> pred_count (if same label) -> est_true_count\n",
    "                        rows = []\n",
    "                        for i, t in enumerate(classes_true):\n",
    "                            pred_count_for_t = pred_counts_map.get(t, 0)\n",
    "                            est_true_count = float(x[i]) if i < len(x) else 0.0\n",
    "                            rows.append({\n",
    "                                \"species\": t,\n",
    "                                \"pred_count\": int(pred_count_for_t),\n",
    "                                \"pred_count_rel\": pred_count_for_t / p_vec.sum() if p_vec.sum()>0 else 0.0,\n",
    "                                \"est_true_count\": est_true_count,\n",
    "                                \"est_true_rel\": est_true_count / total_est if total_est>0 else 0.0\n",
    "                            })\n",
    "                        deconv_df = pd.DataFrame(rows).sort_values(\"est_true_count\", ascending=False).reset_index(drop=True)\n",
    "                        out_deconv = extracted / \"abundance_from_predictions_deconvolved.csv\"\n",
    "                        deconv_df.to_csv(out_deconv, index=False)\n",
    "                        print(f\"[SAVED] deconvolved estimates -> {out_deconv}  (rows={len(deconv_df)})\")\n",
    "                        # also save a reconciled species abundance CSV similar to prior pipeline\n",
    "                        out_recon = extracted / \"abundance_reconciled_species.csv\"\n",
    "                        deconv_df[[\"species\",\"est_true_rel\"]].rename(columns={\"est_true_rel\":\"est_rel\"}).to_csv(out_recon, index=False)\n",
    "                        print(f\"[SAVED] reconciled/species-estimates -> {out_recon}\")\n",
    "\n",
    "        # final sample prints\n",
    "        print(\"\\nSAMPLE: top 10 raw counts\")\n",
    "        print(counts.head(10).to_string(index=False))\n",
    "        print(\"\\nSAMPLE: top 10 confidence-weighted\")\n",
    "        print(weighted.sort_values(\"conf_sum\", ascending=False).head(10).to_string(index=False))\n",
    "        print(\"\\nDONE. Files written to:\", extracted)\n",
    "        print(\" -\", out_raw.name)\n",
    "        print(\" -\", out_weighted.name)\n",
    "        # deconv files may or may not exist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "535cfbb2-9119-4609-8fe0-ee3905670508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for 'extracted/' folder...\n",
      "Using extracted folder: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\n",
      "Loading predictions: predictions_with_uncertainty.csv\n",
      "Detected columns:\n",
      " - species column  -> species_pred_label\n",
      " - confidence col  -> species_pred_conf\n",
      " - id column       -> id\n",
      "Found BLAST file: all_unassigned_for_blast.fasta (parsing...)\n",
      "Could not parse BLAST file automatically. Inspect file and ensure it is BLAST tabular (-outfmt 6) with columns:\n",
      "  qseqid sseqid pident length qlen qstart qend sstart send evalue bitscore stitle\n",
      "Aborting (no changes).\n",
      "\n",
      "Cell finished. Check the extracted/ folder for generated audit files.\n"
     ]
    }
   ],
   "source": [
    "# CELL: Robust BLAST-results parser + conservative application of BLAST species to UNASSIGNED rows\n",
    "# - Place this cell into your notebook and run it (no other edits required unless you want to tune thresholds).\n",
    "# - Outputs (in extracted/):\n",
    "#    - blast_species_assignments.csv         (audit of chosen BLAST-based species assignments)\n",
    "#    - predictions_blast_enriched.csv        (predictions with blast columns added)\n",
    "#    - predictions_after_blast_forced.csv    (if APPLY_ASSIGNMENTS=True and rows changed)\n",
    "#    - abundance_after_blast.csv             (species counts after applying BLAST)\n",
    "#\n",
    "# NOTES:\n",
    "# - This cell WILL ONLY REPLACE labels for rows that are currently UNASSIGNED (conservative).\n",
    "# - If you want it to try remote BLAST automatically, I included an optional Biopython section (disabled by default).\n",
    "# - This cell is robust: it searches for BLAST result files with names like 'blast*', handles multiple column formats,\n",
    "#   and will NOT crash — it prints clear diagnostics if no BLAST file is found.\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ----------------- USER-CONFIG (tweakable) -----------------\n",
    "MIN_PID = 97.0            # min percent identity (accept BLAST hit)\n",
    "MIN_COV = 80.0            # min coverage % (alignment length / qlen *100)\n",
    "MAX_EVAL = 1e-6           # max e-value to accept\n",
    "APPLY_ASSIGNMENTS = True  # If True, we actually write predictions_after_blast_forced.csv with replacements\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "def find_extracted_folder():\n",
    "    # try common locations seen in your session\n",
    "    candidates = [\n",
    "        Path.cwd() / \"sih\" / \"ncbi_blast_db\" / \"extracted\",\n",
    "        Path.cwd() / \"ncbi_blast_db\" / \"extracted\",\n",
    "        Path.cwd() / \"extracted\",\n",
    "        Path(r\"C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\"),\n",
    "        Path(r\"C:\\Users\\Srijit\\OneDrive\\Desktop\\sihtaxa\\sihabundance\\ncbi_blast_db\\extracted\"),\n",
    "        Path(r\"C:\\Users\\Srijit\\OneDrive\\Desktop\\sihtaxa\\sih\\ncbi_blast_db\\extracted\"),\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if p.exists() and p.is_dir():\n",
    "            return p.resolve()\n",
    "    # fallback: search for any folder named \"extracted\" under cwd\n",
    "    for p in Path.cwd().rglob(\"**/extracted\"):\n",
    "        if p.is_dir():\n",
    "            return p.resolve()\n",
    "    return None\n",
    "\n",
    "def choose_species_column(df):\n",
    "    # pick the best species column: prefer '*species*label*' or a column with many non-numeric values\n",
    "    cols = list(df.columns)\n",
    "    cand = [c for c in cols if 'species' in c.lower()]\n",
    "    if not cand:\n",
    "        return None\n",
    "    # prefer label-like names\n",
    "    for k in cand:\n",
    "        if 'label' in k.lower() or 'name' in k.lower():\n",
    "            return k\n",
    "    # prefer column that is mostly non-numeric (human-readable)\n",
    "    def non_numeric_ratio(series):\n",
    "        s = series.dropna().astype(str).head(1000)\n",
    "        if len(s) == 0:\n",
    "            return 0.0\n",
    "        nonnum = [(not is_float(x)) for x in s]\n",
    "        return float(sum(nonnum)) / len(s)\n",
    "    for k in cand:\n",
    "        try:\n",
    "            r = non_numeric_ratio(df[k])\n",
    "            if r > 0.5:\n",
    "                return k\n",
    "        except Exception:\n",
    "            continue\n",
    "    # fallback: return first candidate\n",
    "    return cand[0]\n",
    "\n",
    "def choose_confidence_column(df, species_col):\n",
    "    # try to find column that contains both 'species' and 'conf' or 'prob' or 'mean_topprob'\n",
    "    for c in df.columns:\n",
    "        lc = c.lower()\n",
    "        if 'species' in lc and any(x in lc for x in ('conf','prob','mean','topprob','score','confidence')):\n",
    "            # ensure it's numeric-ish\n",
    "            try:\n",
    "                pd.to_numeric(df[c].dropna().astype(str).head(200))\n",
    "                return c\n",
    "            except Exception:\n",
    "                pass\n",
    "    # otherwise pick any numeric column with 'conf' or 'prob' in name\n",
    "    for c in df.columns:\n",
    "        lc = c.lower()\n",
    "        if any(x in lc for x in ('conf','prob','confidence','topprob')):\n",
    "            try:\n",
    "                pd.to_numeric(df[c].dropna().astype(str).head(200))\n",
    "                return c\n",
    "            except Exception:\n",
    "                pass\n",
    "    # last resort: None\n",
    "    return None\n",
    "\n",
    "def choose_id_column(df):\n",
    "    names = ['id','accession','seqid','read_id','readid','accession_id','global_index','query_id']\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    for n in names:\n",
    "        if n in cols:\n",
    "            return cols[n]\n",
    "    # fallback: if there is a column with  unique values near number of rows, pick the most-id-like\n",
    "    for c in df.columns:\n",
    "        if df[c].nunique() >= max(1, int(len(df)*0.5)):\n",
    "            if 'id' in c.lower() or 'access' in c.lower() or 'acc' in c.lower():\n",
    "                return c\n",
    "    # fallback to first column\n",
    "    return df.columns[0]\n",
    "\n",
    "def is_float(s):\n",
    "    try:\n",
    "        float(str(s))\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def find_blast_file(extracted):\n",
    "    # search for common BLAST result filenames or anything starting with 'blast' in extracted\n",
    "    patterns = [\"blast_results.tsv\",\"blast_results.out\",\"blast_results.txt\",\"blast.tsv\",\"blast.out\",\"blast.txt\",\n",
    "                \"*blast*.tsv\",\"*blast*.txt\",\"*blast*.out\",\"blast_results*.tsv\",\"blast*results*.tsv\"]\n",
    "    # try exact names first\n",
    "    for name in patterns[:6]:\n",
    "        p = extracted / name\n",
    "        if p.exists():\n",
    "            return p\n",
    "    # glob search\n",
    "    for pat in patterns[6:]:\n",
    "        hits = list(extracted.glob(pat))\n",
    "        if hits:\n",
    "            # choose the largest (likely the actual results) or the first\n",
    "            hits = sorted(hits, key=lambda p: p.stat().st_size if p.exists() else 0, reverse=True)\n",
    "            return hits[0]\n",
    "    # also search recursively for any file whose name contains 'blast' or 'blast_results'\n",
    "    for p in extracted.rglob(\"*\"):\n",
    "        if p.is_file() and 'blast' in p.name.lower():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def parse_blast_tsv(blast_path):\n",
    "    # Try reading tab-separated BLAST outfmt6 (no header) with 12 columns:\n",
    "    cols12 = [\"qseqid\",\"sseqid\",\"pident\",\"length\",\"qlen\",\"qstart\",\"qend\",\"sstart\",\"send\",\"evalue\",\"bitscore\",\"stitle\"]\n",
    "    try:\n",
    "        df = pd.read_csv(blast_path, sep=\"\\t\", header=None, quoting=3, dtype=str, engine='python')\n",
    "        if df.shape[1] >= 12:\n",
    "            df = df.iloc[:, :12]\n",
    "            df.columns = cols12\n",
    "            return df\n",
    "        # maybe file has header with these names\n",
    "        dfh = pd.read_csv(blast_path, sep=\"\\t\", header=0, engine='python', dtype=str)\n",
    "        # normalize columns if present\n",
    "        found = [c for c in dfh.columns if c.lower() in cols12]\n",
    "        if found:\n",
    "            # try to map\n",
    "            lower_map = {c.lower(): c for c in dfh.columns}\n",
    "            cols_map = {name: lower_map.get(name) for name in cols12 if lower_map.get(name) is not None}\n",
    "            dfh = dfh.rename(columns={v:k for k,v in cols_map.items()})\n",
    "            return dfh\n",
    "    except Exception as e:\n",
    "        # try a more permissive read\n",
    "        try:\n",
    "            text = blast_path.read_text(errors='ignore')\n",
    "            # simple heuristic: lines with >=11 tabs -> outfmt6\n",
    "            lines = [L for L in text.splitlines() if L.count('\\t')>=11]\n",
    "            if lines:\n",
    "                from io import StringIO\n",
    "                df = pd.read_csv(StringIO(\"\\n\".join(lines)), sep=\"\\t\", header=None, dtype=str)\n",
    "                if df.shape[1] >= 12:\n",
    "                    df = df.iloc[:, :12]\n",
    "                    df.columns = cols12\n",
    "                    return df\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def extract_species_from_title(title):\n",
    "    if not isinstance(title, str) or not title.strip():\n",
    "        return None\n",
    "    title = title.strip()\n",
    "    # 1) binomial: Genus species (Genus capitalized, species lowercase)\n",
    "    m = re.search(r'\\b([A-Z][a-zA-Z\\-]+)\\s+([a-z][a-zA-Z\\-\\(\\)]+)\\b', title)\n",
    "    if m:\n",
    "        return f\"{m.group(1)} {m.group(2)}\"\n",
    "    # 2) Genus sp. or Genus sp\n",
    "    m2 = re.search(r'\\b([A-Z][a-zA-Z\\-]+)\\s+sp\\b', title)\n",
    "    if m2:\n",
    "        return f\"{m2.group(1)} sp.\"\n",
    "    # 3) at least a capitalized Genus word (be conservative: return None in this case)\n",
    "    # Avoid guessing from e.g. 'uncultured bacterium' etc.\n",
    "    return None\n",
    "\n",
    "# --- main ---\n",
    "print(\"Searching for 'extracted/' folder...\")\n",
    "extracted = find_extracted_folder()\n",
    "if extracted is None:\n",
    "    print(\"ERROR: could not find an 'extracted' directory automatically. Please run the earlier extraction cell or set EXTRACT_DIR to the correct path.\")\n",
    "else:\n",
    "    print(\"Using extracted folder:\", extracted)\n",
    "\n",
    "if extracted is None:\n",
    "    # stop gracefully\n",
    "    print(\"No extracted/ found. Aborting this cell (nothing changed).\")\n",
    "else:\n",
    "    # find predictions CSV\n",
    "    pred_candidates = [\"predictions_with_uncertainty.csv\",\"predictions.csv\",\"predictions_blast_enriched.csv\"]\n",
    "    pred_file = None\n",
    "    for name in pred_candidates:\n",
    "        p = extracted / name\n",
    "        if p.exists():\n",
    "            pred_file = p\n",
    "            break\n",
    "    if pred_file is None:\n",
    "        # try any CSV with 'predictions' in name\n",
    "        preds = list(extracted.glob(\"*predic*.csv\"))\n",
    "        if preds:\n",
    "            pred_file = preds[0]\n",
    "    if pred_file is None:\n",
    "        print(\"ERROR: cannot find predictions CSV in extracted/. Expected file e.g. predictions_with_uncertainty.csv\")\n",
    "        print(\"Files in extracted/:\")\n",
    "        for f in sorted(extracted.iterdir()):\n",
    "            print(\" \", f.name)\n",
    "        print(\"\\nAborting — nothing changed.\")\n",
    "    else:\n",
    "        print(\"Loading predictions:\", pred_file.name)\n",
    "        try:\n",
    "            pred = pd.read_csv(pred_file, dtype=str)\n",
    "        except Exception as e:\n",
    "            print(\"Failed to read predictions CSV:\", e)\n",
    "            pred = pd.read_csv(pred_file, engine='python', dtype=str, error_bad_lines=False)\n",
    "\n",
    "        # determine core columns robustly\n",
    "        species_col = choose_species_column(pred)\n",
    "        conf_col = choose_confidence_column(pred, species_col)\n",
    "        id_col = choose_id_column(pred)\n",
    "\n",
    "        print(\"Detected columns:\")\n",
    "        print(\" - species column  ->\", species_col)\n",
    "        print(\" - confidence col  ->\", conf_col)\n",
    "        print(\" - id column       ->\", id_col)\n",
    "\n",
    "        # find blast results file\n",
    "        blast_path = find_blast_file(extracted)\n",
    "        if blast_path is None:\n",
    "            print(\"\\nNo BLAST results file found in extracted/. Searching for files named like 'blast*.tsv' etc returned nothing.\")\n",
    "            # helpful suggestions\n",
    "            print(\"If you already ran BLAST, place the BLAST tabular (outfmt 6) file named 'blast_results.tsv' into:\")\n",
    "            print(\"  \", extracted)\n",
    "            print(\"\\nIf you haven't run BLAST, you can run (local blastn or remote NCBI - remote may be slow and has usage limits).\")\n",
    "            print(\"Example local command (blast+):\\n  blastn -query all_unassigned_for_blast.fasta -db nt -outfmt '6 qseqid sseqid pident length qlen qstart qend sstart send evalue bitscore stitle' -max_target_seqs 5 -evalue 1e-10 -out blast_results.tsv -num_threads 8\")\n",
    "            print(\"\\nAfter you run BLAST and put blast_results.tsv into the extracted/ folder, re-run this cell.\")\n",
    "        else:\n",
    "            print(\"Found BLAST file:\", blast_path.name, \"(parsing...)\")\n",
    "            dfb = parse_blast_tsv(blast_path)\n",
    "            if dfb is None:\n",
    "                print(\"Could not parse BLAST file automatically. Inspect file and ensure it is BLAST tabular (-outfmt 6) with columns:\")\n",
    "                print(\"  qseqid sseqid pident length qlen qstart qend sstart send evalue bitscore stitle\")\n",
    "                print(\"Aborting (no changes).\")\n",
    "            else:\n",
    "                # coerce numeric columns\n",
    "                for col in (\"pident\",\"length\",\"qlen\",\"evalue\",\"bitscore\"):\n",
    "                    if col in dfb.columns:\n",
    "                        dfb[col] = pd.to_numeric(dfb[col], errors='coerce').fillna(0.0)\n",
    "                # compute coverage safely\n",
    "                if \"qlen\" in dfb.columns and \"length\" in dfb.columns:\n",
    "                    dfb[\"coverage_pct\"] = dfb.apply(lambda r: (float(r[\"length\"])/float(r[\"qlen\"])*100.0) if (pd.notna(r[\"qlen\"]) and float(r[\"qlen\"])>0) else 0.0, axis=1)\n",
    "                else:\n",
    "                    dfb[\"coverage_pct\"] = 0.0\n",
    "\n",
    "                # conservative filtering\n",
    "                pre_len = len(dfb)\n",
    "                dfb_high = dfb[(dfb[\"pident\"] >= MIN_PID) & (dfb[\"coverage_pct\"] >= MIN_COV) & (dfb[\"evalue\"] <= MAX_EVAL)].copy()\n",
    "                print(f\"BLAST rows total={pre_len}, passing thresholds (pident>={MIN_PID}, cov>={MIN_COV}, e<={MAX_EVAL}) = {len(dfb_high)}\")\n",
    "\n",
    "                if dfb_high.empty:\n",
    "                    print(\"No BLAST hits passed thresholds. Nothing will be applied.\")\n",
    "                    # still save a tiny audit for the user\n",
    "                    try:\n",
    "                        dfb.to_csv(extracted/\"blast_parsed_all.csv\", index=False)\n",
    "                        print(\"Saved parsed BLAST (unfiltered) -> blast_parsed_all.csv\")\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                else:\n",
    "                    # pick top hit per qseqid by bitscore (most reliable)\n",
    "                    dfb_top = dfb_high.sort_values([\"qseqid\",\"bitscore\"], ascending=[True, False]).groupby(\"qseqid\", as_index=False).first()\n",
    "                    print(\"Unique queries with accepted top-hit:\", len(dfb_top))\n",
    "\n",
    "                    # extract species names conservatively\n",
    "                    assignments = {}\n",
    "                    for _, row in dfb_top.iterrows():\n",
    "                        q = str(row[\"qseqid\"])\n",
    "                        stitle = row.get(\"stitle\", \"\")\n",
    "                        species = extract_species_from_title(stitle)\n",
    "                        if species is None:\n",
    "                            # do not guess from non-binomial titles\n",
    "                            continue\n",
    "                        assignments[q] = {\n",
    "                            \"species\": species,\n",
    "                            \"pident\": float(row[\"pident\"]),\n",
    "                            \"coverage\": float(row[\"coverage_pct\"]),\n",
    "                            \"evalue\": float(row[\"evalue\"]),\n",
    "                            \"sseqid\": row.get(\"sseqid\"),\n",
    "                            \"stitle\": stitle\n",
    "                        }\n",
    "                    print(\"Conservative, parsed BLAST-derived species assignments:\", len(assignments))\n",
    "                    # save assignments for audit\n",
    "                    assign_df = pd.DataFrame([{\"query_id\":q, **v} for q,v in assignments.items()])\n",
    "                    assign_out = extracted / \"blast_species_assignments.csv\"\n",
    "                    assign_df.to_csv(assign_out, index=False)\n",
    "                    print(\"Wrote BLAST assignments ->\", assign_out.name)\n",
    "\n",
    "                    # prepare predictions (do not overwrite original file)\n",
    "                    pred2 = pred.copy()\n",
    "                    # ensure id and species columns exist\n",
    "                    if species_col is None:\n",
    "                        # create one if missing\n",
    "                        species_col = \"species_pred_label\"\n",
    "                        pred2[species_col] = \"\"\n",
    "                    if id_col not in pred2.columns:\n",
    "                        print(f\"ERROR: Could not detect id column in predictions to map BLAST qseqid to rows. Columns available: {list(pred2.columns)}\")\n",
    "                        print(\"Aborting apply. You can still inspect blast_species_assignments.csv manually.\")\n",
    "                    else:\n",
    "                        # normalized current species label string\n",
    "                        pred2[\"_species_norm\"] = pred2[species_col].astype(str).fillna(\"\").apply(lambda s: \" \".join(str(s).split()))\n",
    "                        mask_unassigned = pred2[\"_species_norm\"].str.upper().isin([\"UNASSIGNED\", \"\", \"NONE\", \"NAN\"])\n",
    "                        n_unassigned_before = int(mask_unassigned.sum())\n",
    "                        print(\"UNASSIGNED rows before:\", n_unassigned_before)\n",
    "\n",
    "                        # try multiple matching heuristics; only change rows currently UNASSIGNED\n",
    "                        rows_to_change = []  # (row_index, new_species, q, info)\n",
    "                        for q, info in assignments.items():\n",
    "                            # Exact match\n",
    "                            exact_mask = pred2[id_col].astype(str) == str(q)\n",
    "                            if exact_mask.any():\n",
    "                                for ri in pred2.index[exact_mask]:\n",
    "                                    if pred2.at[ri, \"_species_norm\"].upper() in (\"UNASSIGNED\",\"\", \"NONE\", \"NAN\"):\n",
    "                                        rows_to_change.append((ri, info[\"species\"], q, info))\n",
    "                                continue\n",
    "                            # Versionless match: strip .1 etc\n",
    "                            q_base = q.split('.')[0]\n",
    "                            try:\n",
    "                                id_base = pred2[id_col].astype(str).str.split('.').str[0]\n",
    "                                mask_base = id_base == q_base\n",
    "                                if mask_base.any():\n",
    "                                    for ri in pred2.index[mask_base]:\n",
    "                                        if pred2.at[ri, \"_species_norm\"].upper() in (\"UNASSIGNED\",\"\", \"NONE\", \"NAN\"):\n",
    "                                            rows_to_change.append((ri, info[\"species\"], q, info))\n",
    "                                    continue\n",
    "                            except Exception:\n",
    "                                pass\n",
    "                            # substring match (conservative)\n",
    "                            try:\n",
    "                                mask_sub = pred2[id_col].astype(str).str.contains(str(q), na=False)\n",
    "                                if mask_sub.any():\n",
    "                                    for ri in pred2.index[mask_sub]:\n",
    "                                        if pred2.at[ri, \"_species_norm\"].upper() in (\"UNASSIGNED\",\"\", \"NONE\", \"NAN\"):\n",
    "                                            rows_to_change.append((ri, info[\"species\"], q, info))\n",
    "                                    continue\n",
    "                            except Exception:\n",
    "                                pass\n",
    "                            # no match -> skip\n",
    "\n",
    "                        # de-duplicate picking best pident/coverage if multiple assignments for same row\n",
    "                        row_best = {}\n",
    "                        for ri, new_sp, q, info in rows_to_change:\n",
    "                            key = int(ri)\n",
    "                            if key not in row_best:\n",
    "                                row_best[key] = (new_sp, q, info)\n",
    "                            else:\n",
    "                                existing = row_best[key][2]\n",
    "                                # prefer higher pident, then higher coverage\n",
    "                                if info.get(\"pident\",0) > existing.get(\"pident\",0) or (info.get(\"pident\",0)==existing.get(\"pident\",0) and info.get(\"coverage\",0) > existing.get(\"coverage\",0)):\n",
    "                                    row_best[key] = (new_sp, q, info)\n",
    "                        final_rows = sorted([(ri, *row_best[ri]) for ri in row_best])\n",
    "                        n_candidates = len(final_rows)\n",
    "                        print(\"Candidate UNASSIGNED rows matching BLAST assignments:\", n_candidates)\n",
    "\n",
    "                        if n_candidates == 0:\n",
    "                            print(\"No UNASSIGNED rows matched conservative BLAST assignments (nothing applied).\")\n",
    "                        else:\n",
    "                            # apply if user wants\n",
    "                            if APPLY_ASSIGNMENTS:\n",
    "                                for ri, new_sp, q, info in final_rows:\n",
    "                                    pred2.at[ri, species_col] = new_sp\n",
    "                                    pred2.at[ri, \"blast_assigned_species\"] = new_sp\n",
    "                                    pred2.at[ri, \"blast_assigned_query\"] = q\n",
    "                                    pred2.at[ri, \"blast_pident\"] = info[\"pident\"]\n",
    "                                    pred2.at[ri, \"blast_coverage\"] = info[\"coverage\"]\n",
    "                                    pred2.at[ri, \"blast_evalue\"] = info[\"evalue\"]\n",
    "                                    pred2.at[ri, \"blast_sseqid\"] = info.get(\"sseqid\")\n",
    "                                    pred2.at[ri, \"blast_stitle\"] = info.get(\"stitle\")\n",
    "                                out_forced = extracted / \"predictions_after_blast_forced.csv\"\n",
    "                                pred2.to_csv(out_forced, index=False)\n",
    "                                print(f\"Applied BLAST assignments to {len(final_rows)} UNASSIGNED rows -> saved {out_forced.name}\")\n",
    "                            else:\n",
    "                                print(f\"APPLY_ASSIGNMENTS=False; {len(final_rows)} candidate rows identified but not applied.\")\n",
    "\n",
    "                        # always write enriched copy (with BLAST columns added)\n",
    "                        out_enriched = extracted / \"predictions_blast_enriched.csv\"\n",
    "                        pred2.to_csv(out_enriched, index=False)\n",
    "                        print(\"Wrote enriched predictions (audit) ->\", out_enriched.name)\n",
    "\n",
    "                        # recompute abundance counts from updated predictions\n",
    "                        pred2[\"_species_norm_final\"] = pred2[species_col].astype(str).fillna(\"\").apply(lambda s: \" \".join(str(s).split()))\n",
    "                        counts = pred2[\"_species_norm_final\"].value_counts(dropna=False).reset_index()\n",
    "                        counts.columns = [\"species\",\"count\"]\n",
    "                        counts[\"count\"] = pd.to_numeric(counts[\"count\"], errors='coerce').fillna(0).astype(int)\n",
    "                        total = counts[\"count\"].sum()\n",
    "                        counts[\"rel\"] = counts[\"count\"] / total if total>0 else 0.0\n",
    "                        out_abund = extracted / \"abundance_after_blast.csv\"\n",
    "                        counts.to_csv(out_abund, index=False)\n",
    "                        print(\"Wrote abundance_after_blast.csv ->\", out_abund.name)\n",
    "                        print(\"\\nTop species after applying BLAST assignments (top 20):\")\n",
    "                        print(counts.head(20).to_string(index=False))\n",
    "\n",
    "                        n_unassigned_after = int((pred2[\"_species_norm_final\"].str.upper().isin([\"UNASSIGNED\",\"\", \"NONE\", \"NAN\"])).sum())\n",
    "                        print(f\"\\nSUMMARY: UNASSIGNED before = {n_unassigned_before}; UNASSIGNED after = {n_unassigned_after}; filled = {n_unassigned_before - n_unassigned_after}\")\n",
    "\n",
    "print(\"\\nCell finished. Check the extracted/ folder for generated audit files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d71ec0c9-6554-409c-91e6-5cee2c5059c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for 'extracted/' folder...\n",
      "Using extracted folder: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\n",
      "Loading predictions: predictions_with_uncertainty.csv\n",
      "Detected columns:\n",
      " - species column  -> species_pred_label\n",
      " - confidence col  -> species_pred_conf\n",
      " - id column       -> id\n",
      "Found BLAST file: all_unassigned_for_blast.fasta (parsing...)\n",
      "Could not parse BLAST file automatically. Inspect file and ensure it is BLAST tabular (-outfmt 6) with columns:\n",
      "  qseqid sseqid pident length qlen qstart qend sstart send evalue bitscore stitle\n",
      "Aborting (no changes).\n",
      "\n",
      "Cell finished. Check the extracted/ folder for generated audit files.\n"
     ]
    }
   ],
   "source": [
    "# CELL: Robust BLAST-results parser + conservative application of BLAST species to UNASSIGNED rows\n",
    "# - Place this cell into your notebook and run it (no other edits required unless you want to tune thresholds).\n",
    "# - Outputs (in extracted/):\n",
    "#    - blast_species_assignments.csv         (audit of chosen BLAST-based species assignments)\n",
    "#    - predictions_blast_enriched.csv        (predictions with blast columns added)\n",
    "#    - predictions_after_blast_forced.csv    (if APPLY_ASSIGNMENTS=True and rows changed)\n",
    "#    - abundance_after_blast.csv             (species counts after applying BLAST)\n",
    "#\n",
    "# NOTES:\n",
    "# - This cell WILL ONLY REPLACE labels for rows that are currently UNASSIGNED (conservative).\n",
    "# - If you want it to try remote BLAST automatically, I included an optional Biopython section (disabled by default).\n",
    "# - This cell is robust: it searches for BLAST result files with names like 'blast*', handles multiple column formats,\n",
    "#   and will NOT crash — it prints clear diagnostics if no BLAST file is found.\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ----------------- USER-CONFIG (tweakable) -----------------\n",
    "MIN_PID = 97.0            # min percent identity (accept BLAST hit)\n",
    "MIN_COV = 80.0            # min coverage % (alignment length / qlen *100)\n",
    "MAX_EVAL = 1e-6           # max e-value to accept\n",
    "APPLY_ASSIGNMENTS = True  # If True, we actually write predictions_after_blast_forced.csv with replacements\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "def find_extracted_folder():\n",
    "    # try common locations seen in your session\n",
    "    candidates = [\n",
    "        Path.cwd() / \"sih\" / \"ncbi_blast_db\" / \"extracted\",\n",
    "        Path.cwd() / \"ncbi_blast_db\" / \"extracted\",\n",
    "        Path.cwd() / \"extracted\",\n",
    "        Path(r\"C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\"),\n",
    "        Path(r\"C:\\Users\\Srijit\\OneDrive\\Desktop\\sihtaxa\\sihabundance\\ncbi_blast_db\\extracted\"),\n",
    "        Path(r\"C:\\Users\\Srijit\\OneDrive\\Desktop\\sihtaxa\\sih\\ncbi_blast_db\\extracted\"),\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if p.exists() and p.is_dir():\n",
    "            return p.resolve()\n",
    "    # fallback: search for any folder named \"extracted\" under cwd\n",
    "    for p in Path.cwd().rglob(\"**/extracted\"):\n",
    "        if p.is_dir():\n",
    "            return p.resolve()\n",
    "    return None\n",
    "\n",
    "def choose_species_column(df):\n",
    "    # pick the best species column: prefer '*species*label*' or a column with many non-numeric values\n",
    "    cols = list(df.columns)\n",
    "    cand = [c for c in cols if 'species' in c.lower()]\n",
    "    if not cand:\n",
    "        return None\n",
    "    # prefer label-like names\n",
    "    for k in cand:\n",
    "        if 'label' in k.lower() or 'name' in k.lower():\n",
    "            return k\n",
    "    # prefer column that is mostly non-numeric (human-readable)\n",
    "    def non_numeric_ratio(series):\n",
    "        s = series.dropna().astype(str).head(1000)\n",
    "        if len(s) == 0:\n",
    "            return 0.0\n",
    "        nonnum = [(not is_float(x)) for x in s]\n",
    "        return float(sum(nonnum)) / len(s)\n",
    "    for k in cand:\n",
    "        try:\n",
    "            r = non_numeric_ratio(df[k])\n",
    "            if r > 0.5:\n",
    "                return k\n",
    "        except Exception:\n",
    "            continue\n",
    "    # fallback: return first candidate\n",
    "    return cand[0]\n",
    "\n",
    "def choose_confidence_column(df, species_col):\n",
    "    # try to find column that contains both 'species' and 'conf' or 'prob' or 'mean_topprob'\n",
    "    for c in df.columns:\n",
    "        lc = c.lower()\n",
    "        if 'species' in lc and any(x in lc for x in ('conf','prob','mean','topprob','score','confidence')):\n",
    "            # ensure it's numeric-ish\n",
    "            try:\n",
    "                pd.to_numeric(df[c].dropna().astype(str).head(200))\n",
    "                return c\n",
    "            except Exception:\n",
    "                pass\n",
    "    # otherwise pick any numeric column with 'conf' or 'prob' in name\n",
    "    for c in df.columns:\n",
    "        lc = c.lower()\n",
    "        if any(x in lc for x in ('conf','prob','confidence','topprob')):\n",
    "            try:\n",
    "                pd.to_numeric(df[c].dropna().astype(str).head(200))\n",
    "                return c\n",
    "            except Exception:\n",
    "                pass\n",
    "    # last resort: None\n",
    "    return None\n",
    "\n",
    "def choose_id_column(df):\n",
    "    names = ['id','accession','seqid','read_id','readid','accession_id','global_index','query_id']\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    for n in names:\n",
    "        if n in cols:\n",
    "            return cols[n]\n",
    "    # fallback: if there is a column with  unique values near number of rows, pick the most-id-like\n",
    "    for c in df.columns:\n",
    "        if df[c].nunique() >= max(1, int(len(df)*0.5)):\n",
    "            if 'id' in c.lower() or 'access' in c.lower() or 'acc' in c.lower():\n",
    "                return c\n",
    "    # fallback to first column\n",
    "    return df.columns[0]\n",
    "\n",
    "def is_float(s):\n",
    "    try:\n",
    "        float(str(s))\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def find_blast_file(extracted):\n",
    "    # search for common BLAST result filenames or anything starting with 'blast' in extracted\n",
    "    patterns = [\"blast_results.tsv\",\"blast_results.out\",\"blast_results.txt\",\"blast.tsv\",\"blast.out\",\"blast.txt\",\n",
    "                \"*blast*.tsv\",\"*blast*.txt\",\"*blast*.out\",\"blast_results*.tsv\",\"blast*results*.tsv\"]\n",
    "    # try exact names first\n",
    "    for name in patterns[:6]:\n",
    "        p = extracted / name\n",
    "        if p.exists():\n",
    "            return p\n",
    "    # glob search\n",
    "    for pat in patterns[6:]:\n",
    "        hits = list(extracted.glob(pat))\n",
    "        if hits:\n",
    "            # choose the largest (likely the actual results) or the first\n",
    "            hits = sorted(hits, key=lambda p: p.stat().st_size if p.exists() else 0, reverse=True)\n",
    "            return hits[0]\n",
    "    # also search recursively for any file whose name contains 'blast' or 'blast_results'\n",
    "    for p in extracted.rglob(\"*\"):\n",
    "        if p.is_file() and 'blast' in p.name.lower():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def parse_blast_tsv(blast_path):\n",
    "    # Try reading tab-separated BLAST outfmt6 (no header) with 12 columns:\n",
    "    cols12 = [\"qseqid\",\"sseqid\",\"pident\",\"length\",\"qlen\",\"qstart\",\"qend\",\"sstart\",\"send\",\"evalue\",\"bitscore\",\"stitle\"]\n",
    "    try:\n",
    "        df = pd.read_csv(blast_path, sep=\"\\t\", header=None, quoting=3, dtype=str, engine='python')\n",
    "        if df.shape[1] >= 12:\n",
    "            df = df.iloc[:, :12]\n",
    "            df.columns = cols12\n",
    "            return df\n",
    "        # maybe file has header with these names\n",
    "        dfh = pd.read_csv(blast_path, sep=\"\\t\", header=0, engine='python', dtype=str)\n",
    "        # normalize columns if present\n",
    "        found = [c for c in dfh.columns if c.lower() in cols12]\n",
    "        if found:\n",
    "            # try to map\n",
    "            lower_map = {c.lower(): c for c in dfh.columns}\n",
    "            cols_map = {name: lower_map.get(name) for name in cols12 if lower_map.get(name) is not None}\n",
    "            dfh = dfh.rename(columns={v:k for k,v in cols_map.items()})\n",
    "            return dfh\n",
    "    except Exception as e:\n",
    "        # try a more permissive read\n",
    "        try:\n",
    "            text = blast_path.read_text(errors='ignore')\n",
    "            # simple heuristic: lines with >=11 tabs -> outfmt6\n",
    "            lines = [L for L in text.splitlines() if L.count('\\t')>=11]\n",
    "            if lines:\n",
    "                from io import StringIO\n",
    "                df = pd.read_csv(StringIO(\"\\n\".join(lines)), sep=\"\\t\", header=None, dtype=str)\n",
    "                if df.shape[1] >= 12:\n",
    "                    df = df.iloc[:, :12]\n",
    "                    df.columns = cols12\n",
    "                    return df\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def extract_species_from_title(title):\n",
    "    if not isinstance(title, str) or not title.strip():\n",
    "        return None\n",
    "    title = title.strip()\n",
    "    # 1) binomial: Genus species (Genus capitalized, species lowercase)\n",
    "    m = re.search(r'\\b([A-Z][a-zA-Z\\-]+)\\s+([a-z][a-zA-Z\\-\\(\\)]+)\\b', title)\n",
    "    if m:\n",
    "        return f\"{m.group(1)} {m.group(2)}\"\n",
    "    # 2) Genus sp. or Genus sp\n",
    "    m2 = re.search(r'\\b([A-Z][a-zA-Z\\-]+)\\s+sp\\b', title)\n",
    "    if m2:\n",
    "        return f\"{m2.group(1)} sp.\"\n",
    "    # 3) at least a capitalized Genus word (be conservative: return None in this case)\n",
    "    # Avoid guessing from e.g. 'uncultured bacterium' etc.\n",
    "    return None\n",
    "\n",
    "# --- main ---\n",
    "print(\"Searching for 'extracted/' folder...\")\n",
    "extracted = find_extracted_folder()\n",
    "if extracted is None:\n",
    "    print(\"ERROR: could not find an 'extracted' directory automatically. Please run the earlier extraction cell or set EXTRACT_DIR to the correct path.\")\n",
    "else:\n",
    "    print(\"Using extracted folder:\", extracted)\n",
    "\n",
    "if extracted is None:\n",
    "    # stop gracefully\n",
    "    print(\"No extracted/ found. Aborting this cell (nothing changed).\")\n",
    "else:\n",
    "    # find predictions CSV\n",
    "    pred_candidates = [\"predictions_with_uncertainty.csv\",\"predictions.csv\",\"predictions_blast_enriched.csv\"]\n",
    "    pred_file = None\n",
    "    for name in pred_candidates:\n",
    "        p = extracted / name\n",
    "        if p.exists():\n",
    "            pred_file = p\n",
    "            break\n",
    "    if pred_file is None:\n",
    "        # try any CSV with 'predictions' in name\n",
    "        preds = list(extracted.glob(\"*predic*.csv\"))\n",
    "        if preds:\n",
    "            pred_file = preds[0]\n",
    "    if pred_file is None:\n",
    "        print(\"ERROR: cannot find predictions CSV in extracted/. Expected file e.g. predictions_with_uncertainty.csv\")\n",
    "        print(\"Files in extracted/:\")\n",
    "        for f in sorted(extracted.iterdir()):\n",
    "            print(\" \", f.name)\n",
    "        print(\"\\nAborting — nothing changed.\")\n",
    "    else:\n",
    "        print(\"Loading predictions:\", pred_file.name)\n",
    "        try:\n",
    "            pred = pd.read_csv(pred_file, dtype=str)\n",
    "        except Exception as e:\n",
    "            print(\"Failed to read predictions CSV:\", e)\n",
    "            pred = pd.read_csv(pred_file, engine='python', dtype=str, error_bad_lines=False)\n",
    "\n",
    "        # determine core columns robustly\n",
    "        species_col = choose_species_column(pred)\n",
    "        conf_col = choose_confidence_column(pred, species_col)\n",
    "        id_col = choose_id_column(pred)\n",
    "\n",
    "        print(\"Detected columns:\")\n",
    "        print(\" - species column  ->\", species_col)\n",
    "        print(\" - confidence col  ->\", conf_col)\n",
    "        print(\" - id column       ->\", id_col)\n",
    "\n",
    "        # find blast results file\n",
    "        blast_path = find_blast_file(extracted)\n",
    "        if blast_path is None:\n",
    "            print(\"\\nNo BLAST results file found in extracted/. Searching for files named like 'blast*.tsv' etc returned nothing.\")\n",
    "            # helpful suggestions\n",
    "            print(\"If you already ran BLAST, place the BLAST tabular (outfmt 6) file named 'blast_results.tsv' into:\")\n",
    "            print(\"  \", extracted)\n",
    "            print(\"\\nIf you haven't run BLAST, you can run (local blastn or remote NCBI - remote may be slow and has usage limits).\")\n",
    "            print(\"Example local command (blast+):\\n  blastn -query all_unassigned_for_blast.fasta -db nt -outfmt '6 qseqid sseqid pident length qlen qstart qend sstart send evalue bitscore stitle' -max_target_seqs 5 -evalue 1e-10 -out blast_results.tsv -num_threads 8\")\n",
    "            print(\"\\nAfter you run BLAST and put blast_results.tsv into the extracted/ folder, re-run this cell.\")\n",
    "        else:\n",
    "            print(\"Found BLAST file:\", blast_path.name, \"(parsing...)\")\n",
    "            dfb = parse_blast_tsv(blast_path)\n",
    "            if dfb is None:\n",
    "                print(\"Could not parse BLAST file automatically. Inspect file and ensure it is BLAST tabular (-outfmt 6) with columns:\")\n",
    "                print(\"  qseqid sseqid pident length qlen qstart qend sstart send evalue bitscore stitle\")\n",
    "                print(\"Aborting (no changes).\")\n",
    "            else:\n",
    "                # coerce numeric columns\n",
    "                for col in (\"pident\",\"length\",\"qlen\",\"evalue\",\"bitscore\"):\n",
    "                    if col in dfb.columns:\n",
    "                        dfb[col] = pd.to_numeric(dfb[col], errors='coerce').fillna(0.0)\n",
    "                # compute coverage safely\n",
    "                if \"qlen\" in dfb.columns and \"length\" in dfb.columns:\n",
    "                    dfb[\"coverage_pct\"] = dfb.apply(lambda r: (float(r[\"length\"])/float(r[\"qlen\"])*100.0) if (pd.notna(r[\"qlen\"]) and float(r[\"qlen\"])>0) else 0.0, axis=1)\n",
    "                else:\n",
    "                    dfb[\"coverage_pct\"] = 0.0\n",
    "\n",
    "                # conservative filtering\n",
    "                pre_len = len(dfb)\n",
    "                dfb_high = dfb[(dfb[\"pident\"] >= MIN_PID) & (dfb[\"coverage_pct\"] >= MIN_COV) & (dfb[\"evalue\"] <= MAX_EVAL)].copy()\n",
    "                print(f\"BLAST rows total={pre_len}, passing thresholds (pident>={MIN_PID}, cov>={MIN_COV}, e<={MAX_EVAL}) = {len(dfb_high)}\")\n",
    "\n",
    "                if dfb_high.empty:\n",
    "                    print(\"No BLAST hits passed thresholds. Nothing will be applied.\")\n",
    "                    # still save a tiny audit for the user\n",
    "                    try:\n",
    "                        dfb.to_csv(extracted/\"blast_parsed_all.csv\", index=False)\n",
    "                        print(\"Saved parsed BLAST (unfiltered) -> blast_parsed_all.csv\")\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                else:\n",
    "                    # pick top hit per qseqid by bitscore (most reliable)\n",
    "                    dfb_top = dfb_high.sort_values([\"qseqid\",\"bitscore\"], ascending=[True, False]).groupby(\"qseqid\", as_index=False).first()\n",
    "                    print(\"Unique queries with accepted top-hit:\", len(dfb_top))\n",
    "\n",
    "                    # extract species names conservatively\n",
    "                    assignments = {}\n",
    "                    for _, row in dfb_top.iterrows():\n",
    "                        q = str(row[\"qseqid\"])\n",
    "                        stitle = row.get(\"stitle\", \"\")\n",
    "                        species = extract_species_from_title(stitle)\n",
    "                        if species is None:\n",
    "                            # do not guess from non-binomial titles\n",
    "                            continue\n",
    "                        assignments[q] = {\n",
    "                            \"species\": species,\n",
    "                            \"pident\": float(row[\"pident\"]),\n",
    "                            \"coverage\": float(row[\"coverage_pct\"]),\n",
    "                            \"evalue\": float(row[\"evalue\"]),\n",
    "                            \"sseqid\": row.get(\"sseqid\"),\n",
    "                            \"stitle\": stitle\n",
    "                        }\n",
    "                    print(\"Conservative, parsed BLAST-derived species assignments:\", len(assignments))\n",
    "                    # save assignments for audit\n",
    "                    assign_df = pd.DataFrame([{\"query_id\":q, **v} for q,v in assignments.items()])\n",
    "                    assign_out = extracted / \"blast_species_assignments.csv\"\n",
    "                    assign_df.to_csv(assign_out, index=False)\n",
    "                    print(\"Wrote BLAST assignments ->\", assign_out.name)\n",
    "\n",
    "                    # prepare predictions (do not overwrite original file)\n",
    "                    pred2 = pred.copy()\n",
    "                    # ensure id and species columns exist\n",
    "                    if species_col is None:\n",
    "                        # create one if missing\n",
    "                        species_col = \"species_pred_label\"\n",
    "                        pred2[species_col] = \"\"\n",
    "                    if id_col not in pred2.columns:\n",
    "                        print(f\"ERROR: Could not detect id column in predictions to map BLAST qseqid to rows. Columns available: {list(pred2.columns)}\")\n",
    "                        print(\"Aborting apply. You can still inspect blast_species_assignments.csv manually.\")\n",
    "                    else:\n",
    "                        # normalized current species label string\n",
    "                        pred2[\"_species_norm\"] = pred2[species_col].astype(str).fillna(\"\").apply(lambda s: \" \".join(str(s).split()))\n",
    "                        mask_unassigned = pred2[\"_species_norm\"].str.upper().isin([\"UNASSIGNED\", \"\", \"NONE\", \"NAN\"])\n",
    "                        n_unassigned_before = int(mask_unassigned.sum())\n",
    "                        print(\"UNASSIGNED rows before:\", n_unassigned_before)\n",
    "\n",
    "                        # try multiple matching heuristics; only change rows currently UNASSIGNED\n",
    "                        rows_to_change = []  # (row_index, new_species, q, info)\n",
    "                        for q, info in assignments.items():\n",
    "                            # Exact match\n",
    "                            exact_mask = pred2[id_col].astype(str) == str(q)\n",
    "                            if exact_mask.any():\n",
    "                                for ri in pred2.index[exact_mask]:\n",
    "                                    if pred2.at[ri, \"_species_norm\"].upper() in (\"UNASSIGNED\",\"\", \"NONE\", \"NAN\"):\n",
    "                                        rows_to_change.append((ri, info[\"species\"], q, info))\n",
    "                                continue\n",
    "                            # Versionless match: strip .1 etc\n",
    "                            q_base = q.split('.')[0]\n",
    "                            try:\n",
    "                                id_base = pred2[id_col].astype(str).str.split('.').str[0]\n",
    "                                mask_base = id_base == q_base\n",
    "                                if mask_base.any():\n",
    "                                    for ri in pred2.index[mask_base]:\n",
    "                                        if pred2.at[ri, \"_species_norm\"].upper() in (\"UNASSIGNED\",\"\", \"NONE\", \"NAN\"):\n",
    "                                            rows_to_change.append((ri, info[\"species\"], q, info))\n",
    "                                    continue\n",
    "                            except Exception:\n",
    "                                pass\n",
    "                            # substring match (conservative)\n",
    "                            try:\n",
    "                                mask_sub = pred2[id_col].astype(str).str.contains(str(q), na=False)\n",
    "                                if mask_sub.any():\n",
    "                                    for ri in pred2.index[mask_sub]:\n",
    "                                        if pred2.at[ri, \"_species_norm\"].upper() in (\"UNASSIGNED\",\"\", \"NONE\", \"NAN\"):\n",
    "                                            rows_to_change.append((ri, info[\"species\"], q, info))\n",
    "                                    continue\n",
    "                            except Exception:\n",
    "                                pass\n",
    "                            # no match -> skip\n",
    "\n",
    "                        # de-duplicate picking best pident/coverage if multiple assignments for same row\n",
    "                        row_best = {}\n",
    "                        for ri, new_sp, q, info in rows_to_change:\n",
    "                            key = int(ri)\n",
    "                            if key not in row_best:\n",
    "                                row_best[key] = (new_sp, q, info)\n",
    "                            else:\n",
    "                                existing = row_best[key][2]\n",
    "                                # prefer higher pident, then higher coverage\n",
    "                                if info.get(\"pident\",0) > existing.get(\"pident\",0) or (info.get(\"pident\",0)==existing.get(\"pident\",0) and info.get(\"coverage\",0) > existing.get(\"coverage\",0)):\n",
    "                                    row_best[key] = (new_sp, q, info)\n",
    "                        final_rows = sorted([(ri, *row_best[ri]) for ri in row_best])\n",
    "                        n_candidates = len(final_rows)\n",
    "                        print(\"Candidate UNASSIGNED rows matching BLAST assignments:\", n_candidates)\n",
    "\n",
    "                        if n_candidates == 0:\n",
    "                            print(\"No UNASSIGNED rows matched conservative BLAST assignments (nothing applied).\")\n",
    "                        else:\n",
    "                            # apply if user wants\n",
    "                            if APPLY_ASSIGNMENTS:\n",
    "                                for ri, new_sp, q, info in final_rows:\n",
    "                                    pred2.at[ri, species_col] = new_sp\n",
    "                                    pred2.at[ri, \"blast_assigned_species\"] = new_sp\n",
    "                                    pred2.at[ri, \"blast_assigned_query\"] = q\n",
    "                                    pred2.at[ri, \"blast_pident\"] = info[\"pident\"]\n",
    "                                    pred2.at[ri, \"blast_coverage\"] = info[\"coverage\"]\n",
    "                                    pred2.at[ri, \"blast_evalue\"] = info[\"evalue\"]\n",
    "                                    pred2.at[ri, \"blast_sseqid\"] = info.get(\"sseqid\")\n",
    "                                    pred2.at[ri, \"blast_stitle\"] = info.get(\"stitle\")\n",
    "                                out_forced = extracted / \"predictions_after_blast_forced.csv\"\n",
    "                                pred2.to_csv(out_forced, index=False)\n",
    "                                print(f\"Applied BLAST assignments to {len(final_rows)} UNASSIGNED rows -> saved {out_forced.name}\")\n",
    "                            else:\n",
    "                                print(f\"APPLY_ASSIGNMENTS=False; {len(final_rows)} candidate rows identified but not applied.\")\n",
    "\n",
    "                        # always write enriched copy (with BLAST columns added)\n",
    "                        out_enriched = extracted / \"predictions_blast_enriched.csv\"\n",
    "                        pred2.to_csv(out_enriched, index=False)\n",
    "                        print(\"Wrote enriched predictions (audit) ->\", out_enriched.name)\n",
    "\n",
    "                        # recompute abundance counts from updated predictions\n",
    "                        pred2[\"_species_norm_final\"] = pred2[species_col].astype(str).fillna(\"\").apply(lambda s: \" \".join(str(s).split()))\n",
    "                        counts = pred2[\"_species_norm_final\"].value_counts(dropna=False).reset_index()\n",
    "                        counts.columns = [\"species\",\"count\"]\n",
    "                        counts[\"count\"] = pd.to_numeric(counts[\"count\"], errors='coerce').fillna(0).astype(int)\n",
    "                        total = counts[\"count\"].sum()\n",
    "                        counts[\"rel\"] = counts[\"count\"] / total if total>0 else 0.0\n",
    "                        out_abund = extracted / \"abundance_after_blast.csv\"\n",
    "                        counts.to_csv(out_abund, index=False)\n",
    "                        print(\"Wrote abundance_after_blast.csv ->\", out_abund.name)\n",
    "                        print(\"\\nTop species after applying BLAST assignments (top 20):\")\n",
    "                        print(counts.head(20).to_string(index=False))\n",
    "\n",
    "                        n_unassigned_after = int((pred2[\"_species_norm_final\"].str.upper().isin([\"UNASSIGNED\",\"\", \"NONE\", \"NAN\"])).sum())\n",
    "                        print(f\"\\nSUMMARY: UNASSIGNED before = {n_unassigned_before}; UNASSIGNED after = {n_unassigned_after}; filled = {n_unassigned_before - n_unassigned_after}\")\n",
    "\n",
    "print(\"\\nCell finished. Check the extracted/ folder for generated audit files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e0736a44-8f2b-451f-a951-6db252e8330f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using extracted folder: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\n",
      "Loading predictions: predictions_with_uncertainty.csv\n",
      "Detected columns -> species: species_pred_label | conf: kingdom_pred_conf | novel: species_novel_component | id: global_index\n",
      "UNASSIGNED before autofill (after BLAST step): 144\n",
      "Adaptive thresholds -> conf >= 0.980, novel <= 0.258\n",
      "Auto-fill candidate rows meeting data-driven criteria: 0\n",
      "No auto-fill candidates met the conservative, adaptive criteria. No per-read labels changed by autofill.\n",
      "No assignments were applied (no audit file created).\n",
      "Wrote enriched predictions (audit copy) -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\predictions_after_autofill.csv\n",
      "Wrote post-fill abundances -> abundance_from_predictions_after_autofill.csv and abundance_from_predictions_weighted_after_autofill.csv\n",
      "Confusion matrix empty -> skipping deconvolution.\n",
      "\n",
      "SUMMARY:\n",
      " UNASSIGNED before (after BLAST step): 144\n",
      " Assigned from BLAST: 0\n",
      " Assigned from auto-fill: 0\n",
      " UNASSIGNED after processing: 144\n",
      "\n",
      "Files written to: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\n",
      " - predictions_after_autofill.csv\n",
      " - abundance_from_predictions_after_autofill.csv\n",
      " - abundance_from_predictions_weighted_after_autofill.csv\n",
      " - abundance_from_predictions_deconvolved.csv\n",
      " - abundance_reconciled_species.csv\n",
      "\n",
      "If you want stricter behavior (only BLAST, no autofill) re-run this cell after placing blast_results.tsv into extracted/.\n"
     ]
    }
   ],
   "source": [
    "# === Robust single cell to FIX UNASSIGNED reads (BLAST if present; otherwise safe auto-fill) ===\n",
    "# - If blast_results.tsv is present in extracted/, it uses BLAST assignments (strict).\n",
    "# - Otherwise it computes an adaptive confidence threshold (from existing assigned reads)\n",
    "#   and conservatively fills a subset of UNASSIGNED rows with the model's own predicted species.\n",
    "# - Writes audit files and recomputes abundance CSVs (raw, weighted, deconvolved/reconciled).\n",
    "# - Non-destructive: original predictions CSV is not overwritten.\n",
    "# Paste & run in the same notebook where your earlier cells ran.\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, sys, math, time\n",
    "\n",
    "def find_extracted():\n",
    "    cand = [\n",
    "        Path.cwd() / \"sih\" / \"ncbi_blast_db\" / \"extracted\",\n",
    "        Path.cwd() / \"ncbi_blast_db\" / \"extracted\",\n",
    "        Path.cwd() / \"extracted\",\n",
    "        Path(r\"C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\"),\n",
    "        Path(r\"C:\\Users\\Srijit\\OneDrive\\Desktop\\sihtaxa\\sihabundance\\ncbi_blast_db\\extracted\"),\n",
    "    ]\n",
    "    for p in cand:\n",
    "        if p.exists() and p.is_dir():\n",
    "            if (p/\"predictions_with_uncertainty.csv\").exists() or (p/\"predictions.csv\").exists():\n",
    "                return p.resolve()\n",
    "    # recursive fallback\n",
    "    for p in Path.cwd().rglob(\"predictions_with_uncertainty.csv\"):\n",
    "        return p.parent.resolve()\n",
    "    for p in Path.cwd().rglob(\"predictions.csv\"):\n",
    "        return p.parent.resolve()\n",
    "    return None\n",
    "\n",
    "def detect_species_conf_cols(df):\n",
    "    low = {c.lower(): c for c in df.columns}\n",
    "    # species label detection: prefer *_pred_label or explicit names\n",
    "    for pat in (\"species_pred_label\", \"_pred_label\", \"species_label\", \"species_pred\"):\n",
    "        for c in df.columns:\n",
    "            if pat in c.lower():\n",
    "                # ensure it's not an index column (like species_pred_idx)\n",
    "                if \"idx\" in c.lower() or c.lower().endswith(\"_idx\"):\n",
    "                    continue\n",
    "                return c\n",
    "    # fallback: any non-numeric column that contains 'species'\n",
    "    for c in df.columns:\n",
    "        if \"species\" in c.lower():\n",
    "            if not pd.api.types.is_numeric_dtype(df[c]):\n",
    "                return c\n",
    "    # last resort: pick the column with 'species' even if numeric\n",
    "    for c in df.columns:\n",
    "        if \"species\" in c.lower():\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def detect_confidence_col(df):\n",
    "    for c in df.columns:\n",
    "        lc = c.lower()\n",
    "        if \"species_pred_conf\" in lc or \"species_conf\" in lc or \"pred_conf\" in lc or lc.endswith(\"_conf\"):\n",
    "            return c\n",
    "    # fallback: something like 'species_mc_mean_topprob' or 'mc_mean_topprob'\n",
    "    for c in df.columns:\n",
    "        if \"mc_mean_topprob\" in c.lower() or \"topprob\" in c.lower() or \"prob\" in c.lower():\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def detect_novel_col(df):\n",
    "    for c in df.columns:\n",
    "        if \"novel\" in c.lower() or \"novel_component\" in c.lower() or \"novelty\" in c.lower():\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def nnls_projected_gradient(M, b, max_iter=5000, tol=1e-6, verbose=False):\n",
    "    # M shape: (m x n)? we will follow earlier convention M = A^T\n",
    "    try:\n",
    "        x0 = np.linalg.lstsq(M, b, rcond=None)[0]\n",
    "        x = np.maximum(0.0, x0)\n",
    "    except Exception:\n",
    "        x = np.maximum(0.0, np.ones(M.shape[1]) * (b.sum() / max(1, M.shape[1])))\n",
    "    # estimate Lipschitz L\n",
    "    try:\n",
    "        s = np.linalg.svd(M, compute_uv=False)\n",
    "        L = (s[0] ** 2) if s.size>0 else (np.linalg.norm(M, ord=2)**2)\n",
    "    except Exception:\n",
    "        L = (np.linalg.norm(M, ord=2)**2) + 1e-8\n",
    "    lr = 1.0 / (L + 1e-12)\n",
    "    prev = None\n",
    "    for it in range(max_iter):\n",
    "        r = M.dot(x) - b\n",
    "        grad = M.T.dot(r)\n",
    "        x = x - lr * grad\n",
    "        x = np.maximum(0.0, x)\n",
    "        rnorm = np.linalg.norm(r)\n",
    "        if prev is not None and abs(prev - rnorm) < tol:\n",
    "            if verbose: print(\"nnls conv iter\", it, \"rnorm\", rnorm)\n",
    "            break\n",
    "        prev = rnorm\n",
    "    return x\n",
    "\n",
    "# --- main driver ---\n",
    "extracted = find_extracted()\n",
    "if extracted is None:\n",
    "    print(\"Could not locate an 'extracted/' folder with predictions. Make sure you ran earlier cells and that outputs exist.\")\n",
    "else:\n",
    "    print(\"Using extracted folder:\", extracted)\n",
    "    # find predictions\n",
    "    preds_path = extracted / \"predictions_with_uncertainty.csv\"\n",
    "    if not preds_path.exists():\n",
    "        preds_path = extracted / \"predictions.csv\" if (extracted / \"predictions.csv\").exists() else None\n",
    "    if preds_path is None:\n",
    "        print(\"No predictions CSV found in extracted/. Nothing to do.\")\n",
    "    else:\n",
    "        print(\"Loading predictions:\", preds_path.name)\n",
    "        preds = pd.read_csv(preds_path)\n",
    "        # detect columns\n",
    "        species_col = detect_species_conf_cols(preds)\n",
    "        conf_col    = detect_confidence_col(preds)\n",
    "        novel_col   = detect_novel_col(preds)\n",
    "        id_col = next((c for c in preds.columns if c.lower() in (\"id\",\"accession\",\"seqid\",\"read_id\",\"readid\",\"accession_id\",\"global_index\")), None)\n",
    "        print(\"Detected columns -> species:\", species_col, \"| conf:\", conf_col, \"| novel:\", novel_col, \"| id:\", id_col)\n",
    "\n",
    "        # Normalize species string for checking UNASSIGNED (keep original column untouched)\n",
    "        preds[\"_species_norm\"] = preds[species_col].astype(str).fillna(\"UNASSIGNED\").apply(lambda s: \" \".join(str(s).split()))\n",
    "        # numeric confidence\n",
    "        if conf_col is not None:\n",
    "            preds[\"_conf_num\"] = pd.to_numeric(preds[conf_col], errors=\"coerce\").fillna(0.0)\n",
    "        else:\n",
    "            preds[\"_conf_num\"] = 0.0\n",
    "\n",
    "        # If blast_results.tsv exists, use BLAST-based strict assignment first\n",
    "        blast_file = extracted / \"blast_results.tsv\"\n",
    "        applied_from_blast = 0\n",
    "        applied_from_autofill = 0\n",
    "        applied_rows = []\n",
    "\n",
    "        if blast_file.exists():\n",
    "            print(\"blast_results.tsv found -> using BLAST-based strict assignments first.\")\n",
    "            # reuse earlier robust BLAST parsing logic (conservative thresholds)\n",
    "            cols = [\"qseqid\",\"sseqid\",\"pident\",\"length\",\"qlen\",\"qstart\",\"qend\",\"sstart\",\"send\",\"evalue\",\"bitscore\",\"stitle\"]\n",
    "            try:\n",
    "                dfb = pd.read_csv(blast_file, sep=\"\\t\", names=cols, header=None, quoting=3, dtype={\"stitle\":str})\n",
    "            except Exception as e:\n",
    "                print(\"Failed to read blast_results.tsv:\", e)\n",
    "                dfb = None\n",
    "            if dfb is not None and not dfb.empty:\n",
    "                dfb[\"pident\"] = pd.to_numeric(dfb[\"pident\"], errors=\"coerce\").fillna(0.0)\n",
    "                dfb[\"length\"] = pd.to_numeric(dfb[\"length\"], errors=\"coerce\").fillna(0.0)\n",
    "                dfb[\"qlen\"] = pd.to_numeric(dfb[\"qlen\"], errors=\"coerce\").replace(0, np.nan)\n",
    "                dfb[\"coverage_pct\"] = dfb.apply(lambda r: (r[\"length\"]/r[\"qlen\"]*100.0) if (pd.notna(r[\"qlen\"]) and r[\"qlen\"]>0) else 0.0, axis=1)\n",
    "                dfb[\"evalue\"] = pd.to_numeric(dfb[\"evalue\"], errors=\"coerce\").fillna(1e6)\n",
    "                # conservative filter:\n",
    "                dfb_high = dfb[(dfb[\"pident\"]>=97.0) & (dfb[\"coverage_pct\"]>=80.0) & (dfb[\"evalue\"]<=1e-6)].copy()\n",
    "                if not dfb_high.empty:\n",
    "                    dfb_top = dfb_high.sort_values([\"qseqid\",\"bitscore\"], ascending=[True,False]).groupby(\"qseqid\", as_index=False).first()\n",
    "                    def extract_species_from_title(title):\n",
    "                        if not isinstance(title, str) or not title.strip(): return None\n",
    "                        m = re.search(r'\\b([A-Z][a-z]+(?:\\s+[a-z][a-z\\-]+))\\b', title)\n",
    "                        if m: return m.group(1).strip()\n",
    "                        m2 = re.search(r'\\b([A-Z][a-z]+)\\s+sp\\b', title)\n",
    "                        if m2: return m2.group(0).strip()\n",
    "                        return None\n",
    "                    assignments = {}\n",
    "                    for _,r in dfb_top.iterrows():\n",
    "                        q = str(r[\"qseqid\"]); s = extract_species_from_title(r[\"stitle\"])\n",
    "                        if s: assignments[q] = {\"species\": s, \"pident\": float(r[\"pident\"]), \"coverage\": float(r[\"coverage_pct\"]), \"sseqid\": r[\"sseqid\"], \"stitle\": r[\"stitle\"]}\n",
    "                    # map assignments to prediction rows by id column (exact or substring) and apply only to UNASSIGNED\n",
    "                    if assignments:\n",
    "                        if id_col is None:\n",
    "                            print(\"Cannot map BLAST query IDs to prediction rows: no id-like column in predictions.\")\n",
    "                        else:\n",
    "                            for q,info in assignments.items():\n",
    "                                # exact\n",
    "                                mask_exact = preds[id_col].astype(str) == str(q)\n",
    "                                if mask_exact.any():\n",
    "                                    for ri in preds.index[mask_exact]:\n",
    "                                        if preds.at[ri, \"_species_norm\"].upper()==\"UNASSIGNED\":\n",
    "                                            preds.at[ri, species_col] = info[\"species\"]\n",
    "                                            preds.at[ri, \"blast_assigned_species\"] = info[\"species\"]\n",
    "                                            preds.at[ri, \"blast_assigned_query\"] = q\n",
    "                                            preds.at[ri, \"blast_pident\"] = info[\"pident\"]\n",
    "                                            preds.at[ri, \"blast_coverage\"] = info[\"coverage\"]\n",
    "                                            preds.at[ri, \"blast_sseqid\"] = info[\"sseqid\"]\n",
    "                                            preds.at[ri, \"blast_stitle\"] = info[\"stitle\"]\n",
    "                                            applied_from_blast += 1\n",
    "                                            applied_rows.append((ri, q, info[\"species\"], \"blast\"))\n",
    "                                    continue\n",
    "                                # substring\n",
    "                                mask_sub = preds[id_col].astype(str).str.contains(str(q), na=False)\n",
    "                                if mask_sub.any():\n",
    "                                    for ri in preds.index[mask_sub]:\n",
    "                                        if preds.at[ri, \"_species_norm\"].upper()==\"UNASSIGNED\":\n",
    "                                            preds.at[ri, species_col] = info[\"species\"]\n",
    "                                            preds.at[ri, \"blast_assigned_species\"] = info[\"species\"]\n",
    "                                            preds.at[ri, \"blast_assigned_query\"] = q\n",
    "                                            preds.at[ri, \"blast_pident\"] = info[\"pident\"]\n",
    "                                            preds.at[ri, \"blast_coverage\"] = info[\"coverage\"]\n",
    "                                            preds.at[ri, \"blast_sseqid\"] = info[\"sseqid\"]\n",
    "                                            preds.at[ri, \"blast_stitle\"] = info[\"stitle\"]\n",
    "                                            applied_from_blast += 1\n",
    "                                            applied_rows.append((ri, q, info[\"species\"], \"blast\"))\n",
    "                else:\n",
    "                    print(\"No high-confidence BLAST hits passed the conservative thresholds. Will try auto-fill below.\")\n",
    "            else:\n",
    "                print(\"No BLAST rows loaded. Will try auto-fill below.\")\n",
    "\n",
    "        # --- If BLAST did not fill everything, perform data-driven safe autofill of remaining UNASSIGNED ---\n",
    "        # Compute how many UNASSIGNED remain\n",
    "        preds[\"_species_norm\"] = preds[species_col].astype(str).fillna(\"UNASSIGNED\").apply(lambda s: \" \".join(str(s).split()))\n",
    "        n_unassigned_before = int((preds[\"_species_norm\"].str.upper()==\"UNASSIGNED\").sum())\n",
    "        print(\"UNASSIGNED before autofill (after BLAST step):\", n_unassigned_before)\n",
    "        # Recompute conf numbers if not present\n",
    "        if conf_col and conf_col in preds.columns:\n",
    "            preds[\"_conf_num\"] = pd.to_numeric(preds[conf_col], errors=\"coerce\").fillna(0.0)\n",
    "        else:\n",
    "            # check for any _prob-like column\n",
    "            alt = next((c for c in preds.columns if \"prob\" in c.lower() or \"topprob\" in c.lower()), None)\n",
    "            if alt:\n",
    "                preds[\"_conf_num\"] = pd.to_numeric(preds[alt], errors=\"coerce\").fillna(0.0)\n",
    "                print(\"Using alternative probability column for confidence:\", alt)\n",
    "            else:\n",
    "                preds[\"_conf_num\"] = 0.0\n",
    "\n",
    "        # if there are still UNASSIGNED rows, do adaptive thresholding\n",
    "        remaining_unassigned_mask = preds[\"_species_norm\"].str.upper()==\"UNASSIGNED\"\n",
    "        if remaining_unassigned_mask.sum() > 0:\n",
    "            # Determine adaptive confidence threshold from assigned rows\n",
    "            assigned_mask = preds[\"_species_norm\"].str.upper() != \"UNASSIGNED\"\n",
    "            assigned_conf = preds.loc[assigned_mask, \"_conf_num\"].dropna().astype(float)\n",
    "            if len(assigned_conf) >= 5:\n",
    "                conf_threshold = float(np.percentile(assigned_conf, 90))  # top 10% of assigned confidences\n",
    "                conf_threshold = max(conf_threshold, 0.5)  # ensure not ridiculously low\n",
    "            else:\n",
    "                # fallback when few assigned rows: use 0.65 (data-driven fallback)\n",
    "                conf_threshold = 0.65\n",
    "            # Novel threshold if present (use median of novel among all rows or a conservative small value)\n",
    "            novel_col = novel_col  # already detected\n",
    "            if novel_col and novel_col in preds.columns:\n",
    "                preds[\"_novel_num\"] = pd.to_numeric(preds[novel_col], errors=\"coerce\").fillna(1.0)\n",
    "                # choose median of _novel_num (lower means less novel)\n",
    "                novel_threshold = float(np.percentile(preds[\"_novel_num\"].dropna(), 50))\n",
    "            else:\n",
    "                preds[\"_novel_num\"] = 1.0\n",
    "                novel_threshold = 0.5  # conservative default if no signal\n",
    "            print(f\"Adaptive thresholds -> conf >= {conf_threshold:.3f}, novel <= {novel_threshold:.3f}\")\n",
    "\n",
    "            # Candidate criteria: UNASSIGNED AND predicted species is not 'UNASSIGNED' AND conf >= threshold AND novel <= threshold\n",
    "            # Detect predicted species column (use same species_col)\n",
    "            candidates_mask = remaining_unassigned_mask & (preds[species_col].astype(str).str.upper() != \"UNASSIGNED\") & \\\n",
    "                              (preds[\"_conf_num\"] >= conf_threshold) & (preds[\"_novel_num\"] <= novel_threshold)\n",
    "            cand_count = int(candidates_mask.sum())\n",
    "            print(\"Auto-fill candidate rows meeting data-driven criteria:\", cand_count)\n",
    "            if cand_count > 0:\n",
    "                # Apply the autofill\n",
    "                for ri in preds.index[candidates_mask]:\n",
    "                    new_sp = str(preds.at[ri, species_col])\n",
    "                    if preds.at[ri, \"_species_norm\"].upper()==\"UNASSIGNED\" and new_sp.strip() and new_sp.strip().upper()!=\"UNASSIGNED\":\n",
    "                        preds.at[ri, species_col] = new_sp\n",
    "                        preds.at[ri, \"autofill_assigned_species\"] = new_sp\n",
    "                        preds.at[ri, \"autofill_conf\"] = float(preds.at[ri, \"_conf_num\"])\n",
    "                        preds.at[ri, \"autofill_novel\"] = float(preds.at[ri, \"_novel_num\"])\n",
    "                        applied_from_autofill += 1\n",
    "                        applied_rows.append((ri, preds.at[ri, id_col] if id_col else ri, new_sp, \"autofill\"))\n",
    "            else:\n",
    "                print(\"No auto-fill candidates met the conservative, adaptive criteria. No per-read labels changed by autofill.\")\n",
    "        else:\n",
    "            print(\"No UNASSIGNED rows remain after BLAST step.\")\n",
    "\n",
    "        # Write audit of changed rows (if any)\n",
    "        audit_rows = []\n",
    "        for (ri, q, new_sp, how) in applied_rows:\n",
    "            old = preds.at[ri, \"_species_norm\"]\n",
    "            audit_rows.append({\n",
    "                \"row_index\": int(ri),\n",
    "                \"id\": preds.at[ri, id_col] if id_col else \"\",\n",
    "                \"old_species\": old,\n",
    "                \"new_species\": new_sp,\n",
    "                \"method\": how,\n",
    "                \"conf\": float(preds.at[ri, \"_conf_num\"]) if \"_conf_num\" in preds.columns else None,\n",
    "                \"novel\": float(preds.at[ri, \"_novel_num\"]) if \"_novel_num\" in preds.columns else None\n",
    "            })\n",
    "        if audit_rows:\n",
    "            audit_df = pd.DataFrame(audit_rows)\n",
    "            out_audit = extracted / \"autofill_assignment_audit.csv\"\n",
    "            audit_df.to_csv(out_audit, index=False)\n",
    "            print(\"Wrote audit of applied assignments ->\", out_audit, \" (rows changed:\", len(audit_df), \")\")\n",
    "        else:\n",
    "            print(\"No assignments were applied (no audit file created).\")\n",
    "\n",
    "        # Save enriched predictions (audit copy)\n",
    "        enriched_out = extracted / \"predictions_after_autofill.csv\"\n",
    "        preds.to_csv(enriched_out, index=False)\n",
    "        print(\"Wrote enriched predictions (audit copy) ->\", enriched_out)\n",
    "\n",
    "        # Recompute abundances from updated predictions (raw + weighted)\n",
    "        preds[\"_species_norm_final\"] = preds[species_col].astype(str).fillna(\"\").apply(lambda s: \" \".join(str(s).split()))\n",
    "        counts = preds[\"_species_norm_final\"].value_counts().reset_index()\n",
    "        counts.columns = [\"species\",\"count\"]\n",
    "        counts[\"relative_abundance\"] = counts[\"count\"] / counts[\"count\"].sum() if counts[\"count\"].sum()>0 else 0.0\n",
    "        out_raw = extracted / \"abundance_from_predictions_after_autofill.csv\"\n",
    "        counts.to_csv(out_raw, index=False)\n",
    "        # weighted\n",
    "        preds[\"_conf_num\"] = preds[\"_conf_num\"].fillna(0.0)\n",
    "        weighted = preds.groupby(\"_species_norm_final\")[\"_conf_num\"].sum().reset_index().rename(columns={\"_conf_num\":\"conf_sum\"})\n",
    "        total_conf = weighted[\"conf_sum\"].sum() if weighted[\"conf_sum\"].sum()>0 else 1.0\n",
    "        weighted[\"relative_abundance_weighted\"] = weighted[\"conf_sum\"] / total_conf\n",
    "        out_weight = extracted / \"abundance_from_predictions_weighted_after_autofill.csv\"\n",
    "        weighted.to_csv(out_weight, index=False)\n",
    "        print(\"Wrote post-fill abundances ->\", out_raw.name, \"and\", out_weight.name)\n",
    "\n",
    "        # Attempt confusion-aware deconvolution with validation preds if available (same as earlier)\n",
    "        val_path = None\n",
    "        for cand in (extracted / \"val_predictions_calibrated.csv\", extracted / \"val_predictions.csv\"):\n",
    "            if cand.exists(): val_path = cand; break\n",
    "        if val_path is None:\n",
    "            for p in extracted.rglob(\"val_predictions*\"):\n",
    "                if p.is_file(): val_path = p; break\n",
    "\n",
    "        if val_path is None:\n",
    "            print(\"No validation predictions found; skipping confusion-based deconvolution.\")\n",
    "        else:\n",
    "            try:\n",
    "                val = pd.read_csv(val_path)\n",
    "                # detect true/pred columns heuristically\n",
    "                val_true_col = next((c for c in val.columns if \"true\" in c.lower() and \"species\" in c.lower()), None)\n",
    "                if val_true_col is None:\n",
    "                    val_true_col = next((c for c in val.columns if \"species_true\" in c.lower() or c.lower()==\"species_true_idx\"), None)\n",
    "                val_pred_col = next((c for c in val.columns if \"species_pred_label\" in c.lower() or \"species_pred\" in c.lower()), None)\n",
    "                if val_true_col is None or val_pred_col is None:\n",
    "                    print(\"Could not detect true/pred columns in validation file; skipping deconv.\")\n",
    "                else:\n",
    "                    val[\"_true_norm\"] = val[val_true_col].astype(str).fillna(\"\").apply(lambda s: \" \".join(str(s).split()))\n",
    "                    val[\"_pred_norm\"] = val[val_pred_col].astype(str).fillna(\"\").apply(lambda s: \" \".join(str(s).split()))\n",
    "                    classes_true = sorted(val[\"_true_norm\"].unique().tolist())\n",
    "                    classes_pred = sorted(preds[\"_species_norm_final\"].unique().tolist())\n",
    "                    A = np.zeros((len(classes_true), len(classes_pred)), dtype=float)\n",
    "                    for i,t in enumerate(classes_true):\n",
    "                        sub = val[val[\"_true_norm\"]==t]\n",
    "                        if len(sub)==0: continue\n",
    "                        vc = sub[\"_pred_norm\"].value_counts()\n",
    "                        for j,p_label in enumerate(classes_pred):\n",
    "                            A[i,j] = vc.get(p_label, 0) / len(sub)\n",
    "                    pred_counts_map = dict(zip(counts[\"species\"], counts[\"count\"]))\n",
    "                    p_vec = np.array([pred_counts_map.get(lbl, 0.0) for lbl in classes_pred], dtype=float)\n",
    "                    M = A.T\n",
    "                    if M.size==0 or np.allclose(M, 0.0):\n",
    "                        print(\"Confusion matrix empty -> skipping deconvolution.\")\n",
    "                    else:\n",
    "                        print(\"Running NNLS deconvolution (projected gradient) ...\")\n",
    "                        x = nnls_projected_gradient(M, p_vec, max_iter=5000, tol=1e-6)\n",
    "                        x = np.maximum(0.0, x)\n",
    "                        tot = x.sum()\n",
    "                        rows_out = []\n",
    "                        for i,t in enumerate(classes_true):\n",
    "                            pred_count_for_t = pred_counts_map.get(t, 0)\n",
    "                            est_true_count = float(x[i]) if i < len(x) else 0.0\n",
    "                            rows_out.append({\"species\": t, \"pred_count\": int(pred_count_for_t), \"pred_count_rel\": pred_count_for_t/(p_vec.sum() if p_vec.sum()>0 else 1.0), \"est_true_count\": est_true_count, \"est_true_rel\": (est_true_count/tot if tot>0 else 0.0)})\n",
    "                        deconv_df = pd.DataFrame(rows_out).sort_values(\"est_true_count\", ascending=False).reset_index(drop=True)\n",
    "                        out_deconv = extracted / \"abundance_from_predictions_deconvolved_after_autofill.csv\"\n",
    "                        deconv_df.to_csv(out_deconv, index=False)\n",
    "                        out_recon = extracted / \"abundance_reconciled_species_after_autofill.csv\"\n",
    "                        deconv_df[[\"species\",\"est_true_rel\"]].rename(columns={\"est_true_rel\":\"est_rel\"}).to_csv(out_recon, index=False)\n",
    "                        print(\"Wrote deconvolved & reconciled ->\", out_deconv.name, out_recon.name)\n",
    "            except Exception as e:\n",
    "                print(\"Failed to run deconvolution on validation file:\", e)\n",
    "\n",
    "        # Final summary\n",
    "        n_unassigned_after = int((preds[\"_species_norm_final\"].str.upper()==\"UNASSIGNED\").sum())\n",
    "        print(\"\\nSUMMARY:\")\n",
    "        print(\" UNASSIGNED before (after BLAST step):\", n_unassigned_before)\n",
    "        print(\" Assigned from BLAST:\", applied_from_blast)\n",
    "        print(\" Assigned from auto-fill:\", applied_from_autofill)\n",
    "        print(\" UNASSIGNED after processing:\", n_unassigned_after)\n",
    "        print(\"\\nFiles written to:\", extracted)\n",
    "        print(\" -\", enriched_out.name)\n",
    "        if 'out_audit' in locals() and out_audit.exists(): print(\" -\", out_audit.name)\n",
    "        print(\" -\", out_raw.name)\n",
    "        print(\" -\", out_weight.name)\n",
    "        if 'out_deconv' in locals() and out_deconv.exists(): print(\" -\", out_deconv.name)\n",
    "        if 'out_recon' in locals() and out_recon.exists(): print(\" -\", out_recon.name)\n",
    "        print(\"\\nIf you want stricter behavior (only BLAST, no autofill) re-run this cell after placing blast_results.tsv into extracted/.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "da13d8ab-8747-4d8a-87f2-6140a494727c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using extracted folder: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\n",
      "Loaded raw file: abundance_from_predictions.csv\n",
      "Loaded weighted file: abundance_from_predictions_weighted.csv\n",
      "Loaded deconv file: abundance_from_predictions_deconvolved.csv\n",
      "Loaded reconciled file: abundance_reconciled_species.csv\n",
      "Wrote publication-ready CSV -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\abundance_publication_ready.csv\n",
      "Note: No redistribution performed.\n",
      "\n",
      "Top 20 taxa by est_rel (species, pred_count, est_pct):\n",
      "                        species  pred_count   est_pct\n",
      "                     UNASSIGNED         0.0 52.783613\n",
      "                Maylandia zebra         0.0 21.008403\n",
      "               Chaetodon auriga         0.0  5.777311\n",
      "          Arvicanthis niloticus         0.0  1.838235\n",
      "                  Morchella sp.         0.0  1.313025\n",
      "           Aonchotheca annulosa         0.0  1.050420\n",
      "            Amanita fuscozonata         0.0  0.840336\n",
      "       Pseudopestalotiopsis sp.         0.0  0.787815\n",
      "Deuterostichococcus epilithicus         0.0  0.787815\n",
      "     Chloroidium saccharophilum         0.0  0.787815\n",
      "      Aspergillus costaricensis         0.0  0.525210\n",
      "                Inocybe miranda         0.0  0.525210\n",
      "          Morchella nipponensis         0.0  0.525210\n",
      "          Nannizziopsis guarroi         0.0  0.525210\n",
      "      Omphalotus flagelliformis         0.0  0.525210\n",
      "                Cortinarius sp.         0.0  0.525210\n",
      "    Cardimyxobolus iriomotensis         0.0  0.525210\n",
      "        Baruscapillaria inflexa         0.0  0.525210\n",
      "                    Inocybe sp.         0.0  0.525210\n",
      "         Pseudosperma squamatum         0.0  0.525210\n",
      "Wrote summary -> C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\\abundance_publication_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# FINALIZE (fixed): produce publication-ready abundance CSV from files in extracted/\n",
    "# - Fixes earlier pandas truth-value error.\n",
    "# - Works only with files already in extracted/.\n",
    "# - Non-destructive: original files remain untouched.\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------- Config ----------\n",
    "APPLY_REDISTRIBUTE_UNASSIGNED = True\n",
    "REDISTRIBUTE_ONLY_IF_UNASSIGNED_FRAC_GT = 0.0\n",
    "TOP_N_PRINT = 20\n",
    "# ----------------------------\n",
    "\n",
    "def find_extracted():\n",
    "    candidates = [\n",
    "        Path.cwd() / \"sih\" / \"ncbi_blast_db\" / \"extracted\",\n",
    "        Path.cwd() / \"ncbi_blast_db\" / \"extracted\",\n",
    "        Path.cwd() / \"extracted\",\n",
    "        Path(r\"C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\"),\n",
    "        Path(r\"C:\\Users\\Srijit\\OneDrive\\Desktop\\sihtaxa\\sihabundance\\ncbi_blast_db\\extracted\"),\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if p.exists() and p.is_dir():\n",
    "            return p.resolve()\n",
    "    for p in Path.cwd().rglob(\"**/extracted\"):\n",
    "        if p.is_dir():\n",
    "            return p.resolve()\n",
    "    return None\n",
    "\n",
    "def load_if_exists(path_obj):\n",
    "    if path_obj.exists():\n",
    "        try:\n",
    "            return pd.read_csv(path_obj)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: failed to read {path_obj.name}: {e}\")\n",
    "    return None\n",
    "\n",
    "extracted = find_extracted()\n",
    "if extracted is None:\n",
    "    print(\"Could not find an 'extracted/' folder. Place outputs in an 'extracted/' folder and re-run.\")\n",
    "else:\n",
    "    print(\"Using extracted folder:\", extracted)\n",
    "\n",
    "    # locate the best-available raw/weighted/deconv/reconciled files (first match wins)\n",
    "    raw_candidates = [\n",
    "        \"abundance_from_predictions.csv\",\n",
    "        \"abundance_from_predictions_after_blast.csv\",\n",
    "        \"abundance_from_predictions_after_autofill.csv\",\n",
    "        \"abundance_from_predictions_after_blast.csv\"\n",
    "    ]\n",
    "    weighted_candidates = [\n",
    "        \"abundance_from_predictions_weighted.csv\",\n",
    "        \"abundance_from_predictions_weighted_after_blast.csv\",\n",
    "        \"abundance_from_predictions_weighted_after_autofill.csv\"\n",
    "    ]\n",
    "    deconv_candidates = [\n",
    "        \"abundance_from_predictions_deconvolved.csv\",\n",
    "        \"abundance_from_predictions_deconvolved_after_blast.csv\",\n",
    "        \"abundance_from_predictions_deconvolved_after_autofill.csv\"\n",
    "    ]\n",
    "    reconciled_candidates = [\n",
    "        \"abundance_reconciled_species.csv\",\n",
    "        \"abundance_reconciled_species_after_blast.csv\",\n",
    "        \"abundance_reconciled_species_after_autofill.csv\"\n",
    "    ]\n",
    "\n",
    "    raw = None\n",
    "    for fn in raw_candidates:\n",
    "        raw = load_if_exists(extracted / fn)\n",
    "        if raw is not None:\n",
    "            print(\"Loaded raw file:\", fn)\n",
    "            break\n",
    "\n",
    "    weighted = None\n",
    "    for fn in weighted_candidates:\n",
    "        weighted = load_if_exists(extracted / fn)\n",
    "        if weighted is not None:\n",
    "            print(\"Loaded weighted file:\", fn)\n",
    "            break\n",
    "\n",
    "    deconv = None\n",
    "    for fn in deconv_candidates:\n",
    "        deconv = load_if_exists(extracted / fn)\n",
    "        if deconv is not None:\n",
    "            print(\"Loaded deconv file:\", fn)\n",
    "            break\n",
    "\n",
    "    reconciled = None\n",
    "    for fn in reconciled_candidates:\n",
    "        reconciled = load_if_exists(extracted / fn)\n",
    "        if reconciled is not None:\n",
    "            print(\"Loaded reconciled file:\", fn)\n",
    "            break\n",
    "\n",
    "    # If reconciled missing but deconv is present, form reconciled from deconv if possible\n",
    "    if reconciled is None and deconv is not None:\n",
    "        if 'est_true_rel' in deconv.columns:\n",
    "            reconciled = deconv[['species','est_true_rel']].rename(columns={'est_true_rel':'est_rel'})\n",
    "            print(\"Built reconciled table from deconv (est_true_rel).\")\n",
    "        elif 'est_true_count' in deconv.columns:\n",
    "            tmp = deconv[['species','est_true_count']].copy()\n",
    "            s = tmp['est_true_count'].sum() if tmp['est_true_count'].sum()>0 else 1.0\n",
    "            tmp['est_rel'] = tmp['est_true_count'] / s\n",
    "            reconciled = tmp[['species','est_rel']]\n",
    "            print(\"Built reconciled table from deconv (est_true_count).\")\n",
    "        else:\n",
    "            reconciled = None\n",
    "\n",
    "    # Collect all species present across available tables\n",
    "    species_set = set()\n",
    "    if raw is not None and 'species' in raw.columns:\n",
    "        species_set.update(raw['species'].astype(str).tolist())\n",
    "    if weighted is not None and 'species' in weighted.columns:\n",
    "        species_set.update(weighted['species'].astype(str).tolist())\n",
    "    if reconciled is not None and 'species' in reconciled.columns:\n",
    "        species_set.update(reconciled['species'].astype(str).tolist())\n",
    "\n",
    "    species_list = sorted(list(species_set), key=lambda x: (x.upper()==\"UNASSIGNED\", x))\n",
    "\n",
    "    if not species_list:\n",
    "        print(\"No abundance data files were found in extracted/. Nothing to build.\")\n",
    "    else:\n",
    "        res = pd.DataFrame({'species': species_list})\n",
    "\n",
    "        # map raw counts if available\n",
    "        if raw is not None and 'species' in raw.columns and 'count' in raw.columns:\n",
    "            raw_map = dict(zip(raw['species'].astype(str), raw['count'].astype(float)))\n",
    "            res['pred_count'] = res['species'].map(raw_map).fillna(0.0).astype(float)\n",
    "        else:\n",
    "            res['pred_count'] = 0.0\n",
    "\n",
    "        total_raw = res['pred_count'].sum() if res['pred_count'].sum()>0 else 1.0\n",
    "        res['pred_count_rel'] = res['pred_count'] / total_raw\n",
    "\n",
    "        # map weighted (conf_sum)\n",
    "        if weighted is not None and 'species' in weighted.columns:\n",
    "            if 'conf_sum' in weighted.columns:\n",
    "                wmap = dict(zip(weighted['species'].astype(str), weighted['conf_sum'].astype(float)))\n",
    "            else:\n",
    "                # assume second numeric column is conf_sum\n",
    "                second_col = weighted.columns[1] if len(weighted.columns)>1 else None\n",
    "                if second_col:\n",
    "                    wmap = dict(zip(weighted['species'].astype(str), pd.to_numeric(weighted[second_col], errors='coerce').fillna(0.0)))\n",
    "                else:\n",
    "                    wmap = {}\n",
    "            res['conf_sum'] = res['species'].map(wmap).fillna(0.0).astype(float)\n",
    "        else:\n",
    "            res['conf_sum'] = 0.0\n",
    "        total_conf = res['conf_sum'].sum() if res['conf_sum'].sum()>0 else 1.0\n",
    "        res['pred_conf_rel'] = res['conf_sum'] / total_conf\n",
    "\n",
    "        # map reconciled est_rel\n",
    "        if reconciled is not None and 'species' in reconciled.columns and 'est_rel' in reconciled.columns:\n",
    "            rmap = dict(zip(reconciled['species'].astype(str), reconciled['est_rel'].astype(float)))\n",
    "            res['est_rel'] = res['species'].map(rmap).fillna(0.0).astype(float)\n",
    "        else:\n",
    "            # fallback to pred_count_rel to fill est_rel when none available\n",
    "            res['est_rel'] = res['pred_count_rel'].copy()\n",
    "\n",
    "        # normalize est_rel\n",
    "        total_est = res['est_rel'].sum()\n",
    "        if total_est > 0:\n",
    "            res['est_rel'] = res['est_rel'] / total_est\n",
    "\n",
    "        # handle UNASSIGNED redistribution at abundance-level only\n",
    "        unassigned_mask = res['species'].astype(str).str.upper() == 'UNASSIGNED'\n",
    "        unassigned_count = float(res.loc[unassigned_mask, 'pred_count'].sum()) if unassigned_mask.any() else 0.0\n",
    "        frac_unassigned = unassigned_count / res['pred_count'].sum() if res['pred_count'].sum()>0 else 0.0\n",
    "\n",
    "        if APPLY_REDISTRIBUTE_UNASSIGNED and frac_unassigned > REDISTRIBUTE_ONLY_IF_UNASSIGNED_FRAC_GT:\n",
    "            # distribute UNASSIGNED count proportionally to est_rel over non-unassigned taxa\n",
    "            target = res.loc[~unassigned_mask].copy()\n",
    "            if target.empty:\n",
    "                print(\"Only UNASSIGNED present; cannot redistribute.\")\n",
    "            else:\n",
    "                est_vals = target['est_rel'].values\n",
    "                if est_vals.sum() <= 0:\n",
    "                    est_vals = target['pred_count_rel'].values\n",
    "                    if est_vals.sum() <= 0:\n",
    "                        est_vals = np.ones(len(target))\n",
    "                weights = est_vals / est_vals.sum()\n",
    "                added = weights * unassigned_count\n",
    "                target = target.reset_index(drop=True)\n",
    "                target['pred_count_after'] = target['pred_count'] + added\n",
    "                redistributed = res.copy()\n",
    "                redistributed.loc[~unassigned_mask, 'pred_count'] = target['pred_count_after'].values\n",
    "                redistributed.loc[unassigned_mask, 'pred_count'] = 0.0\n",
    "                total_new = redistributed['pred_count'].sum() if redistributed['pred_count'].sum()>0 else 1.0\n",
    "                redistributed['pred_count_rel'] = redistributed['pred_count'] / total_new\n",
    "                # renormalize est_rel\n",
    "                if redistributed['est_rel'].sum() > 0:\n",
    "                    redistributed['est_rel'] = redistributed['est_rel'] / redistributed['est_rel'].sum()\n",
    "                redistributed['conf_sum'] = redistributed['conf_sum'].where(~unassigned_mask, 0.0)\n",
    "                total_conf2 = redistributed['conf_sum'].sum() if redistributed['conf_sum'].sum()>0 else 1.0\n",
    "                redistributed['pred_conf_rel'] = redistributed['conf_sum'] / total_conf2\n",
    "                final_df = redistributed\n",
    "                note = f\"Redistributed UNASSIGNED mass at abundance level (unassigned_count={unassigned_count:.0f}).\"\n",
    "        else:\n",
    "            final_df = res.copy()\n",
    "            note = \"No redistribution performed.\"\n",
    "\n",
    "        # cleanup and ensure numeric\n",
    "        for c in ['pred_count','pred_count_rel','conf_sum','pred_conf_rel','est_rel']:\n",
    "            if c in final_df.columns:\n",
    "                final_df[c] = pd.to_numeric(final_df[c], errors='coerce').fillna(0.0)\n",
    "\n",
    "        # add est_pct\n",
    "        final_df['est_pct'] = final_df['est_rel'] * 100.0\n",
    "\n",
    "        # write publication-ready CSV\n",
    "        pub_out = extracted / \"abundance_publication_ready.csv\"\n",
    "        cols_out = ['species','pred_count','pred_count_rel','conf_sum','pred_conf_rel','est_rel','est_pct']\n",
    "        # some columns may be missing depending on inputs; select those present\n",
    "        cols_present = [c for c in cols_out if c in final_df.columns]\n",
    "        final_df[cols_present].to_csv(pub_out, index=False)\n",
    "        print(\"Wrote publication-ready CSV ->\", pub_out)\n",
    "        print(\"Note:\", note)\n",
    "\n",
    "        # print top N\n",
    "        top = final_df.sort_values('est_rel', ascending=False).reset_index(drop=True).head(TOP_N_PRINT)\n",
    "        print(f\"\\nTop {TOP_N_PRINT} taxa by est_rel (species, pred_count, est_pct):\")\n",
    "        print(top[['species','pred_count','est_pct']].to_string(index=False))\n",
    "\n",
    "        # summary file\n",
    "        summary = {\n",
    "            \"pub_csv\": pub_out.name,\n",
    "            \"raw_used\": bool(raw is not None),\n",
    "            \"weighted_used\": bool(weighted is not None),\n",
    "            \"reconciled_used\": bool(reconciled is not None),\n",
    "            \"redistributed_unassigned\": APPLY_REDISTRIBUTE_UNASSIGNED and frac_unassigned > REDISTRIBUTE_ONLY_IF_UNASSIGNED_FRAC_GT,\n",
    "            \"fraction_unassigned_before\": frac_unassigned,\n",
    "            \"rows_in_pub_table\": len(final_df)\n",
    "        }\n",
    "        summary_df = pd.DataFrame(list(summary.items()), columns=[\"key\",\"value\"])\n",
    "        summary_out = extracted / \"abundance_publication_summary.csv\"\n",
    "        summary_df.to_csv(summary_out, index=False)\n",
    "        print(\"Wrote summary ->\", summary_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ad433db0-f1c1-437f-9108-2b5e2370ba22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using extracted: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\n",
      "blast_results.tsv not found in extracted/. Run BLAST externally (use earlier printed command) and copy the file here, then re-run this cell.\n"
     ]
    }
   ],
   "source": [
    "# CELL: Apply BLAST assignments (strict) and recompute abundances (run only if blast_results.tsv is present)\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np, re\n",
    "\n",
    "# Settings (conservative)\n",
    "MIN_PID = 97.0\n",
    "MIN_COV = 80.0\n",
    "MAX_EVAL = 1e-6\n",
    "APPLY_ASSIGNMENTS = True  # will only change rows that are currently UNASSIGNED\n",
    "\n",
    "# locate extracted/\n",
    "def find_extracted():\n",
    "    cand = [\n",
    "        Path.cwd()/\"sih\"/\"ncbi_blast_db\"/\"extracted\",\n",
    "        Path.cwd()/\"ncbi_blast_db\"/\"extracted\",\n",
    "        Path.cwd()/ \"extracted\",\n",
    "        Path(r\"C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\"),\n",
    "        Path(r\"C:\\Users\\Srijit\\OneDrive\\Desktop\\sihtaxa\\sihabundance\\ncbi_blast_db\\extracted\"),\n",
    "    ]\n",
    "    for p in cand:\n",
    "        if p.exists() and p.is_dir(): return p.resolve()\n",
    "    for p in Path.cwd().rglob(\"predictions_with_uncertainty.csv\"):\n",
    "        return p.parent.resolve()\n",
    "    return None\n",
    "\n",
    "extracted = find_extracted()\n",
    "if extracted is None:\n",
    "    print(\"Could not locate 'extracted/'. Stop.\")\n",
    "else:\n",
    "    print(\"Using extracted:\", extracted)\n",
    "    blast_file = extracted / \"blast_results.tsv\"\n",
    "    if not blast_file.exists():\n",
    "        print(\"blast_results.tsv not found in extracted/. Run BLAST externally (use earlier printed command) and copy the file here, then re-run this cell.\")\n",
    "    else:\n",
    "        print(\"Parsing BLAST file:\", blast_file.name)\n",
    "        cols = [\"qseqid\",\"sseqid\",\"pident\",\"length\",\"qlen\",\"qstart\",\"qend\",\"sstart\",\"send\",\"evalue\",\"bitscore\",\"stitle\"]\n",
    "        dfb = pd.read_csv(blast_file, sep=\"\\t\", names=cols, header=None, quoting=3, dtype={\"stitle\":str})\n",
    "        dfb[\"pident\"] = pd.to_numeric(dfb[\"pident\"], errors=\"coerce\").fillna(0.0)\n",
    "        dfb[\"length\"] = pd.to_numeric(dfb[\"length\"], errors=\"coerce\").fillna(0.0)\n",
    "        dfb[\"qlen\"] = pd.to_numeric(dfb[\"qlen\"], errors=\"coerce\").replace(0, np.nan)\n",
    "        dfb[\"coverage_pct\"] = dfb.apply(lambda r: (r[\"length\"]/r[\"qlen\"]*100.0) if (pd.notna(r[\"qlen\"]) and r[\"qlen\"]>0) else 0.0, axis=1)\n",
    "        dfb[\"evalue\"] = pd.to_numeric(dfb[\"evalue\"], errors=\"coerce\").fillna(1e6)\n",
    "        dfb_high = dfb[(dfb[\"pident\"]>=MIN_PID) & (dfb[\"coverage_pct\"]>=MIN_COV) & (dfb[\"evalue\"]<=MAX_EVAL)].copy()\n",
    "        print(\"BLAST rows passing thresholds:\", len(dfb_high))\n",
    "        if dfb_high.empty:\n",
    "            print(\"No high-confidence BLAST hits found. Nothing to apply.\")\n",
    "        else:\n",
    "            dfb_top = dfb_high.sort_values([\"qseqid\",\"bitscore\"], ascending=[True,False]).groupby(\"qseqid\", as_index=False).first()\n",
    "            def extract_species(title):\n",
    "                if not isinstance(title,str) or not title.strip(): return None\n",
    "                m = re.search(r'\\b([A-Z][a-z]+(?:\\s+[a-z][a-z\\-]+))\\b', title)\n",
    "                if m: return m.group(1).strip()\n",
    "                m2 = re.search(r'\\b([A-Z][a-z]+)\\s+sp\\b', title)\n",
    "                if m2: return m2.group(0).strip()\n",
    "                return None\n",
    "            assignments = {}\n",
    "            for _, r in dfb_top.iterrows():\n",
    "                q = str(r[\"qseqid\"]); s = extract_species(r[\"stitle\"])\n",
    "                if s:\n",
    "                    assignments[q] = {\"species\": s, \"pident\": float(r[\"pident\"]), \"coverage\": float(r[\"coverage_pct\"]), \"sseqid\": r[\"sseqid\"], \"stitle\": r[\"stitle\"]}\n",
    "            # audit CSV\n",
    "            assign_df = pd.DataFrame([{\"query_id\":q,**v} for q,v in assignments.items()])\n",
    "            assign_df.to_csv(extracted/\"blast_species_assignments.csv\", index=False)\n",
    "            print(\"Wrote blast_species_assignments.csv\")\n",
    "            # load predictions\n",
    "            pred_file = extracted/\"predictions_with_uncertainty.csv\"\n",
    "            if not pred_file.exists(): pred_file = extracted/\"predictions.csv\"\n",
    "            if not pred_file.exists():\n",
    "                print(\"Predictions CSV not found. Abort.\")\n",
    "            else:\n",
    "                pred = pd.read_csv(pred_file)\n",
    "                cols_lower = {c.lower():c for c in pred.columns}\n",
    "                species_col = next((cols_lower[k] for k in cols_lower if \"species_pred_label\" in k or (\"species\" in k and \"pred\" in k)), None)\n",
    "                if species_col is None:\n",
    "                    species_col = next((c for c in pred.columns if \"species\" in c.lower()), pred.columns[-1])\n",
    "                id_col = next((cols_lower[k] for k in cols_lower if k in (\"id\",\"accession\",\"seqid\",\"read_id\",\"readid\",\"accession_id\",\"global_index\")), None)\n",
    "                if id_col is None:\n",
    "                    print(\"No id column in predictions — cannot map BLAST queries. Abort.\")\n",
    "                else:\n",
    "                    pred[\"_species_norm\"] = pred[species_col].astype(str).fillna(\"UNASSIGNED\").apply(lambda s: \" \".join(str(s).split()))\n",
    "                    applied = 0\n",
    "                    for q,info in assignments.items():\n",
    "                        exact_mask = pred[id_col].astype(str) == str(q)\n",
    "                        if exact_mask.any():\n",
    "                            for ri in pred.index[exact_mask]:\n",
    "                                if pred.at[ri, \"_species_norm\"].upper()==\"UNASSIGNED\":\n",
    "                                    pred.at[ri, species_col] = info[\"species\"]\n",
    "                                    pred.at[ri, \"blast_assigned_species\"] = info[\"species\"]\n",
    "                                    pred.at[ri, \"blast_pident\"] = info[\"pident\"]\n",
    "                                    pred.at[ri, \"blast_coverage\"] = info[\"coverage\"]\n",
    "                                    pred.at[ri, \"blast_sseqid\"] = info[\"sseqid\"]\n",
    "                                    pred.at[ri, \"blast_stitle\"] = info[\"stitle\"]\n",
    "                                    applied += 1\n",
    "                            continue\n",
    "                        substr_mask = pred[id_col].astype(str).str.contains(str(q), na=False)\n",
    "                        if substr_mask.any():\n",
    "                            for ri in pred.index[substr_mask]:\n",
    "                                if pred.at[ri, \"_species_norm\"].upper()==\"UNASSIGNED\":\n",
    "                                    pred.at[ri, species_col] = info[\"species\"]\n",
    "                                    pred.at[ri, \"blast_assigned_species\"] = info[\"species\"]\n",
    "                                    pred.at[ri, \"blast_pident\"] = info[\"pident\"]\n",
    "                                    pred.at[ri, \"blast_coverage\"] = info[\"coverage\"]\n",
    "                                    pred.at[ri, \"blast_sseqid\"] = info[\"sseqid\"]\n",
    "                                    pred.at[ri, \"blast_stitle\"] = info[\"stitle\"]\n",
    "                                    applied += 1\n",
    "                    if applied==0:\n",
    "                        print(\"No UNASSIGNED rows matched the high-confidence BLAST hits. Nothing changed.\")\n",
    "                    else:\n",
    "                        pred.to_csv(extracted/\"predictions_after_blast_forced.csv\", index=False)\n",
    "                        print(f\"Applied BLAST assignments to {applied} rows -> predictions_after_blast_forced.csv\")\n",
    "                        # recompute abundances (simple)\n",
    "                        pred[\"_species_norm_final\"] = pred[species_col].astype(str).fillna(\"\").apply(lambda s:\" \".join(str(s).split()))\n",
    "                        counts = pred[\"_species_norm_final\"].value_counts().reset_index()\n",
    "                        counts.columns = [\"species\",\"count\"]\n",
    "                        counts[\"rel\"] = counts[\"count\"]/counts[\"count\"].sum()\n",
    "                        counts.to_csv(extracted/\"abundance_after_blast.csv\", index=False)\n",
    "                        print(\"Wrote abundance_after_blast.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3c598716-c3c3-43e7-9421-2d0a3c7bf227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using extracted folder: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\n",
      "Loaded predictions: (380, 54)\n",
      "Detected columns -> species: species_pred_label | id: global_index | index_col: global_index\n",
      "Loaded embeddings shape: (2555, 64)\n",
      "Assigned rows: 236 Unassigned rows: 144\n",
      "median assigned nearest-neighbor similarity: 1.0\n",
      "KNN assignments applied to UNASSIGNED rows: 3\n",
      "Wrote files:\n",
      " - predictions_after_knn_assignments.csv\n",
      " - knn_assignments_audit.csv\n",
      " - abundance_from_predictions_after_knn.csv\n",
      " - abundance_from_predictions_weighted_after_knn.csv\n",
      "\n",
      "Top species assigned by KNN (audit):\n",
      "                 count  count\n",
      "       Maylandia zebra      2\n",
      "Hysterothylacium fabri      1\n"
     ]
    }
   ],
   "source": [
    "# KNN-based assignment of UNASSIGNED reads using embeddings (no BLAST needed).\n",
    "# Conservative, audited, uses only files in extracted/.\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math, sys\n",
    "\n",
    "# --- settings (tweak if you want more/less aggressive) ---\n",
    "K_NEIGH = 7               # number of neighbor votes to consider (auto-limited to assigned count)\n",
    "VOTE_FRACTION_THRESH = 0.60   # fraction of neighbor-weighted vote needed to accept top species\n",
    "SIM_REL_TO_ASSIGNED_MED = 0.85  # require top-sim >= median_assigned_sim * this factor (keeps conservative)\n",
    "MIN_TOP_SIM_ABS = 0.70    # always require this absolute minimum top similarity\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def find_extracted():\n",
    "    candidates = [\n",
    "        Path.cwd() / \"sih\" / \"ncbi_blast_db\" / \"extracted\",\n",
    "        Path.cwd() / \"ncbi_blast_db\" / \"extracted\",\n",
    "        Path.cwd() / \"extracted\",\n",
    "        Path(r\"C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\"),\n",
    "        Path(r\"C:\\Users\\Srijit\\OneDrive\\Desktop\\sihtaxa\\sihabundance\\ncbi_blast_db\\extracted\"),\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if p.exists() and p.is_dir(): return p.resolve()\n",
    "    for p in Path.cwd().rglob(\"predictions_with_uncertainty.csv\"):\n",
    "        return p.parent.resolve()\n",
    "    return None\n",
    "\n",
    "extracted = find_extracted()\n",
    "if extracted is None:\n",
    "    raise SystemExit(\"Could not locate extracted/ folder with your predictions and embeddings. Place files in an 'extracted' folder and re-run.\")\n",
    "\n",
    "print(\"Using extracted folder:\", extracted)\n",
    "\n",
    "# load predictions\n",
    "pred_file = extracted / \"predictions_with_uncertainty.csv\"\n",
    "if not pred_file.exists():\n",
    "    pred_file = extracted / \"predictions.csv\"\n",
    "if not pred_file.exists():\n",
    "    raise SystemExit(\"Predictions CSV not found in extracted/.\")\n",
    "\n",
    "pred = pd.read_csv(pred_file)\n",
    "print(\"Loaded predictions:\", pred.shape)\n",
    "\n",
    "# detect species column / id / index mapping\n",
    "species_col = next((c for c in pred.columns if \"species_pred_label\" in c.lower()), None)\n",
    "if species_col is None:\n",
    "    species_col = next((c for c in pred.columns if \"species\" in c.lower()), None)\n",
    "id_col = next((c for c in pred.columns if c.lower() in (\"id\",\"accession\",\"seqid\",\"global_index\",\"read_id\",\"readid\")), None)\n",
    "# pick embedding index column\n",
    "index_col = None\n",
    "if \"global_index\" in pred.columns:\n",
    "    index_col = \"global_index\"\n",
    "elif \"pca_index\" in pred.columns:\n",
    "    index_col = \"pca_index\"\n",
    "else:\n",
    "    # assume order corresponds to embeddings\n",
    "    index_col = None\n",
    "\n",
    "print(\"Detected columns -> species:\", species_col, \"| id:\", id_col, \"| index_col:\", index_col)\n",
    "\n",
    "# load embeddings\n",
    "emb_path = extracted / \"embeddings_pca.npy\"\n",
    "if not emb_path.exists():\n",
    "    raise SystemExit(\"embeddings_pca.npy not found in extracted/. Cannot run KNN assignment without embeddings.\")\n",
    "X = np.load(emb_path)\n",
    "print(\"Loaded embeddings shape:\", X.shape)\n",
    "\n",
    "# map prediction rows -> embedding indices\n",
    "if index_col is not None:\n",
    "    try:\n",
    "        pred[\"_emb_index\"] = pred[index_col].astype(int).values\n",
    "    except Exception:\n",
    "        pred[\"_emb_index\"] = pred[index_col].astype(float).fillna(0).astype(int).values\n",
    "else:\n",
    "    # fallback: assume same order and lengths match\n",
    "    if len(pred) == X.shape[0]:\n",
    "        pred[\"_emb_index\"] = np.arange(len(pred))\n",
    "    else:\n",
    "        raise SystemExit(\"Cannot determine mapping from prediction rows to embedding rows; predictions length != embeddings rows and no index column found.\")\n",
    "\n",
    "# normalize embeddings (for cosine similarity)\n",
    "eps = 1e-12\n",
    "norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "norms = np.maximum(norms, eps)\n",
    "Xn = X / norms\n",
    "\n",
    "# determine assigned vs unassigned\n",
    "pred[\"_species_norm\"] = pred[species_col].astype(str).fillna(\"UNASSIGNED\").apply(lambda s: \" \".join(str(s).split()))\n",
    "mask_assigned = pred[\"_species_norm\"].str.upper() != \"UNASSIGNED\"\n",
    "mask_unassigned = pred[\"_species_norm\"].str.upper() == \"UNASSIGNED\"\n",
    "assigned_rows = pred[mask_assigned].copy()\n",
    "unassigned_rows = pred[mask_unassigned].copy()\n",
    "print(\"Assigned rows:\", len(assigned_rows), \"Unassigned rows:\", len(unassigned_rows))\n",
    "\n",
    "if len(assigned_rows) < 5:\n",
    "    raise SystemExit(\"Too few assigned rows to run embedding-based assignment (need >=5). Use BLAST or supply more labeled data.\")\n",
    "\n",
    "# embedding indices arrays\n",
    "assigned_idx = assigned_rows[\"_emb_index\"].astype(int).values\n",
    "unassigned_idx = unassigned_rows[\"_emb_index\"].astype(int).values\n",
    "\n",
    "# build assigned normalized matrix and species list\n",
    "Xa = Xn[assigned_idx, :]\n",
    "labels_assigned = assigned_rows[\"_species_norm\"].values\n",
    "\n",
    "# compute per-assigned nearest neighbor similarity (to get median within-assigned similarity)\n",
    "# compute dot product matrix limited to assigned (small)\n",
    "Sa = Xa.dot(Xa.T)  # shape (na, na)\n",
    "# set self to -inf\n",
    "np.fill_diagonal(Sa, -np.inf)\n",
    "nearest_assigned_sim = np.max(Sa, axis=1)\n",
    "median_assigned_sim = float(np.median(nearest_assigned_sim[np.isfinite(nearest_assigned_sim)]))\n",
    "if not np.isfinite(median_assigned_sim):\n",
    "    median_assigned_sim = 0.5\n",
    "print(\"median assigned nearest-neighbor similarity:\", round(median_assigned_sim,4))\n",
    "\n",
    "# compute similarity of each unassigned to all assigned: S_u_a = X_un @ Xa.T\n",
    "Xu = Xn[unassigned_idx, :]\n",
    "S_u_a = Xu.dot(Xa.T)  # shape (n_unassigned, n_assigned)\n",
    "\n",
    "# for each unassigned compute top-k neighbors and weighted vote\n",
    "k = min(K_NEIGH, Xa.shape[0])\n",
    "assigned_species_list = list(labels_assigned)\n",
    "assigned_species_array = np.array(labels_assigned)\n",
    "\n",
    "assignments = []\n",
    "for i_un, sims in enumerate(S_u_a):\n",
    "    if sims.max() <= 0:\n",
    "        assignments.append(None); continue\n",
    "    # top k indices by similarity\n",
    "    topk_idx = np.argsort(-sims)[:k]\n",
    "    topk_sims = sims[topk_idx]\n",
    "    sum_sims = topk_sims.sum()\n",
    "    if sum_sims <= 0:\n",
    "        assignments.append(None); continue\n",
    "    # weighted votes by similarity\n",
    "    species_votes = {}\n",
    "    for idx_local, sim_val in zip(topk_idx, topk_sims):\n",
    "        sp = assigned_species_array[idx_local]\n",
    "        species_votes[sp] = species_votes.get(sp, 0.0) + float(sim_val)\n",
    "    # pick top species\n",
    "    top_species, top_vote = max(species_votes.items(), key=lambda x: x[1])\n",
    "    vote_fraction = top_vote / sum_sims\n",
    "    top_similarity = float(topk_sims.max())\n",
    "    # acceptance criteria\n",
    "    accept = (vote_fraction >= VOTE_FRACTION_THRESH) and (top_similarity >= max(MIN_TOP_SIM_ABS, SIM_REL_TO_ASSIGNED_MED * median_assigned_sim))\n",
    "    if accept:\n",
    "        assignments.append({\n",
    "            \"assigned_species\": top_species,\n",
    "            \"vote_fraction\": float(vote_fraction),\n",
    "            \"top_similarity\": float(top_similarity),\n",
    "            \"k\": int(k)\n",
    "        })\n",
    "    else:\n",
    "        assignments.append(None)\n",
    "\n",
    "# apply assignments to a copy of predictions, but do NOT overwrite original\n",
    "pred_knn = pred.copy()\n",
    "applied_count = 0\n",
    "audit_rows = []\n",
    "for i_row, (ri, row) in enumerate(unassigned_rows.iterrows()):\n",
    "    out = assignments[i_row]\n",
    "    if out is not None:\n",
    "        idx_global = int(row.name)  # original pred row index\n",
    "        # write columns\n",
    "        pred_knn.at[idx_global, \"knn_assigned_species\"] = out[\"assigned_species\"]\n",
    "        pred_knn.at[idx_global, \"knn_vote_fraction\"] = out[\"vote_fraction\"]\n",
    "        pred_knn.at[idx_global, \"knn_top_similarity\"] = out[\"top_similarity\"]\n",
    "        pred_knn.at[idx_global, \"knn_k\"] = out[\"k\"]\n",
    "        # apply assignment to species column (non-destructive; original file stays)\n",
    "        pred_knn.at[idx_global, species_col] = out[\"assigned_species\"]\n",
    "        applied_count += 1\n",
    "        audit_rows.append({\n",
    "            \"row_index\": int(idx_global),\n",
    "            \"id\": pred_knn.at[idx_global, id_col] if id_col in pred_knn.columns else \"\",\n",
    "            \"new_species\": out[\"assigned_species\"],\n",
    "            \"vote_fraction\": out[\"vote_fraction\"],\n",
    "            \"top_similarity\": out[\"top_similarity\"]\n",
    "        })\n",
    "\n",
    "print(\"KNN assignments applied to UNASSIGNED rows:\", applied_count)\n",
    "\n",
    "# save outputs\n",
    "out_preds = extracted / \"predictions_after_knn_assignments.csv\"\n",
    "pred_knn.to_csv(out_preds, index=False)\n",
    "out_audit = extracted / \"knn_assignments_audit.csv\"\n",
    "pd.DataFrame(audit_rows).to_csv(out_audit, index=False)\n",
    "\n",
    "# recompute abundances from the updated predictions\n",
    "pred_knn[\"_species_norm_final\"] = pred_knn[species_col].astype(str).fillna(\"\").apply(lambda s: \" \".join(str(s).split()))\n",
    "counts = pred_knn[\"_species_norm_final\"].value_counts().reset_index()\n",
    "counts.columns = [\"species\",\"count\"]\n",
    "counts[\"relative_abundance\"] = counts[\"count\"] / counts[\"count\"].sum() if counts[\"count\"].sum()>0 else 0.0\n",
    "out_abund = extracted / \"abundance_from_predictions_after_knn.csv\"\n",
    "counts.to_csv(out_abund, index=False)\n",
    "\n",
    "# confidence-weighted: try to detect a confidence column\n",
    "conf_col = next((c for c in pred_knn.columns if \"species_pred_conf\" in c.lower() or c.lower().endswith(\"_conf\") or \"topprob\" in c.lower()), None)\n",
    "if conf_col:\n",
    "    pred_knn[\"_conf_num\"] = pd.to_numeric(pred_knn[conf_col], errors=\"coerce\").fillna(0.0)\n",
    "else:\n",
    "    pred_knn[\"_conf_num\"] = 0.0\n",
    "weighted = pred_knn.groupby(\"_species_norm_final\")[\"_conf_num\"].sum().reset_index().rename(columns={\"_conf_num\":\"conf_sum\"})\n",
    "total_conf = weighted[\"conf_sum\"].sum() if weighted[\"conf_sum\"].sum()>0 else 1.0\n",
    "weighted[\"relative_abundance_weighted\"] = weighted[\"conf_sum\"] / total_conf\n",
    "out_weight = extracted / \"abundance_from_predictions_weighted_after_knn.csv\"\n",
    "weighted.to_csv(out_weight, index=False)\n",
    "\n",
    "print(\"Wrote files:\")\n",
    "print(\" -\", out_preds.name)\n",
    "print(\" -\", out_audit.name)\n",
    "print(\" -\", out_abund.name)\n",
    "print(\" -\", out_weight.name)\n",
    "\n",
    "# show top assigned species from audit\n",
    "if applied_count>0:\n",
    "    adf = pd.read_csv(out_audit)\n",
    "    top_assigned = adf['new_species'].value_counts().reset_index().rename(columns={'index':'species','new_species':'count'})\n",
    "    print(\"\\nTop species assigned by KNN (audit):\")\n",
    "    print(top_assigned.head(15).to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nNo KNN assignments met the conservative criteria. You can loosen thresholds (increase VOTE_FRACTION_THRESH lower, or lower SIM_REL_TO_ASSIGNED_MED) and re-run the cell.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "13b855ea-7dfe-4bc1-8fac-b63f3287a023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using extracted folder: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\n",
      "Loaded predictions file: predictions_after_knn_assignments.csv\n",
      "Predictions rows: 380\n",
      "Found KNN audit: knn_assignments_audit.csv rows = 3\n",
      "KNN audit (first rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_index</th>\n",
       "      <th>id</th>\n",
       "      <th>new_species</th>\n",
       "      <th>vote_fraction</th>\n",
       "      <th>top_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>2541</td>\n",
       "      <td>Hysterothylacium fabri</td>\n",
       "      <td>0.729104</td>\n",
       "      <td>0.864676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55</td>\n",
       "      <td>1572</td>\n",
       "      <td>Maylandia zebra</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.851294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>226</td>\n",
       "      <td>1741</td>\n",
       "      <td>Maylandia zebra</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.851294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_index    id             new_species  vote_fraction  top_similarity\n",
       "0         20  2541  Hysterothylacium fabri       0.729104        0.864676\n",
       "1         55  1572         Maylandia zebra       1.000000        0.851294\n",
       "2        226  1741         Maylandia zebra       1.000000        0.851294"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote recomputed abundance -> abundance_from_predictions_after_knn_recomputed.csv\n",
      "Top taxa (post-KNN recomputed):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>count</th>\n",
       "      <th>relative_abundance</th>\n",
       "      <th>conf_sum</th>\n",
       "      <th>relative_abundance_weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>UNASSIGNED</td>\n",
       "      <td>141</td>\n",
       "      <td>0.371053</td>\n",
       "      <td>135.859698</td>\n",
       "      <td>0.406443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Maylandia zebra</td>\n",
       "      <td>82</td>\n",
       "      <td>0.215789</td>\n",
       "      <td>79.453327</td>\n",
       "      <td>0.237695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Chaetodon auriga</td>\n",
       "      <td>46</td>\n",
       "      <td>0.121053</td>\n",
       "      <td>25.948245</td>\n",
       "      <td>0.077628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Morchella sp.</td>\n",
       "      <td>10</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>6.121686</td>\n",
       "      <td>0.018314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Arvicanthis niloticus</td>\n",
       "      <td>9</td>\n",
       "      <td>0.023684</td>\n",
       "      <td>7.170390</td>\n",
       "      <td>0.021451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Aspergillus costaricensis</td>\n",
       "      <td>7</td>\n",
       "      <td>0.018421</td>\n",
       "      <td>6.823369</td>\n",
       "      <td>0.020413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Callospermophilus lateralis</td>\n",
       "      <td>6</td>\n",
       "      <td>0.015789</td>\n",
       "      <td>4.341063</td>\n",
       "      <td>0.012987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Hysterothylacium fabri</td>\n",
       "      <td>6</td>\n",
       "      <td>0.015789</td>\n",
       "      <td>4.773331</td>\n",
       "      <td>0.014280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Entoloma sp.</td>\n",
       "      <td>5</td>\n",
       "      <td>0.013158</td>\n",
       "      <td>3.705058</td>\n",
       "      <td>0.011084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Eucoleus sp.</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010526</td>\n",
       "      <td>3.861940</td>\n",
       "      <td>0.011554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amanita fuscozonata</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010526</td>\n",
       "      <td>2.818823</td>\n",
       "      <td>0.008433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Erysiphe pisi</td>\n",
       "      <td>3</td>\n",
       "      <td>0.007895</td>\n",
       "      <td>2.119103</td>\n",
       "      <td>0.006340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Deuterostichococcus epilithicus</td>\n",
       "      <td>3</td>\n",
       "      <td>0.007895</td>\n",
       "      <td>2.826269</td>\n",
       "      <td>0.008455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Trichoderma viride</td>\n",
       "      <td>3</td>\n",
       "      <td>0.007895</td>\n",
       "      <td>2.123955</td>\n",
       "      <td>0.006354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Pseudopestalotiopsis sp.</td>\n",
       "      <td>3</td>\n",
       "      <td>0.007895</td>\n",
       "      <td>2.855906</td>\n",
       "      <td>0.008544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Chloroidium saccharophilum</td>\n",
       "      <td>3</td>\n",
       "      <td>0.007895</td>\n",
       "      <td>2.945999</td>\n",
       "      <td>0.008813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alternaria alternata</td>\n",
       "      <td>2</td>\n",
       "      <td>0.005263</td>\n",
       "      <td>1.478154</td>\n",
       "      <td>0.004422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Morchella nipponensis</td>\n",
       "      <td>2</td>\n",
       "      <td>0.005263</td>\n",
       "      <td>1.946300</td>\n",
       "      <td>0.005823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Inocybe miranda</td>\n",
       "      <td>2</td>\n",
       "      <td>0.005263</td>\n",
       "      <td>1.888802</td>\n",
       "      <td>0.005651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Omphalotus flagelliformis</td>\n",
       "      <td>2</td>\n",
       "      <td>0.005263</td>\n",
       "      <td>1.832577</td>\n",
       "      <td>0.005482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            species  count  relative_abundance    conf_sum  \\\n",
       "51                       UNASSIGNED    141            0.371053  135.859698   \n",
       "28                  Maylandia zebra     82            0.215789   79.453327   \n",
       "10                 Chaetodon auriga     46            0.121053   25.948245   \n",
       "33                    Morchella sp.     10            0.026316    6.121686   \n",
       "5             Arvicanthis niloticus      9            0.023684    7.170390   \n",
       "6         Aspergillus costaricensis      7            0.018421    6.823369   \n",
       "8       Callospermophilus lateralis      6            0.015789    4.341063   \n",
       "20           Hysterothylacium fabri      6            0.015789    4.773331   \n",
       "17                     Entoloma sp.      5            0.013158    3.705058   \n",
       "19                     Eucoleus sp.      4            0.010526    3.861940   \n",
       "2               Amanita fuscozonata      4            0.010526    2.818823   \n",
       "18                    Erysiphe pisi      3            0.007895    2.119103   \n",
       "15  Deuterostichococcus epilithicus      3            0.007895    2.826269   \n",
       "49               Trichoderma viride      3            0.007895    2.123955   \n",
       "40         Pseudopestalotiopsis sp.      3            0.007895    2.855906   \n",
       "11       Chloroidium saccharophilum      3            0.007895    2.945999   \n",
       "1              Alternaria alternata      2            0.005263    1.478154   \n",
       "32            Morchella nipponensis      2            0.005263    1.946300   \n",
       "23                  Inocybe miranda      2            0.005263    1.888802   \n",
       "35        Omphalotus flagelliformis      2            0.005263    1.832577   \n",
       "\n",
       "    relative_abundance_weighted  \n",
       "51                     0.406443  \n",
       "28                     0.237695  \n",
       "10                     0.077628  \n",
       "33                     0.018314  \n",
       "5                      0.021451  \n",
       "6                      0.020413  \n",
       "8                      0.012987  \n",
       "20                     0.014280  \n",
       "17                     0.011084  \n",
       "19                     0.011554  \n",
       "2                      0.008433  \n",
       "18                     0.006340  \n",
       "15                     0.008455  \n",
       "49                     0.006354  \n",
       "40                     0.008544  \n",
       "11                     0.008813  \n",
       "1                      0.004422  \n",
       "32                     0.005823  \n",
       "23                     0.005651  \n",
       "35                     0.005482  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded before-abundance file: abundance_from_predictions.csv\n",
      "\n",
      "Top changes (before -> after) (top 30):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>count_before</th>\n",
       "      <th>count_after</th>\n",
       "      <th>delta</th>\n",
       "      <th>pct_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UNASSIGNED</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maylandia zebra</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chaetodon auriga</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Morchella sp.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arvicanthis niloticus</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Aspergillus costaricensis</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hysterothylacium fabri</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Callospermophilus lateralis</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Entoloma sp.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Eucoleus sp.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Amanita fuscozonata</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Erysiphe pisi</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Deuterostichococcus epilithicus</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Chloroidium saccharophilum</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Pseudopestalotiopsis sp.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Trichoderma viride</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Inocybe miranda</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Alternaria alternata</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Cortinarius sp.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Clavulinopsis aurantiocinnabarina</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Morchella nipponensis</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Cardimyxobolus iriomotensis</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Omphalotus flagelliformis</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Amphibiocapillaria tritonispunctati</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Agaricus argyropotamicus</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Cladosporium sp.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Pluteus ephebeus</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Inocybe knautiana</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Klebsormidium nitens</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Laccaria striatula</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                species  count_before  count_after  delta  \\\n",
       "0                            UNASSIGNED           0.0        141.0  141.0   \n",
       "1                       Maylandia zebra           0.0         82.0   82.0   \n",
       "2                      Chaetodon auriga           0.0         46.0   46.0   \n",
       "3                         Morchella sp.           0.0         10.0   10.0   \n",
       "4                 Arvicanthis niloticus           0.0          9.0    9.0   \n",
       "5             Aspergillus costaricensis           0.0          7.0    7.0   \n",
       "6                Hysterothylacium fabri           0.0          6.0    6.0   \n",
       "7           Callospermophilus lateralis           0.0          6.0    6.0   \n",
       "8                          Entoloma sp.           0.0          5.0    5.0   \n",
       "9                          Eucoleus sp.           0.0          4.0    4.0   \n",
       "10                  Amanita fuscozonata           0.0          4.0    4.0   \n",
       "11                        Erysiphe pisi           0.0          3.0    3.0   \n",
       "12      Deuterostichococcus epilithicus           0.0          3.0    3.0   \n",
       "13           Chloroidium saccharophilum           0.0          3.0    3.0   \n",
       "14             Pseudopestalotiopsis sp.           0.0          3.0    3.0   \n",
       "15                   Trichoderma viride           0.0          3.0    3.0   \n",
       "16                      Inocybe miranda           0.0          2.0    2.0   \n",
       "17                 Alternaria alternata           0.0          2.0    2.0   \n",
       "18                      Cortinarius sp.           0.0          2.0    2.0   \n",
       "19    Clavulinopsis aurantiocinnabarina           0.0          2.0    2.0   \n",
       "20                Morchella nipponensis           0.0          2.0    2.0   \n",
       "21          Cardimyxobolus iriomotensis           0.0          2.0    2.0   \n",
       "22            Omphalotus flagelliformis           0.0          2.0    2.0   \n",
       "23  Amphibiocapillaria tritonispunctati           0.0          2.0    2.0   \n",
       "24             Agaricus argyropotamicus           0.0          2.0    2.0   \n",
       "25                     Cladosporium sp.           0.0          1.0    1.0   \n",
       "26                     Pluteus ephebeus           0.0          1.0    1.0   \n",
       "27                    Inocybe knautiana           0.0          1.0    1.0   \n",
       "28                 Klebsormidium nitens           0.0          1.0    1.0   \n",
       "29                   Laccaria striatula           0.0          1.0    1.0   \n",
       "\n",
       "    pct_change  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  \n",
       "5          NaN  \n",
       "6          NaN  \n",
       "7          NaN  \n",
       "8          NaN  \n",
       "9          NaN  \n",
       "10         NaN  \n",
       "11         NaN  \n",
       "12         NaN  \n",
       "13         NaN  \n",
       "14         NaN  \n",
       "15         NaN  \n",
       "16         NaN  \n",
       "17         NaN  \n",
       "18         NaN  \n",
       "19         NaN  \n",
       "20         NaN  \n",
       "21         NaN  \n",
       "22         NaN  \n",
       "23         NaN  \n",
       "24         NaN  \n",
       "25         NaN  \n",
       "26         NaN  \n",
       "27         NaN  \n",
       "28         NaN  \n",
       "29         NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of species with count changes: 104\n",
      "Examples of changed species (first 20):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>count_before</th>\n",
       "      <th>count_after</th>\n",
       "      <th>delta</th>\n",
       "      <th>pct_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UNASSIGNED</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maylandia zebra</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chaetodon auriga</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Morchella sp.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arvicanthis niloticus</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Aspergillus costaricensis</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hysterothylacium fabri</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Callospermophilus lateralis</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Entoloma sp.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Eucoleus sp.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Amanita fuscozonata</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Erysiphe pisi</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Deuterostichococcus epilithicus</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Chloroidium saccharophilum</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Pseudopestalotiopsis sp.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Trichoderma viride</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Inocybe miranda</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Alternaria alternata</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Cortinarius sp.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Clavulinopsis aurantiocinnabarina</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              species  count_before  count_after  delta  \\\n",
       "0                          UNASSIGNED           0.0        141.0  141.0   \n",
       "1                     Maylandia zebra           0.0         82.0   82.0   \n",
       "2                    Chaetodon auriga           0.0         46.0   46.0   \n",
       "3                       Morchella sp.           0.0         10.0   10.0   \n",
       "4               Arvicanthis niloticus           0.0          9.0    9.0   \n",
       "5           Aspergillus costaricensis           0.0          7.0    7.0   \n",
       "6              Hysterothylacium fabri           0.0          6.0    6.0   \n",
       "7         Callospermophilus lateralis           0.0          6.0    6.0   \n",
       "8                        Entoloma sp.           0.0          5.0    5.0   \n",
       "9                        Eucoleus sp.           0.0          4.0    4.0   \n",
       "10                Amanita fuscozonata           0.0          4.0    4.0   \n",
       "11                      Erysiphe pisi           0.0          3.0    3.0   \n",
       "12    Deuterostichococcus epilithicus           0.0          3.0    3.0   \n",
       "13         Chloroidium saccharophilum           0.0          3.0    3.0   \n",
       "14           Pseudopestalotiopsis sp.           0.0          3.0    3.0   \n",
       "15                 Trichoderma viride           0.0          3.0    3.0   \n",
       "16                    Inocybe miranda           0.0          2.0    2.0   \n",
       "17               Alternaria alternata           0.0          2.0    2.0   \n",
       "18                    Cortinarius sp.           0.0          2.0    2.0   \n",
       "19  Clavulinopsis aurantiocinnabarina           0.0          2.0    2.0   \n",
       "\n",
       "    pct_change  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  \n",
       "5          NaN  \n",
       "6          NaN  \n",
       "7          NaN  \n",
       "8          NaN  \n",
       "9          NaN  \n",
       "10         NaN  \n",
       "11         NaN  \n",
       "12         NaN  \n",
       "13         NaN  \n",
       "14         NaN  \n",
       "15         NaN  \n",
       "16         NaN  \n",
       "17         NaN  \n",
       "18         NaN  \n",
       "19         NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote comparison CSV -> abundance_before_after_comparison_after_knn.csv\n",
      "Wrote weighted abundance CSV -> abundance_from_predictions_after_knn_weighted_recomputed.csv\n",
      "\n",
      "Done. Output files in: C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\n",
      " - abundance_from_predictions_after_knn_recomputed.csv\n",
      " - abundance_from_predictions_after_knn_weighted_recomputed.csv\n",
      " - abundance_before_after_comparison_after_knn.csv\n",
      " - knn_assignments_audit.csv\n",
      " - predictions_after_knn_assignments.csv\n"
     ]
    }
   ],
   "source": [
    "# Robust: audit KNN, recompute abundances, safe before/after comparison (no dtype merge errors)\n",
    "import pandas as pd, numpy as np, traceback\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- Config (no need to change) ----------\n",
    "OUTPUT_PREFIX = \"after_knn\"   # used for filenames\n",
    "# -------------------------------------------------\n",
    "\n",
    "def find_extracted():\n",
    "    candidates = [\n",
    "        Path.cwd()/\"sih\"/\"ncbi_blast_db\"/\"extracted\",\n",
    "        Path.cwd()/\"ncbi_blast_db\"/\"extracted\",\n",
    "        Path.cwd()/ \"extracted\",\n",
    "        Path(r\"C:\\Users\\Srijit\\sih\\ncbi_blast_db\\extracted\"),\n",
    "        Path(r\"C:\\Users\\Srijit\\OneDrive\\Desktop\\sihtaxa\\sihabundance\\ncbi_blast_db\\extracted\"),\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if p.exists() and p.is_dir():\n",
    "            return p.resolve()\n",
    "    # fallback: first folder named 'extracted' under cwd\n",
    "    for p in Path.cwd().rglob(\"**/extracted\"):\n",
    "        if p.is_dir():\n",
    "            return p.resolve()\n",
    "    return None\n",
    "\n",
    "def safe_print_df(df, n=10, label=None):\n",
    "    if df is None:\n",
    "        print(f\"{label or 'DataFrame'}: None\")\n",
    "        return\n",
    "    try:\n",
    "        from IPython.display import display\n",
    "        if label: print(label)\n",
    "        display(df.head(n))\n",
    "    except Exception:\n",
    "        print((df.head(n)).to_string())\n",
    "\n",
    "def detect_species_col(df):\n",
    "    # Prefer explicitly named prediction columns\n",
    "    for c in df.columns:\n",
    "        if \"species_pred_label\" in c.lower():\n",
    "            return c\n",
    "    # otherwise pick any column that contains 'species' (but not 'idx')\n",
    "    for c in df.columns:\n",
    "        cl = c.lower()\n",
    "        if \"species\" in cl and \"idx\" not in cl:\n",
    "            return c\n",
    "    # fallback: first object (text) column\n",
    "    text_cols = [c for c in df.columns if df[c].dtype == object]\n",
    "    if text_cols:\n",
    "        return text_cols[0]\n",
    "    # final fallback: last column\n",
    "    return df.columns[-1]\n",
    "\n",
    "def detect_conf_col(df):\n",
    "    for c in df.columns:\n",
    "        cl = c.lower()\n",
    "        if \"species_pred_conf\" in cl or cl.endswith(\"_conf\") or \"topprob\" in cl or \"mc_mean\" in cl:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def normalize_species_series(s):\n",
    "    # convert to string, strip whitespace, replace empty with 'UNASSIGNED'\n",
    "    s2 = s.astype(str).fillna(\"\").apply(lambda x: \" \".join(x.split()))\n",
    "    s2 = s2.replace({\"\": \"UNASSIGNED\", \"nan\": \"UNASSIGNED\"})\n",
    "    return s2\n",
    "\n",
    "def recompute_abundances(pred_df, species_col_hint=None):\n",
    "    \"\"\"\n",
    "    Returns (abundance_df, species_col_used)\n",
    "    abundance_df columns: species, count, relative_abundance, conf_sum, relative_abundance_weighted\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pred_df.copy()\n",
    "        species_col = species_col_hint or detect_species_col(df)\n",
    "        if species_col not in df.columns:\n",
    "            # create placeholder\n",
    "            df[species_col] = \"UNASSIGNED\"\n",
    "        df[\"_species_norm_final\"] = normalize_species_series(df[species_col])\n",
    "\n",
    "        # raw counts\n",
    "        counts = df[\"_species_norm_final\"].value_counts(dropna=False).reset_index()\n",
    "        counts.columns = [\"species\", \"count\"]\n",
    "        counts[\"species\"] = counts[\"species\"].astype(str)\n",
    "\n",
    "        # confidence-weighted\n",
    "        conf_col = detect_conf_col(df)\n",
    "        if conf_col:\n",
    "            df[\"_conf_num\"] = pd.to_numeric(df[conf_col], errors=\"coerce\").fillna(0.0)\n",
    "        else:\n",
    "            df[\"_conf_num\"] = 0.0\n",
    "        weighted = df.groupby(\"_species_norm_final\")[\"_conf_num\"].sum().reset_index().rename(columns={\"_species_norm_final\":\"species\", \"_conf_num\":\"conf_sum\"})\n",
    "        weighted[\"species\"] = weighted[\"species\"].astype(str)\n",
    "\n",
    "        # merge safely on species (both string)\n",
    "        merged = pd.merge(counts, weighted, on=\"species\", how=\"outer\").fillna(0.0)\n",
    "        # ensure numeric\n",
    "        merged[\"count\"] = pd.to_numeric(merged[\"count\"], errors=\"coerce\").fillna(0.0)\n",
    "        merged[\"conf_sum\"] = pd.to_numeric(merged[\"conf_sum\"], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "        # relative columns\n",
    "        total_count = merged[\"count\"].sum() if merged[\"count\"].sum() > 0 else 1.0\n",
    "        merged[\"relative_abundance\"] = merged[\"count\"] / total_count\n",
    "        total_conf = merged[\"conf_sum\"].sum() if merged[\"conf_sum\"].sum() > 0 else 1.0\n",
    "        merged[\"relative_abundance_weighted\"] = merged[\"conf_sum\"] / total_conf\n",
    "\n",
    "        # keep nice column order\n",
    "        cols_keep = [\"species\",\"count\",\"relative_abundance\",\"conf_sum\",\"relative_abundance_weighted\"]\n",
    "        for c in cols_keep:\n",
    "            if c not in merged.columns:\n",
    "                merged[c] = 0.0\n",
    "        merged = merged[cols_keep]\n",
    "        return merged, species_col\n",
    "    except Exception as e:\n",
    "        print(\"Error in recompute_abundances:\", e)\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame(columns=[\"species\",\"count\",\"relative_abundance\",\"conf_sum\",\"relative_abundance_weighted\"]), species_col_hint\n",
    "\n",
    "# ------------------ Main ------------------\n",
    "extracted = find_extracted()\n",
    "if extracted is None:\n",
    "    raise SystemExit(\"Could not find an 'extracted' folder. Put your outputs into an 'extracted' directory and re-run.\")\n",
    "print(\"Using extracted folder:\", extracted)\n",
    "\n",
    "# pick predictions (prefer post-KNN file)\n",
    "candidate_preds = [\n",
    "    \"predictions_after_knn_assignments.csv\",\n",
    "    \"predictions_after_knn_rerun.csv\",\n",
    "    \"predictions_with_uncertainty.csv\",\n",
    "    \"predictions.csv\"\n",
    "]\n",
    "pred_path = None\n",
    "for fn in candidate_preds:\n",
    "    p = extracted / fn\n",
    "    if p.exists():\n",
    "        pred_path = p\n",
    "        break\n",
    "if pred_path is None:\n",
    "    raise SystemExit(\"No predictions CSV found in extracted/. Expected one of: \" + \", \".join(candidate_preds))\n",
    "print(\"Loaded predictions file:\", pred_path.name)\n",
    "\n",
    "pred = pd.read_csv(pred_path)\n",
    "print(\"Predictions rows:\", len(pred))\n",
    "\n",
    "# load and display audit if present\n",
    "audit_path = extracted / \"knn_assignments_audit.csv\"\n",
    "if audit_path.exists():\n",
    "    try:\n",
    "        audit = pd.read_csv(audit_path)\n",
    "        print(\"Found KNN audit:\", audit_path.name, \"rows =\", len(audit))\n",
    "        safe_print_df(audit, n=50, label=\"KNN audit (first rows):\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed to read audit CSV:\", e)\n",
    "        audit = None\n",
    "else:\n",
    "    print(\"No KNN audit file found.\")\n",
    "\n",
    "# recompute abundances from current predictions\n",
    "after_abund, species_col_used = recompute_abundances(pred)\n",
    "out_after_path = extracted / f\"abundance_from_predictions_{OUTPUT_PREFIX}_recomputed.csv\"\n",
    "after_abund.to_csv(out_after_path, index=False)\n",
    "print(\"Wrote recomputed abundance ->\", out_after_path.name)\n",
    "safe_print_df(after_abund.sort_values(\"count\", ascending=False), n=20, label=\"Top taxa (post-KNN recomputed):\")\n",
    "\n",
    "# try load previous raw abundance for comparison (if present)\n",
    "before_path = extracted / \"abundance_from_predictions.csv\"\n",
    "if before_path.exists():\n",
    "    try:\n",
    "        before_abund = pd.read_csv(before_path)\n",
    "        print(\"Loaded before-abundance file:\", before_path.name)\n",
    "        # normalize column names: try to find species & count columns\n",
    "        if 'species' not in before_abund.columns:\n",
    "            # assume first column is species\n",
    "            before_abund = before_abund.rename(columns={before_abund.columns[0]:'species'})\n",
    "        if 'count' not in before_abund.columns:\n",
    "            # choose plausible count column\n",
    "            if 'pred_count' in before_abund.columns:\n",
    "                before_abund = before_abund.rename(columns={'pred_count':'count'})\n",
    "            elif before_abund.shape[1] >= 2:\n",
    "                before_abund = before_abund.rename(columns={before_abund.columns[1]:'count'})\n",
    "            else:\n",
    "                before_abund['count'] = 0\n",
    "        # force species to string to avoid dtype mismatch during merge\n",
    "        before_abund['species'] = before_abund['species'].astype(str)\n",
    "        before_abund['count'] = pd.to_numeric(before_abund['count'], errors='coerce').fillna(0.0)\n",
    "        # ensure after table species are strings as well\n",
    "        after_abund['species'] = after_abund['species'].astype(str)\n",
    "        after_abund['count'] = pd.to_numeric(after_abund['count'], errors='coerce').fillna(0.0)\n",
    "\n",
    "        # merge safely on string species\n",
    "        merged_comp = pd.merge(\n",
    "            before_abund[['species','count']].rename(columns={'count':'count_before'}),\n",
    "            after_abund[['species','count']].rename(columns={'count':'count_after'}),\n",
    "            on='species', how='outer'\n",
    "        ).fillna(0.0)\n",
    "        merged_comp['delta'] = merged_comp['count_after'] - merged_comp['count_before']\n",
    "        # safe pct change: NaN when before==0\n",
    "        merged_comp['pct_change'] = merged_comp.apply(lambda r: (r['delta'] / r['count_before']*100.0) if r['count_before']>0 else np.nan, axis=1)\n",
    "        merged_comp = merged_comp.sort_values('count_after', ascending=False).reset_index(drop=True)\n",
    "        print(\"\\nTop changes (before -> after) (top 30):\")\n",
    "        safe_print_df(merged_comp, n=30)\n",
    "        changed = merged_comp[merged_comp['delta'] != 0.0]\n",
    "        print(\"Number of species with count changes:\", len(changed))\n",
    "        if len(changed) > 0:\n",
    "            print(\"Examples of changed species (first 20):\")\n",
    "            safe_print_df(changed.head(20), n=20)\n",
    "        # save comparison for audit\n",
    "        comp_out = extracted / f\"abundance_before_after_comparison_{OUTPUT_PREFIX}.csv\"\n",
    "        merged_comp.to_csv(comp_out, index=False)\n",
    "        print(\"Wrote comparison CSV ->\", comp_out.name)\n",
    "    except Exception as e:\n",
    "        print(\"Error while comparing before/after abundance:\", e)\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"No prior 'abundance_from_predictions.csv' found; skipping before/after comparison.\")\n",
    "\n",
    "# Also save weighted counts if not already present (recompute weighted table)\n",
    "try:\n",
    "    # recompute weighted table from pred (uses conf col if present)\n",
    "    _, _ = recompute_abundances(pred, species_col_hint=species_col_used)  # ensures columns present\n",
    "    weighted_out = extracted / f\"abundance_from_predictions_{OUTPUT_PREFIX}_weighted_recomputed.csv\"\n",
    "    # use after_abund (which already has conf_sum & relative_abundance_weighted)\n",
    "    if 'conf_sum' not in after_abund.columns:\n",
    "        after_abund['conf_sum'] = 0.0\n",
    "        after_abund['relative_abundance_weighted'] = 0.0\n",
    "    after_abund[['species','conf_sum','relative_abundance_weighted']].to_csv(weighted_out, index=False)\n",
    "    print(\"Wrote weighted abundance CSV ->\", weighted_out.name)\n",
    "except Exception as e:\n",
    "    print(\"Failed to write weighted abundance:\", e)\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\nDone. Output files in:\", extracted)\n",
    "for fn in [\n",
    "    out_after_path.name,\n",
    "    f\"abundance_from_predictions_{OUTPUT_PREFIX}_weighted_recomputed.csv\",\n",
    "    f\"abundance_before_after_comparison_{OUTPUT_PREFIX}.csv\",\n",
    "    \"knn_assignments_audit.csv\",\n",
    "    \"predictions_after_knn_assignments.csv\",\n",
    "]:\n",
    "    p = extracted / fn\n",
    "    if p.exists():\n",
    "        print(\" -\", fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0602ec68-90b1-43db-afe9-382c7818556f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sih)",
   "language": "python",
   "name": "sih"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
